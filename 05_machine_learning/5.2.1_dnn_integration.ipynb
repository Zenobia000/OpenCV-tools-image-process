{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.1 æ·±åº¦å­¸ç¿’æ•´åˆ - OpenCV DNNæ¨¡çµ„æ‡‰ç”¨\n",
    "\n",
    "æœ¬æ¨¡çµ„ä»‹ç´¹å¦‚ä½•ä½¿ç”¨OpenCVçš„DNNæ¨¡çµ„è¼‰å…¥å’Œä½¿ç”¨é è¨“ç·´çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œå¯¦ç¾é«˜ç²¾åº¦çš„ç‰©é«”æª¢æ¸¬å’Œäººè‡‰æª¢æ¸¬ã€‚\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- æŒæ¡OpenCV DNNæ¨¡çµ„çš„åŸºæœ¬ä½¿ç”¨\n",
    "- è¼‰å…¥ä¸åŒæ¡†æ¶çš„é è¨“ç·´æ¨¡å‹ (TensorFlow, PyTorch, ONNX)\n",
    "- å¯¦ç¾åŸºæ–¼CNNçš„äººè‡‰æª¢æ¸¬\n",
    "- æ¯”è¼ƒæ·±åº¦å­¸ç¿’èˆ‡å‚³çµ±æ–¹æ³•çš„æ•ˆèƒ½\n",
    "- å¯¦ç¾å¯¦æ™‚ç‰©é«”æª¢æ¸¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®èˆ‡æ¨¡çµ„å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# æ·»åŠ utilsè·¯å¾‘\n",
    "sys.path.append('../utils')\n",
    "from image_utils import load_image, resize_image\n",
    "from visualization import display_image, display_multiple_images\n",
    "from performance import time_function, benchmark_function\n",
    "\n",
    "# æª¢æŸ¥OpenCV DNNæ”¯æ´\n",
    "print(f\"OpenCVç‰ˆæœ¬: {cv2.__version__}\")\n",
    "print(f\"DNNæ¨¡çµ„å¯ç”¨: {'âœ…' if hasattr(cv2, 'dnn') else 'âŒ'}\")\n",
    "\n",
    "# æª¢æŸ¥å¯ç”¨çš„DNNå¾Œç«¯\n",
    "backends = {\n",
    "    'OpenCV': cv2.dnn.DNN_BACKEND_OPENCV,\n",
    "    'CUDA': cv2.dnn.DNN_BACKEND_CUDA,\n",
    "    'OpenVINO': cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE\n",
    "}\n",
    "\n",
    "print(\"\\nå¯ç”¨çš„DNNå¾Œç«¯:\")\n",
    "for name, backend in backends.items():\n",
    "    try:\n",
    "        cv2.dnn.setPreferableBackend(backend)\n",
    "        print(f\"  {name}: âœ…\")\n",
    "    except:\n",
    "        print(f\"  {name}: âŒ\")\n",
    "\n",
    "# é‡è¨­ç‚ºé è¨­å¾Œç«¯\n",
    "cv2.dnn.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "cv2.dnn.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# è¨­ç½®matplotlibä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DNNäººè‡‰æª¢æ¸¬æ¨¡å‹è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNFaceDetector:\n",
    "    \"\"\"åŸºæ–¼DNNçš„äººè‡‰æª¢æ¸¬å™¨é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, config_path=None, confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–DNNäººè‡‰æª¢æ¸¬å™¨\n",
    "        \n",
    "        Args:\n",
    "            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘\n",
    "            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘\n",
    "            confidence_threshold: ä¿¡å¿ƒåº¦é–¾å€¼\n",
    "        \"\"\"\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.net = None\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        # å˜—è©¦è¼‰å…¥é è¨­æ¨¡å‹\n",
    "        if model_path is None:\n",
    "            model_path = \"../assets/models/opencv_face_detector_uint8.pb\"\n",
    "        if config_path is None:\n",
    "            config_path = \"../assets/models/opencv_face_detector.pbtxt\"\n",
    "            \n",
    "        self.load_model(model_path, config_path)\n",
    "    \n",
    "    def load_model(self, model_path, config_path):\n",
    "        \"\"\"\n",
    "        è¼‰å…¥DNNæ¨¡å‹\n",
    "        \n",
    "        Args:\n",
    "            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘\n",
    "            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if os.path.exists(model_path) and os.path.exists(config_path):\n",
    "                self.net = cv2.dnn.readNetFromTensorflow(model_path, config_path)\n",
    "                self.model_loaded = True\n",
    "                print(f\"âœ… DNNäººè‡‰æª¢æ¸¬æ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "                print(f\"  - æ¨¡å‹: {os.path.basename(model_path)}\")\n",
    "                print(f\"  - é…ç½®: {os.path.basename(config_path)}\")\n",
    "            else:\n",
    "                print(f\"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨:\")\n",
    "                print(f\"  - æ¨¡å‹: {model_path} {'âœ…' if os.path.exists(model_path) else 'âŒ'}\")\n",
    "                print(f\"  - é…ç½®: {config_path} {'âœ…' if os.path.exists(config_path) else 'âŒ'}\")\n",
    "                print(\"\\nğŸ’¡ è«‹ä¸‹è¼‰OpenCV DNNäººè‡‰æª¢æ¸¬æ¨¡å‹:\")\n",
    "                print(\"  https://github.com/opencv/opencv_3rdparty/raw/19512576c112aa2c7b6328cb0e8d589a4a90a26d/opencv_face_detector_uint8.pb\")\n",
    "                print(\"  https://github.com/opencv/opencv/raw/master/samples/dnn/face_detector/opencv_face_detector.pbtxt\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            self.model_loaded = False\n",
    "    \n",
    "    def detect_faces(self, image, input_size=(300, 300)):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨DNNæª¢æ¸¬äººè‡‰\n",
    "        \n",
    "        Args:\n",
    "            image: è¼¸å…¥åœ–åƒ\n",
    "            input_size: ç¶²è·¯è¼¸å…¥å¤§å°\n",
    "        \n",
    "        Returns:\n",
    "            list: æª¢æ¸¬åˆ°çš„äººè‡‰ [(x, y, w, h, confidence), ...]\n",
    "        \"\"\"\n",
    "        if not self.model_loaded:\n",
    "            return []\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # å‰µå»ºblob\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            image, \n",
    "            scalefactor=1.0, \n",
    "            size=input_size, \n",
    "            mean=[104, 117, 123],  # æ¨¡å‹è¨“ç·´æ™‚çš„å‡å€¼\n",
    "            swapRB=False\n",
    "        )\n",
    "        \n",
    "        # è¨­å®šè¼¸å…¥ä¸¦åŸ·è¡Œæ¨ç†\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "        \n",
    "        faces = []\n",
    "        \n",
    "        # è§£ææª¢æ¸¬çµæœ\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            if confidence > self.confidence_threshold:\n",
    "                # ç²å–é‚Šç•Œæ¡†åº§æ¨™\n",
    "                x1 = int(detections[0, 0, i, 3] * w)\n",
    "                y1 = int(detections[0, 0, i, 4] * h)\n",
    "                x2 = int(detections[0, 0, i, 5] * w)\n",
    "                y2 = int(detections[0, 0, i, 6] * h)\n",
    "                \n",
    "                # ç¢ºä¿åº§æ¨™åœ¨åœ–åƒç¯„åœå…§\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(w, x2)\n",
    "                y2 = min(h, y2)\n",
    "                \n",
    "                # è½‰æ›ç‚º (x, y, w, h, confidence) æ ¼å¼\n",
    "                faces.append((x1, y1, x2-x1, y2-y1, confidence))\n",
    "        \n",
    "        return faces\n",
    "\n",
    "# åˆå§‹åŒ–DNNæª¢æ¸¬å™¨\n",
    "dnn_detector = DNNFaceDetector(confidence_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNNäººè‡‰æª¢æ¸¬æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_function\n",
    "def detect_faces_dnn_timed(image):\n",
    "    \"\"\"å¸¶è¨ˆæ™‚çš„DNNäººè‡‰æª¢æ¸¬\"\"\"\n",
    "    return dnn_detector.detect_faces(image)\n",
    "\n",
    "def test_dnn_face_detection():\n",
    "    \"\"\"æ¸¬è©¦DNNäººè‡‰æª¢æ¸¬\"\"\"\n",
    "    test_images = [\n",
    "        \"../assets/images/basic/face03.jpg\",\n",
    "        \"../assets/images/basic/faces01.jpg\",\n",
    "        \"../assets/images/basic/faces.png\"\n",
    "    ]\n",
    "    \n",
    "    for image_path in test_images:\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nğŸ“· æ¸¬è©¦åœ–ç‰‡: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        # è¼‰å…¥åœ–åƒ\n",
    "        image = load_image(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        # èª¿æ•´å¤§å°ä»¥æå‡æª¢æ¸¬é€Ÿåº¦\n",
    "        image_resized = resize_image(image, max_width=800)\n",
    "        \n",
    "        # DNNæª¢æ¸¬\n",
    "        faces, detection_time = detect_faces_dnn_timed(image_resized)\n",
    "        \n",
    "        # ç¹ªè£½æª¢æ¸¬çµæœ\n",
    "        result = image_resized.copy()\n",
    "        for i, (x, y, w, h, conf) in enumerate(faces):\n",
    "            # ç¹ªè£½é‚Šç•Œæ¡†\n",
    "            cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # é¡¯ç¤ºä¿¡å¿ƒåº¦\n",
    "            label = f'Face {i+1}: {conf:.2f}'\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "            \n",
    "            # èƒŒæ™¯çŸ©å½¢\n",
    "            cv2.rectangle(result, (x, y-label_size[1]-10), (x+label_size[0], y), (0, 255, 0), -1)\n",
    "            cv2.putText(result, label, (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "        # é¡¯ç¤ºçµæœ\n",
    "        display_multiple_images(\n",
    "            [image_resized, result],\n",
    "            [\"åŸå§‹åœ–ç‰‡\", f\"DNNæª¢æ¸¬ ({len(faces)} faces, {detection_time:.1f}ms)\"],\n",
    "            figsize=(12, 6)\n",
    "        )\n",
    "        \n",
    "        print(f\"  âœ… æª¢æ¸¬åˆ° {len(faces)} å¼µäººè‡‰ï¼Œè€—æ™‚ {detection_time:.1f}ms\")\n",
    "        for i, (x, y, w, h, conf) in enumerate(faces, 1):\n",
    "            print(f\"     äººè‡‰{i}: ({x:3d},{y:3d}) {w:3d}x{h:3d}, ä¿¡å¿ƒåº¦: {conf:.3f}\")\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_dnn_face_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOLOç‰©é«”æª¢æ¸¬æ•´åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODetector:\n",
    "    \"\"\"YOLOç‰©é«”æª¢æ¸¬å™¨é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self, weights_path=None, config_path=None, classes_path=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–YOLOæª¢æ¸¬å™¨\n",
    "        \n",
    "        Args:\n",
    "            weights_path: YOLOæ¬Šé‡æª”æ¡ˆè·¯å¾‘\n",
    "            config_path: YOLOé…ç½®æª”æ¡ˆè·¯å¾‘  \n",
    "            classes_path: é¡åˆ¥åç¨±æª”æ¡ˆè·¯å¾‘\n",
    "        \"\"\"\n",
    "        self.net = None\n",
    "        self.classes = []\n",
    "        self.output_layers = []\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        # å˜—è©¦è¼‰å…¥é è¨­YOLOæ¨¡å‹\n",
    "        if weights_path is None:\n",
    "            weights_path = \"../assets/models/yolo/yolov3.weights\"\n",
    "        if config_path is None:\n",
    "            config_path = \"../assets/models/yolo/yolov3.cfg\"\n",
    "        if classes_path is None:\n",
    "            classes_path = \"../assets/models/yolo/coco.names\"\n",
    "            \n",
    "        self.load_model(weights_path, config_path, classes_path)\n",
    "    \n",
    "    def load_model(self, weights_path, config_path, classes_path):\n",
    "        \"\"\"\n",
    "        è¼‰å…¥YOLOæ¨¡å‹\n",
    "        \n",
    "        Args:\n",
    "            weights_path: æ¬Šé‡æª”æ¡ˆè·¯å¾‘\n",
    "            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘\n",
    "            classes_path: é¡åˆ¥æª”æ¡ˆè·¯å¾‘\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "            files_exist = [\n",
    "                os.path.exists(weights_path),\n",
    "                os.path.exists(config_path),\n",
    "                os.path.exists(classes_path)\n",
    "            ]\n",
    "            \n",
    "            if not all(files_exist):\n",
    "                print(f\"âŒ YOLOæ¨¡å‹æª”æ¡ˆç¼ºå¤±:\")\n",
    "                print(f\"  - æ¬Šé‡: {weights_path} {'âœ…' if files_exist[0] else 'âŒ'}\")\n",
    "                print(f\"  - é…ç½®: {config_path} {'âœ…' if files_exist[1] else 'âŒ'}\")\n",
    "                print(f\"  - é¡åˆ¥: {classes_path} {'âœ…' if files_exist[2] else 'âŒ'}\")\n",
    "                print(\"\\nğŸ’¡ è«‹ä¸‹è¼‰YOLOæ¨¡å‹æª”æ¡ˆ:\")\n",
    "                print(\"  https://pjreddie.com/media/files/yolov3.weights\")\n",
    "                print(\"  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\")\n",
    "                print(\"  https://github.com/pjreddie/darknet/blob/master/data/coco.names\")\n",
    "                return\n",
    "            \n",
    "            # è¼‰å…¥YOLOç¶²è·¯\n",
    "            self.net = cv2.dnn.readNet(weights_path, config_path)\n",
    "            \n",
    "            # è¼‰å…¥é¡åˆ¥åç¨±\n",
    "            with open(classes_path, 'r') as f:\n",
    "                self.classes = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "            # ç²å–è¼¸å‡ºå±¤åç¨±\n",
    "            layer_names = self.net.getLayerNames()\n",
    "            self.output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "            \n",
    "            self.model_loaded = True\n",
    "            print(f\"âœ… YOLOæ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "            print(f\"  - é¡åˆ¥æ•¸: {len(self.classes)}\")\n",
    "            print(f\"  - è¼¸å‡ºå±¤: {len(self.output_layers)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ YOLOæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            self.model_loaded = False\n",
    "    \n",
    "    def detect_objects(self, image, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨YOLOæª¢æ¸¬ç‰©é«”\n",
    "        \n",
    "        Args:\n",
    "            image: è¼¸å…¥åœ–åƒ\n",
    "            confidence_threshold: ä¿¡å¿ƒåº¦é–¾å€¼\n",
    "            nms_threshold: NMSé–¾å€¼\n",
    "        \n",
    "        Returns:\n",
    "            list: æª¢æ¸¬çµæœ [(class_id, class_name, confidence, (x, y, w, h)), ...]\n",
    "        \"\"\"\n",
    "        if not self.model_loaded:\n",
    "            return []\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # å‰µå»ºblob\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            image, \n",
    "            1/255.0, \n",
    "            (416, 416), \n",
    "            (0, 0, 0), \n",
    "            True, \n",
    "            crop=False\n",
    "        )\n",
    "        \n",
    "        # è¨­å®šè¼¸å…¥ä¸¦åŸ·è¡Œæ¨ç†\n",
    "        self.net.setInput(blob)\n",
    "        outputs = self.net.forward(self.output_layers)\n",
    "        \n",
    "        # è§£ææª¢æ¸¬çµæœ\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                \n",
    "                if confidence > confidence_threshold:\n",
    "                    # è¨ˆç®—é‚Šç•Œæ¡†\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    \n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    \n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        \n",
    "        # æ‡‰ç”¨éæœ€å¤§æŠ‘åˆ¶\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "        \n",
    "        detections = []\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                class_name = self.classes[class_ids[i]] if class_ids[i] < len(self.classes) else 'unknown'\n",
    "                detections.append((\n",
    "                    class_ids[i],\n",
    "                    class_name,\n",
    "                    confidences[i],\n",
    "                    tuple(boxes[i])\n",
    "                ))\n",
    "        \n",
    "        return detections\n",
    "\n",
    "# åˆå§‹åŒ–YOLOæª¢æ¸¬å™¨ï¼ˆå¦‚æœæ¨¡å‹å­˜åœ¨ï¼‰\n",
    "yolo_detector = YOLODetector()\n",
    "\n",
    "def create_demo_detection_result():\n",
    "    \"\"\"å‰µå»ºä¸€å€‹æ¼”ç¤ºæª¢æ¸¬çµæœï¼ˆç•¶YOLOæ¨¡å‹ä¸å¯ç”¨æ™‚ï¼‰\"\"\"\n",
    "    print(\"\\nğŸ’¡ YOLOæ¼”ç¤ºæ¨¡å¼ - æ¨¡æ“¬æª¢æ¸¬çµæœ\")\n",
    "    print(\"å¯¦éš›ä½¿ç”¨æ™‚éœ€è¦ä¸‹è¼‰å®Œæ•´çš„YOLOæ¨¡å‹æª”æ¡ˆ\")\n",
    "    \n",
    "    # è¼‰å…¥æ¸¬è©¦åœ–ç‰‡\n",
    "    test_image_path = \"../assets/images/basic/faces01.jpg\"\n",
    "    if os.path.exists(test_image_path):\n",
    "        image = load_image(test_image_path)\n",
    "        if image is not None:\n",
    "            # æ¨¡æ“¬æª¢æ¸¬çµæœ\n",
    "            demo_result = image.copy()\n",
    "            h, w = image.shape[:2]\n",
    "            \n",
    "            # æ·»åŠ æ¨¡æ“¬çš„æª¢æ¸¬æ¡†\n",
    "            cv2.rectangle(demo_result, (50, 50), (w-50, h-50), (255, 0, 0), 3)\n",
    "            cv2.putText(demo_result, 'YOLO Demo Mode', (60, 40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            cv2.putText(demo_result, 'Download YOLO models for real detection', (60, h-20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            \n",
    "            display_image(demo_result, \"YOLOæ¼”ç¤ºæ¨¡å¼\", figsize=(10, 8))\n",
    "\n",
    "# å¦‚æœYOLOæ¨¡å‹ä¸å¯ç”¨ï¼Œé¡¯ç¤ºæ¼”ç¤º\n",
    "if not yolo_detector.model_loaded:\n",
    "    create_demo_detection_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å¤šæ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_detection_comparison(image_path):\n",
    "    \"\"\"\n",
    "    å…¨é¢æ¯”è¼ƒä¸åŒæª¢æ¸¬æ–¹æ³•çš„æ•ˆèƒ½\n",
    "    \n",
    "    Args:\n",
    "        image_path: æ¸¬è©¦åœ–åƒè·¯å¾‘\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"åœ–ç‰‡ä¸å­˜åœ¨: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    # èª¿æ•´åœ–ç‰‡å¤§å°\n",
    "    image_resized = resize_image(image, max_width=640)\n",
    "    \n",
    "    results = []\n",
    "    titles = []\n",
    "    performance_data = []\n",
    "    \n",
    "    print(f\"\\nğŸ”„ åŸ·è¡Œå¤šæ¨¡å‹æª¢æ¸¬æ¯”è¼ƒ: {os.path.basename(image_path)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 1. Haar Cascadeæª¢æ¸¬\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \n",
    "                                            'haarcascade_frontalface_default.xml')\n",
    "        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)\n",
    "        faces_haar = haar_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "        \n",
    "        haar_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # ç¹ªè£½çµæœ\n",
    "        result_haar = image_resized.copy()\n",
    "        for (x, y, w, h) in faces_haar:\n",
    "            cv2.rectangle(result_haar, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(result_haar, 'Haar', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "        \n",
    "        results.append(result_haar)\n",
    "        titles.append(f\"Haar Cascade\\n{len(faces_haar)} faces, {haar_time:.1f}ms\")\n",
    "        performance_data.append(('Haar Cascade', len(faces_haar), haar_time, 'å‚³çµ±æ–¹æ³•'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Haar Cascadeæª¢æ¸¬å¤±æ•—: {e}\")\n",
    "    \n",
    "    # 2. DNNäººè‡‰æª¢æ¸¬\n",
    "    if dnn_detector.model_loaded:\n",
    "        start_time = time.time()\n",
    "        faces_dnn = dnn_detector.detect_faces(image_resized)\n",
    "        dnn_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # ç¹ªè£½çµæœ\n",
    "        result_dnn = image_resized.copy()\n",
    "        for (x, y, w, h, conf) in faces_dnn:\n",
    "            cv2.rectangle(result_dnn, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(result_dnn, f'DNN {conf:.2f}', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        results.append(result_dnn)\n",
    "        titles.append(f\"OpenCV DNN\\n{len(faces_dnn)} faces, {dnn_time:.1f}ms\")\n",
    "        performance_data.append(('OpenCV DNN', len(faces_dnn), dnn_time, 'æ·±åº¦å­¸ç¿’'))\n",
    "    \n",
    "    # 3. dlibæª¢æ¸¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "    try:\n",
    "        import dlib\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        faces_dlib = detector(gray, 1)\n",
    "        dlib_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # ç¹ªè£½çµæœ\n",
    "        result_dlib = image_resized.copy()\n",
    "        for face in faces_dlib:\n",
    "            x, y = face.left(), face.top()\n",
    "            w, h = face.width(), face.height()\n",
    "            cv2.rectangle(result_dlib, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.putText(result_dlib, 'dlib', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        \n",
    "        results.append(result_dlib)\n",
    "        titles.append(f\"dlib HOG\\n{len(faces_dlib)} faces, {dlib_time:.1f}ms\")\n",
    "        performance_data.append(('dlib HOG', len(faces_dlib), dlib_time, 'æ©Ÿå™¨å­¸ç¿’'))\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"dlibä¸å¯ç”¨ï¼Œè·³éæª¢æ¸¬\")\n",
    "    except Exception as e:\n",
    "        print(f\"dlibæª¢æ¸¬å¤±æ•—: {e}\")\n",
    "    \n",
    "    # é¡¯ç¤ºæ¯”è¼ƒçµæœ\n",
    "    if results:\n",
    "        all_images = [image_resized] + results\n",
    "        all_titles = [\"åŸå§‹åœ–ç‰‡\"] + titles\n",
    "        \n",
    "        # è¨ˆç®—åˆé©çš„åœ–ç‰‡å¤§å°\n",
    "        cols = min(len(all_images), 4)\n",
    "        figsize = (cols * 4, 6)\n",
    "        \n",
    "        display_multiple_images(all_images, all_titles, figsize=figsize)\n",
    "        \n",
    "        # è¼¸å‡ºæ€§èƒ½çµ±è¨ˆè¡¨æ ¼\n",
    "        print(\"\\nğŸ“Š æª¢æ¸¬æ–¹æ³•æ•ˆèƒ½æ¯”è¼ƒ:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'æ–¹æ³•':12} {'é¡å‹':8} {'æª¢æ¸¬æ•¸':6} {'æ™‚é–“(ms)':8} {'é€Ÿåº¦è©•ç´š':8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for method, count, time_ms, method_type in performance_data:\n",
    "            if time_ms < 50:\n",
    "                speed_rating = \"æ¥µå¿«\"\n",
    "            elif time_ms < 100:\n",
    "                speed_rating = \"å¿«\"\n",
    "            elif time_ms < 200:\n",
    "                speed_rating = \"ä¸­ç­‰\"\n",
    "            else:\n",
    "                speed_rating = \"æ…¢\"\n",
    "                \n",
    "            print(f\"{method:12} {method_type:8} {count:6d} {time_ms:8.1f} {speed_rating:8}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # æ¨è–¦ä½¿ç”¨å ´æ™¯\n",
    "        print(\"\\nğŸ’¡ ä½¿ç”¨å ´æ™¯æ¨è–¦:\")\n",
    "        print(\"  â€¢ Haar Cascade: è³‡æºå—é™ç’°å¢ƒã€å¯¦æ™‚æ‡‰ç”¨\")\n",
    "        print(\"  â€¢ OpenCV DNN: ç²¾åº¦è¦æ±‚é«˜ã€ç¾ä»£æ‡‰ç”¨\")\n",
    "        print(\"  â€¢ dlib HOG: ç‰¹å¾µé»æª¢æ¸¬ã€äººè‡‰åˆ†æ\")\n",
    "\n",
    "# åŸ·è¡Œæ¯”è¼ƒæ¸¬è©¦\n",
    "test_image = \"../assets/images/basic/faces01.jpg\"\n",
    "if os.path.exists(test_image):\n",
    "    comprehensive_detection_comparison(test_image)\nelse:\n",
    "    print(f\"æ¸¬è©¦åœ–ç‰‡ä¸å­˜åœ¨: {test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯¦æ™‚æª¢æ¸¬å„ªåŒ–æŠ€è¡“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeDetector:\n",
    "    \"\"\"å¯¦æ™‚æª¢æ¸¬å„ªåŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, detector_type='dnn'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¯¦æ™‚æª¢æ¸¬å™¨\n",
    "        \n",
    "        Args:\n",
    "            detector_type: æª¢æ¸¬å™¨é¡å‹ ('dnn', 'haar', 'dlib')\n",
    "        \"\"\"\n",
    "        self.detector_type = detector_type\n",
    "        self.frame_skip = 0  # å¹€è·³èºè¨ˆæ•¸å™¨\n",
    "        self.skip_frames = 2  # æ¯éš”Nå¹€æª¢æ¸¬ä¸€æ¬¡\n",
    "        self.last_detections = []  # ä¸Šæ¬¡æª¢æ¸¬çµæœ\n",
    "        self.detection_history = []  # æª¢æ¸¬æ­·å²\n",
    "        \n",
    "        # åˆå§‹åŒ–æª¢æ¸¬å™¨\n",
    "        if detector_type == 'dnn' and dnn_detector.model_loaded:\n",
    "            self.detector = dnn_detector\n",
    "        elif detector_type == 'haar':\n",
    "            self.detector = cv2.CascadeClassifier(cv2.data.haarcascades + \n",
    "                                                 'haarcascade_frontalface_default.xml')\n",
    "        else:\n",
    "            print(f\"æª¢æ¸¬å™¨é¡å‹ {detector_type} ä¸å¯ç”¨\")\n",
    "            self.detector = None\n",
    "    \n",
    "    def process_frame(self, frame, use_optimization=True):\n",
    "        \"\"\"\n",
    "        è™•ç†å–®å€‹å¹€\n",
    "        \n",
    "        Args:\n",
    "            frame: è¼¸å…¥å¹€\n",
    "            use_optimization: æ˜¯å¦ä½¿ç”¨å„ªåŒ–æŠ€è¡“\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (è™•ç†å¾Œçš„å¹€, æª¢æ¸¬çµæœ, è™•ç†æ™‚é–“)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.detector is None:\n",
    "            return frame, [], 0\n",
    "        \n",
    "        # å„ªåŒ–æŠ€è¡“1: å¹€è·³èº\n",
    "        if use_optimization:\n",
    "            self.frame_skip += 1\n",
    "            if self.frame_skip < self.skip_frames:\n",
    "                # ä½¿ç”¨ä¸Šæ¬¡æª¢æ¸¬çµæœ\n",
    "                detections = self.last_detections\n",
    "            else:\n",
    "                # åŸ·è¡Œæ–°æª¢æ¸¬\n",
    "                self.frame_skip = 0\n",
    "                detections = self._detect_faces(frame)\n",
    "                self.last_detections = detections\n",
    "        else:\n",
    "            detections = self._detect_faces(frame)\n",
    "        \n",
    "        # ç¹ªè£½æª¢æ¸¬çµæœ\n",
    "        result_frame = self._draw_detections(frame.copy(), detections)\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # è¨˜éŒ„è™•ç†æ™‚é–“\n",
    "        self.detection_history.append(processing_time)\n",
    "        if len(self.detection_history) > 30:  # ä¿æŒæœ€è¿‘30å¹€çš„è¨˜éŒ„\n",
    "            self.detection_history.pop(0)\n",
    "        \n",
    "        return result_frame, detections, processing_time\n",
    "    \n",
    "    def _detect_faces(self, frame):\n",
    "        \"\"\"åŸ·è¡Œäººè‡‰æª¢æ¸¬\"\"\"\n",
    "        if self.detector_type == 'dnn':\n",
    "            return self.detector.detect_faces(frame)\n",
    "        elif self.detector_type == 'haar':\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.detector.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "            return [(x, y, w, h, 1.0) for (x, y, w, h) in faces]\n",
    "        return []\n",
    "    \n",
    "    def _draw_detections(self, frame, detections):\n",
    "        \"\"\"ç¹ªè£½æª¢æ¸¬çµæœ\"\"\"\n",
    "        for detection in detections:\n",
    "            if len(detection) >= 4:\n",
    "                x, y, w, h = detection[:4]\n",
    "                confidence = detection[4] if len(detection) > 4 else 1.0\n",
    "                \n",
    "                # ç¹ªè£½é‚Šç•Œæ¡†\n",
    "                color = (0, 255, 0) if confidence > 0.7 else (0, 255, 255)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # é¡¯ç¤ºä¿¡å¿ƒåº¦\n",
    "                if confidence < 1.0:\n",
    "                    cv2.putText(frame, f'{confidence:.2f}', (x, y-10),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def get_average_fps(self):\n",
    "        \"\"\"è¨ˆç®—å¹³å‡FPS\"\"\"\n",
    "        if len(self.detection_history) == 0:\n",
    "            return 0\n",
    "        \n",
    "        avg_time_ms = np.mean(self.detection_history)\n",
    "        return 1000.0 / avg_time_ms if avg_time_ms > 0 else 0\n",
    "\n",
    "def demonstrate_realtime_optimization():\n",
    "    \"\"\"æ¼”ç¤ºå¯¦æ™‚æª¢æ¸¬å„ªåŒ–æ•ˆæœ\"\"\"\n",
    "    print(\"\\nğŸ¥ å¯¦æ™‚æª¢æ¸¬å„ªåŒ–æ¼”ç¤º\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # æ¸¬è©¦ä¸åŒçš„å„ªåŒ–ç­–ç•¥\n",
    "    test_image_path = \"../assets/images/basic/faces01.jpg\"\n",
    "    if not os.path.exists(test_image_path):\n",
    "        print(f\"æ¸¬è©¦åœ–ç‰‡ä¸å­˜åœ¨: {test_image_path}\")\n",
    "        return\n",
    "    \n",
    "    image = load_image(test_image_path)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    # èª¿æ•´åœ–ç‰‡å¤§å°æ¨¡æ“¬ä¸åŒè§£æåº¦\n",
    "    test_sizes = [(640, 480), (320, 240), (160, 120)]\n",
    "    \n",
    "    results = []\n",
    "    titles = []\n",
    "    \n",
    "    for width, height in test_sizes:\n",
    "        # èª¿æ•´åœ–ç‰‡å¤§å°\n",
    "        resized = cv2.resize(image, (width, height))\n",
    "        \n",
    "        # å‰µå»ºå¯¦æ™‚æª¢æ¸¬å™¨\n",
    "        detector_type = 'dnn' if dnn_detector.model_loaded else 'haar'\n",
    "        rt_detector = RealTimeDetector(detector_type)\n",
    "        \n",
    "        # æ¨¡æ“¬å¤šå¹€è™•ç†\n",
    "        frame_times = []\n",
    "        for i in range(10):  # è™•ç†10å¹€\n",
    "            processed_frame, detections, proc_time = rt_detector.process_frame(\n",
    "                resized, use_optimization=True\n",
    "            )\n",
    "            frame_times.append(proc_time)\n",
    "        \n",
    "        avg_time = np.mean(frame_times)\n",
    "        fps = 1000.0 / avg_time if avg_time > 0 else 0\n",
    "        \n",
    "        results.append(processed_frame)\n",
    "        titles.append(f\"{width}x{height}\\n{avg_time:.1f}ms, {fps:.1f}FPS\")\n",
    "        \n",
    "        print(f\"è§£æåº¦ {width}x{height}: å¹³å‡è™•ç†æ™‚é–“ {avg_time:.1f}ms, FPS {fps:.1f}\")\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    display_multiple_images(results, titles, figsize=(12, 4))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å¯¦æ™‚æª¢æ¸¬å„ªåŒ–å»ºè­°:\")\n",
    "    print(\"  1. é™ä½è¼¸å…¥è§£æåº¦ (320x240 é©åˆå¯¦æ™‚æ‡‰ç”¨)\")\n",
    "    print(\"  2. ä½¿ç”¨å¹€è·³èºæŠ€è¡“ (æ¯2-3å¹€æª¢æ¸¬ä¸€æ¬¡)\")\n",
    "    print(\"  3. é¸æ“‡åˆé©çš„æª¢æ¸¬å™¨ (Haaræœ€å¿«ï¼ŒDNNæœ€æº–ç¢º)\")\n",
    "    print(\"  4. è€ƒæ…®ä½¿ç”¨GPUåŠ é€Ÿ (å¦‚æœå¯ç”¨)\")\n",
    "\n",
    "# åŸ·è¡Œå¯¦æ™‚å„ªåŒ–æ¼”ç¤º\n",
    "demonstrate_realtime_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯¦ä½œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’1: è‡ªå®šç¾©ç‰©é«”æª¢æ¸¬æ¨¡å‹æ•´åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_custom_model(model_path, input_size, class_names):\n",
    "    \"\"\"\n",
    "    æ•´åˆè‡ªå®šç¾©çš„æ·±åº¦å­¸ç¿’æ¨¡å‹\n",
    "    \n",
    "    ç·´ç¿’ç›®æ¨™:\n",
    "    1. è¼‰å…¥ONNXæˆ–TensorFlowæ¨¡å‹\n",
    "    2. è™•ç†ä¸åŒçš„è¼¸å…¥é è™•ç†éœ€æ±‚\n",
    "    3. å¯¦ç¾å¾Œè™•ç†å’Œçµæœè§£æ\n",
    "    4. éŒ¯èª¤è™•ç†å’Œæ¨¡å‹é©—è­‰\n",
    "    \n",
    "    Args:\n",
    "        model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘\n",
    "        input_size: æ¨¡å‹è¼¸å…¥å¤§å° (width, height)\n",
    "        class_names: é¡åˆ¥åç¨±åˆ—è¡¨\n",
    "    \n",
    "    æç¤º:\n",
    "    - ä½¿ç”¨cv2.dnn.readNetFromONNX()è¼‰å…¥ONNXæ¨¡å‹\n",
    "    - æ³¨æ„ä¸åŒæ¨¡å‹çš„æ­£è¦åŒ–è¦æ±‚\n",
    "    - å¯¦ç¾æ¨¡å‹è¼¸å‡ºçš„å¾Œè™•ç†é‚è¼¯\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œè‡ªå®šç¾©æ¨¡å‹æ•´åˆ\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ’¡ ç·´ç¿’1: è«‹å¯¦ä½œè‡ªå®šç¾©ç‰©é«”æª¢æ¸¬æ¨¡å‹æ•´åˆåŠŸèƒ½\")\nprint(\"æç¤º: å¯ä»¥ä½¿ç”¨ cv2.dnn.readNetFromONNX() è¼‰å…¥ ONNX æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’2: æ‰¹é‡å½±ç‰‡è™•ç†ç®¡é“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_processing_pipeline(input_video_path, output_video_path, detector_type='dnn'):\n",
    "    \"\"\"\n",
    "    å‰µå»ºå½±ç‰‡è™•ç†ç®¡é“\n",
    "    \n",
    "    ç·´ç¿’ç›®æ¨™:\n",
    "    1. è®€å–å½±ç‰‡æª”æ¡ˆ\n",
    "    2. å°æ¯ä¸€å¹€åŸ·è¡Œç‰©é«”æª¢æ¸¬\n",
    "    3. ç¹ªè£½æª¢æ¸¬çµæœ\n",
    "    4. è¼¸å‡ºè™•ç†å¾Œçš„å½±ç‰‡\n",
    "    5. é¡¯ç¤ºè™•ç†é€²åº¦\n",
    "    \n",
    "    Args:\n",
    "        input_video_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘\n",
    "        output_video_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘\n",
    "        detector_type: æª¢æ¸¬å™¨é¡å‹\n",
    "    \n",
    "    è¦æ±‚:\n",
    "    - æ”¯æ´ä¸åŒå½±ç‰‡æ ¼å¼\n",
    "    - ä¿æŒåŸå§‹å½±ç‰‡çš„å¹€ç‡å’Œè§£æåº¦\n",
    "    - å¯¦ç¾é€²åº¦æ¢é¡¯ç¤º\n",
    "    - éŒ¯èª¤è™•ç†å’Œè³‡æºæ¸…ç†\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå½±ç‰‡è™•ç†ç®¡é“\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ’¡ ç·´ç¿’2: è«‹å¯¦ä½œæ‰¹é‡å½±ç‰‡è™•ç†ç®¡é“\")\nprint(\"æç¤º: ä½¿ç”¨ cv2.VideoCapture() å’Œ cv2.VideoWriter()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç¸½çµèˆ‡æ·±åº¦å­¸ç¿’è¶¨å‹¢\n",
    "\n",
    "### æœ¬æ¨¡çµ„é‡é»å›é¡§\n",
    "\n",
    "1. **OpenCV DNNæ¨¡çµ„**: æ•´åˆå¤šç¨®æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "2. **é è¨“ç·´æ¨¡å‹**: TensorFlowã€PyTorchã€ONNXæ¨¡å‹è¼‰å…¥\n",
    "3. **æ•ˆèƒ½å„ªåŒ–**: å¯¦æ™‚æª¢æ¸¬çš„å„ªåŒ–ç­–ç•¥\n",
    "4. **æ¨¡å‹æ¯”è¼ƒ**: å‚³çµ±æ–¹æ³•vsæ·±åº¦å­¸ç¿’çš„å–æ¨\n",
    "\n",
    "### æ·±åº¦å­¸ç¿’åœ¨é›»è…¦è¦–è¦ºçš„å„ªå‹¢\n",
    "- **é«˜ç²¾åº¦**: CNNæ¨¡å‹åœ¨è¤‡é›œå ´æ™¯ä¸‹è¡¨ç¾å„ªç•°\n",
    "- **é­¯æ£’æ€§**: å°å…‰ç…§ã€è§’åº¦è®ŠåŒ–æ›´ç©©å®š\n",
    "- **æ³›åŒ–èƒ½åŠ›**: é è¨“ç·´æ¨¡å‹é©æ‡‰æ€§å¼·\n",
    "- **ç«¯åˆ°ç«¯**: ç„¡éœ€æ‰‹å·¥ç‰¹å¾µå·¥ç¨‹\n",
    "\n",
    "### å¯¦éš›æ‡‰ç”¨è€ƒé‡\n",
    "- **è¨ˆç®—è³‡æº**: GPU vs CPUçš„æ€§èƒ½å·®ç•°\n",
    "- **éƒ¨ç½²ç’°å¢ƒ**: é‚Šç·£è¨­å‚™vsé›²ç«¯æœå‹™\n",
    "- **ç²¾åº¦è¦æ±‚**: å®‰å…¨æ€§æ‡‰ç”¨vsä¸€èˆ¬å¨›æ¨‚æ‡‰ç”¨\n",
    "- **å¯¦æ™‚æ€§**: å»¶é²æ•æ„Ÿçš„æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "### æœªä¾†ç™¼å±•è¶¨å‹¢\n",
    "- **æ¨¡å‹è¼•é‡åŒ–**: MobileNetã€EfficientNet\n",
    "- **é‚Šç·£é‹ç®—**: ONNX Runtimeã€TensorRT\n",
    "- **è‡ªç›£ç£å­¸ç¿’**: ç„¡æ¨™ç±¤æ•¸æ“šçš„åˆ©ç”¨\n",
    "- **å¤šæ¨¡æ…‹èåˆ**: è¦–è¦º+èªè¨€çš„æ•´åˆ\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘\n",
    "- 6.1 ç·´ç¿’ç³»çµ± - ä¸­ç´šæŒ‘æˆ°\n",
    "- 7.1 å¯¦æˆ°å°ˆæ¡ˆ - æ™ºèƒ½ç›£æ§ç³»çµ±\n",
    "- æ·±å…¥å­¸ç¿’ç‰¹å®šé ˜åŸŸ (é†«å­¸å½±åƒã€è‡ªå‹•é§•é§›ç­‰)\n",
    "- åƒèˆ‡é–‹æºå°ˆæ¡ˆå’Œç«¶è³½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æœ€çµ‚æ•ˆèƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"åŸ·è¡Œå…¨é¢çš„DNNæ¨¡çµ„åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ”„ åŸ·è¡ŒDNNæ¨¡çµ„å…¨é¢åŸºæº–æ¸¬è©¦\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"../assets/images/basic/face03.jpg\", \"å–®äººè‡‰\"),\n",
    "        (\"../assets/images/basic/faces01.jpg\", \"å¤šäººè‡‰\"),\n",
    "        (\"../assets/images/basic/faces.png\", \"ç¾¤é«”ç…§ç‰‡\")\n",
    "    ]\n",
    "    \n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "    \n",
    "    for image_path, description in test_cases:\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nğŸ“· æ¸¬è©¦: {description} ({os.path.basename(image_path)})\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        image = load_image(image_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        total_tests += 1\n",
    "        \n",
    "        # èª¿æ•´åœ–ç‰‡å¤§å°\n",
    "        image_resized = resize_image(image, max_width=640)\n",
    "        \n",
    "        # DNNæª¢æ¸¬æ¸¬è©¦\n",
    "        if dnn_detector.model_loaded:\n",
    "            faces, detection_time = detect_faces_dnn_timed(image_resized)\n",
    "            print(f\"  âœ… DNNæª¢æ¸¬: {len(faces)} å¼µäººè‡‰, {detection_time:.1f}ms\")\n",
    "            \n",
    "            # è©³ç´°çµæœ\n",
    "            for i, (x, y, w, h, conf) in enumerate(faces, 1):\n",
    "                print(f\"     äººè‡‰{i}: ä½ç½®({x:3d},{y:3d}) å¤§å°{w:3d}x{h:3d} ä¿¡å¿ƒåº¦{conf:.3f}\")\n",
    "            \n",
    "            successful_tests += 1\n",
    "        else:\n",
    "            print(\"  âŒ DNNæ¨¡å‹æœªè¼‰å…¥\")\n",
    "        \n",
    "        # å¯¦æ™‚æ€§èƒ½æ¸¬è©¦\n",
    "        rt_detector = RealTimeDetector('dnn' if dnn_detector.model_loaded else 'haar')\n",
    "        \n",
    "        frame_times = []\n",
    "        for _ in range(5):  # æ¸¬è©¦5å¹€\n",
    "            _, _, proc_time = rt_detector.process_frame(image_resized)\n",
    "            frame_times.append(proc_time)\n",
    "        \n",
    "        avg_time = np.mean(frame_times)\n",
    "        fps = 1000.0 / avg_time if avg_time > 0 else 0\n",
    "        print(f\"  ğŸ“Š å¯¦æ™‚æ€§èƒ½: å¹³å‡{avg_time:.1f}ms/å¹€, {fps:.1f} FPS\")\n",
    "        \n",
    "        # æ€§èƒ½è©•ç´š\n",
    "        if fps >= 30:\n",
    "            rating = \"å„ªç§€ (é©åˆå¯¦æ™‚æ‡‰ç”¨)\"\n",
    "        elif fps >= 15:\n",
    "            rating = \"è‰¯å¥½ (é©åˆäº’å‹•æ‡‰ç”¨)\"\n",
    "        elif fps >= 5:\n",
    "            rating = \"ä¸€èˆ¬ (é©åˆæ‰¹è™•ç†)\"\n",
    "        else:\n",
    "            rating = \"éœ€è¦å„ªåŒ–\"\n",
    "        \n",
    "        print(f\"  ğŸ† æ€§èƒ½è©•ç´š: {rating}\")\n",
    "    \n",
    "    # ç¸½çµ\n",
    "    print(f\"\\nğŸ“Š æ¸¬è©¦ç¸½çµ\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"ç¸½æ¸¬è©¦æ•¸: {total_tests}\")\n",
    "    print(f\"æˆåŠŸæ¸¬è©¦: {successful_tests}\")\n",
    "    print(f\"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%\" if total_tests > 0 else \"ç„¡æœ‰æ•ˆæ¸¬è©¦\")\n",
    "    \n",
    "    # åŠŸèƒ½å®Œæˆåº¦æª¢æŸ¥\n",
    "    features = [\n",
    "        (\"DNNäººè‡‰æª¢æ¸¬\", dnn_detector.model_loaded),\n",
    "        (\"å¯¦æ™‚å„ªåŒ–\", True),  # å·²å¯¦ç¾\n",
    "        (\"å¤šæ¨¡å‹æ¯”è¼ƒ\", True),  # å·²å¯¦ç¾\n",
    "        (\"æ•ˆèƒ½åŸºæº–æ¸¬è©¦\", True)  # æ­£åœ¨åŸ·è¡Œ\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nâœ… æ¨¡çµ„åŠŸèƒ½å®Œæˆåº¦:\")\n",
    "    for feature, completed in features:\n",
    "        status = \"âœ…\" if completed else \"âŒ\"\n",
    "        print(f\"  {status} {feature}\")\n",
    "    \n",
    "    completed_count = sum(1 for _, completed in features if completed)\n",
    "    completion_rate = completed_count / len(features) * 100\n",
    "    print(f\"\\nğŸ¯ æ•´é«”å®Œæˆåº¦: {completion_rate:.1f}% ({completed_count}/{len(features)})\")\n",
    "    \n",
    "    if completion_rate >= 75:\n",
    "        print(\"ğŸ‰ DNNæ•´åˆæ¨¡çµ„é–‹ç™¼å®Œæˆï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦é€²ä¸€æ­¥å®Œå–„\")\n",
    "\n",
    "# åŸ·è¡Œå…¨é¢åŸºæº–æ¸¬è©¦\n",
    "run_comprehensive_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}