{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.1 OpenCV DNN 模組整合 (OpenCV DNN Module Integration)\n",
    "\n",
    "**WBS 5.2.1**: 深度學習模型整合與推理\n",
    "\n",
    "本模組深入探討 OpenCV DNN 模組的使用，學習如何載入並運行來自不同深度學習框架的預訓練模型。\n",
    "\n",
    "## 學習目標\n",
    "- 理解 OpenCV DNN 模組的架構與優勢\n",
    "- 掌握多種深度學習框架模型的載入方法\n",
    "- 實作人臉檢測、物體檢測、圖像分類、語義分割\n",
    "- 學習模型推理優化技巧\n",
    "- 比較不同 backend 和 target 的性能差異\n",
    "\n",
    "## 前置知識\n",
    "- 深度學習基礎概念\n",
    "- OpenCV 圖像處理 (Stage 2 & 3)\n",
    "- Python 與 NumPy\n",
    "- 傳統機器學習方法 (WBS 5.1)\n",
    "\n",
    "## 課程大綱\n",
    "1. OpenCV DNN 模組簡介 (5%)\n",
    "2. 支持的深度學習框架 (10%)\n",
    "3. 載入預訓練模型 (15%)\n",
    "4. 人臉檢測 (15%)\n",
    "5. 物體檢測 (20%)\n",
    "6. 圖像分類 (10%)\n",
    "7. 語義分割 (10%)\n",
    "8. 性能優化 (10%)\n",
    "9. 實戰練習 (3%)\n",
    "10. 總結與延伸 (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configure matplotlib for Chinese display\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Disable warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"OpenCV DNN module available: {hasattr(cv2, 'dnn')}\")\n",
    "\n",
    "# Check available backends and targets\n",
    "print(\"\\nAvailable DNN backends:\")\n",
    "backends = [\n",
    "    (\"DEFAULT\", cv2.dnn.DNN_BACKEND_DEFAULT),\n",
    "    (\"HALIDE\", cv2.dnn.DNN_BACKEND_HALIDE),\n",
    "    (\"INFERENCE_ENGINE\", cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE),\n",
    "    (\"OPENCV\", cv2.dnn.DNN_BACKEND_OPENCV),\n",
    "    (\"VKCOM\", cv2.dnn.DNN_BACKEND_VKCOM),\n",
    "    (\"CUDA\", cv2.dnn.DNN_BACKEND_CUDA)\n",
    "]\n",
    "\n",
    "for name, backend in backends:\n",
    "    print(f\"  - {name}: {backend}\")\n",
    "\n",
    "print(\"\\nAvailable DNN targets:\")\n",
    "targets = [\n",
    "    (\"CPU\", cv2.dnn.DNN_TARGET_CPU),\n",
    "    (\"OPENCL\", cv2.dnn.DNN_TARGET_OPENCL),\n",
    "    (\"OPENCL_FP16\", cv2.dnn.DNN_TARGET_OPENCL_FP16),\n",
    "    (\"MYRIAD\", cv2.dnn.DNN_TARGET_MYRIAD),\n",
    "    (\"CUDA\", cv2.dnn.DNN_TARGET_CUDA),\n",
    "    (\"CUDA_FP16\", cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "]\n",
    "\n",
    "for name, target in targets:\n",
    "    print(f\"  - {name}: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenCV DNN 模組簡介 (Introduction) - 5%\n",
    "\n",
    "### 什麼是 OpenCV DNN 模組？\n",
    "\n",
    "**OpenCV DNN (Deep Neural Networks) 模組**是 OpenCV 3.3 版本引入的深度學習推理引擎，讓開發者能夠直接在 OpenCV 中運行深度學習模型。\n",
    "\n",
    "### 核心優勢\n",
    "\n",
    "1. **統一接口**: 單一 API 支持多種深度學習框架\n",
    "2. **零依賴**: 不需要安裝 TensorFlow、PyTorch 等框架\n",
    "3. **跨平台**: 支援 Windows、Linux、macOS、Android、iOS\n",
    "4. **高效率**: 針對推理優化，支援多種硬體加速\n",
    "5. **輕量級**: 模型推理無需完整深度學習框架\n",
    "\n",
    "### OpenCV DNN vs 深度學習框架\n",
    "\n",
    "| 特性 | OpenCV DNN | TensorFlow/PyTorch |\n",
    "|------|-----------|-------------------|\n",
    "| **用途** | 推理 (Inference) | 訓練 + 推理 |\n",
    "| **安裝** | 內建於 OpenCV | 需額外安裝 |\n",
    "| **大小** | 輕量 (~50MB) | 重量 (>500MB) |\n",
    "| **速度** | 優化過的推理 | 通用框架 |\n",
    "| **部署** | 簡單 | 複雜 |\n",
    "| **GPU支援** | CUDA, OpenCL | CUDA, ROCm |\n",
    "\n",
    "### 適用場景\n",
    "\n",
    "✅ **適合使用 OpenCV DNN**:\n",
    "- 僅需模型推理，不需訓練\n",
    "- 希望減少依賴和部署複雜度\n",
    "- 嵌入式或移動設備部署\n",
    "- 與 OpenCV 視覺管線整合\n",
    "\n",
    "❌ **不適合使用 OpenCV DNN**:\n",
    "- 需要訓練模型\n",
    "- 需要最新的模型架構\n",
    "- 需要自定義層或複雜操作\n",
    "\n",
    "### DNN 模組架構\n",
    "\n",
    "```\n",
    "Model Loading → Preprocessing → Forward Pass → Post-processing\n",
    "     ↓              ↓              ↓               ↓\n",
    "readNet()    blobFromImage()   net.forward()   NMS, decode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 支持的深度學習框架 (Supported Frameworks) - 10%\n",
    "\n",
    "### 支援的框架與格式\n",
    "\n",
    "OpenCV DNN 模組支援多種深度學習框架導出的模型格式。\n",
    "\n",
    "#### 1. Caffe\n",
    "- **文件**: `.caffemodel` (weights) + `.prototxt` (architecture)\n",
    "- **載入**: `cv2.dnn.readNetFromCaffe(prototxt, caffemodel)`\n",
    "- **優點**: 早期主流框架，模型豐富\n",
    "- **應用**: 人臉檢測、影像分類、語義分割\n",
    "\n",
    "```python\n",
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'model.caffemodel')\n",
    "```\n",
    "\n",
    "#### 2. TensorFlow\n",
    "- **文件**: `.pb` (frozen graph) 或 SavedModel 格式\n",
    "- **載入**: `cv2.dnn.readNetFromTensorflow(model, config)`\n",
    "- **優點**: Google 支援，模型最多\n",
    "- **應用**: 物體檢測 (SSD, Faster R-CNN)、影像分類\n",
    "\n",
    "```python\n",
    "net = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb', 'config.pbtxt')\n",
    "```\n",
    "\n",
    "#### 3. PyTorch\n",
    "- **文件**: `.onnx` (需先轉換)\n",
    "- **載入**: `cv2.dnn.readNetFromONNX(onnx_file)`\n",
    "- **優點**: 研究界主流，模型創新快\n",
    "- **應用**: 最新研究模型\n",
    "\n",
    "```python\n",
    "# PyTorch model must be exported to ONNX first\n",
    "# torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
    "net = cv2.dnn.readNetFromONNX('model.onnx')\n",
    "```\n",
    "\n",
    "#### 4. ONNX\n",
    "- **文件**: `.onnx` (open format)\n",
    "- **載入**: `cv2.dnn.readNetFromONNX(onnx_file)`\n",
    "- **優點**: 跨框架標準格式\n",
    "- **應用**: 框架間模型轉換\n",
    "\n",
    "```python\n",
    "net = cv2.dnn.readNetFromONNX('model.onnx')\n",
    "```\n",
    "\n",
    "#### 5. Darknet (YOLO)\n",
    "- **文件**: `.weights` (weights) + `.cfg` (config)\n",
    "- **載入**: `cv2.dnn.readNetFromDarknet(cfg, weights)`\n",
    "- **優點**: YOLO 系列專用\n",
    "- **應用**: 實時物體檢測\n",
    "\n",
    "```python\n",
    "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "```\n",
    "\n",
    "#### 6. Torch (Legacy)\n",
    "- **文件**: `.t7` (Lua Torch)\n",
    "- **載入**: `cv2.dnn.readNetFromTorch(model)`\n",
    "- **優點**: 老舊模型相容\n",
    "- **應用**: 風格轉換等舊模型\n",
    "\n",
    "### 通用載入函數\n",
    "\n",
    "```python\n",
    "# Automatically detect framework\n",
    "net = cv2.dnn.readNet('model_file', 'config_file')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework comparison table\n",
    "framework_info = {\n",
    "    \"Framework\": [\"Caffe\", \"TensorFlow\", \"PyTorch (ONNX)\", \"ONNX\", \"Darknet (YOLO)\", \"Torch (Legacy)\"],\n",
    "    \"File Format\": [\n",
    "        \".caffemodel + .prototxt\",\n",
    "        \".pb + .pbtxt\",\n",
    "        \".onnx\",\n",
    "        \".onnx\",\n",
    "        \".weights + .cfg\",\n",
    "        \".t7\"\n",
    "    ],\n",
    "    \"Load Function\": [\n",
    "        \"readNetFromCaffe()\",\n",
    "        \"readNetFromTensorflow()\",\n",
    "        \"readNetFromONNX()\",\n",
    "        \"readNetFromONNX()\",\n",
    "        \"readNetFromDarknet()\",\n",
    "        \"readNetFromTorch()\"\n",
    "    ],\n",
    "    \"Typical Use\": [\n",
    "        \"Face detection, Classification\",\n",
    "        \"Object detection (SSD)\",\n",
    "        \"Latest research models\",\n",
    "        \"Cross-framework models\",\n",
    "        \"YOLO object detection\",\n",
    "        \"Style transfer (old)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nSupported Deep Learning Frameworks:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Framework':<20} {'File Format':<30} {'Load Function':<30} {'Typical Use':<40}\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "for i in range(len(framework_info[\"Framework\"])):\n",
    "    print(f\"{framework_info['Framework'][i]:<20} \"\n",
    "          f\"{framework_info['File Format'][i]:<30} \"\n",
    "          f\"{framework_info['Load Function'][i]:<30} \"\n",
    "          f\"{framework_info['Typical Use'][i]:<40}\")\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 載入預訓練模型 (Loading Models) - 15%\n",
    "\n",
    "### 模型下載與準備\n",
    "\n",
    "在使用 OpenCV DNN 之前，我們需要下載預訓練模型。\n",
    "\n",
    "### 常用模型資源\n",
    "\n",
    "1. **OpenCV Model Zoo**: https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API\n",
    "2. **Caffe Model Zoo**: https://github.com/BVLC/caffe/wiki/Model-Zoo\n",
    "3. **ONNX Model Zoo**: https://github.com/onnx/models\n",
    "4. **Darknet (YOLO)**: https://pjreddie.com/darknet/yolo/\n",
    "5. **TensorFlow Hub**: https://www.tensorflow.org/hub\n",
    "\n",
    "### Model Download Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model download utility\n",
    "class ModelDownloader:\n",
    "    \"\"\"\n",
    "    Utility class for downloading pre-trained models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='../assets/models'):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def download(self, url, filename, force=False):\n",
    "        \"\"\"\n",
    "        Download file from URL\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        url : str\n",
    "            URL to download from\n",
    "        filename : str\n",
    "            Local filename to save\n",
    "        force : bool\n",
    "            Force re-download if file exists\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        if filepath.exists() and not force:\n",
    "            print(f\"✓ {filename} already exists\")\n",
    "            return str(filepath)\n",
    "        \n",
    "        print(f\"Downloading {filename}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, str(filepath))\n",
    "            print(f\"✓ Downloaded {filename} ({filepath.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "            return str(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to download {filename}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize downloader\n",
    "downloader = ModelDownloader()\n",
    "\n",
    "# Model URLs (these are examples - actual URLs may change)\n",
    "MODEL_URLS = {\n",
    "    # Face detection (Caffe)\n",
    "    'face_detector_prototxt': 'https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt',\n",
    "    'face_detector_model': 'https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel',\n",
    "    \n",
    "    # MobileNet-SSD (Caffe)\n",
    "    'mobilenet_prototxt': 'https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/deploy.prototxt',\n",
    "    'mobilenet_model': 'https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/mobilenet_iter_73000.caffemodel',\n",
    "}\n",
    "\n",
    "print(\"Available models for download:\")\n",
    "for key, url in MODEL_URLS.items():\n",
    "    print(f\"  - {key}: {url[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型載入示例\n",
    "\n",
    "讓我們示範如何載入不同類型的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_safe(model_type, model_path, config_path=None):\n",
    "    \"\"\"\n",
    "    Safely load DNN model with error handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_type : str\n",
    "        Type of model ('caffe', 'tensorflow', 'onnx', 'darknet', 'torch')\n",
    "    model_path : str\n",
    "        Path to model weights file\n",
    "    config_path : str, optional\n",
    "        Path to model configuration file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    net : cv2.dnn.Net or None\n",
    "        Loaded network or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_type.lower() == 'caffe':\n",
    "            if config_path is None:\n",
    "                raise ValueError(\"Caffe requires config_path (prototxt)\")\n",
    "            net = cv2.dnn.readNetFromCaffe(config_path, model_path)\n",
    "            \n",
    "        elif model_type.lower() == 'tensorflow':\n",
    "            if config_path:\n",
    "                net = cv2.dnn.readNetFromTensorflow(model_path, config_path)\n",
    "            else:\n",
    "                net = cv2.dnn.readNetFromTensorflow(model_path)\n",
    "                \n",
    "        elif model_type.lower() == 'onnx':\n",
    "            net = cv2.dnn.readNetFromONNX(model_path)\n",
    "            \n",
    "        elif model_type.lower() == 'darknet':\n",
    "            if config_path is None:\n",
    "                raise ValueError(\"Darknet requires config_path (.cfg)\")\n",
    "            net = cv2.dnn.readNetFromDarknet(config_path, model_path)\n",
    "            \n",
    "        elif model_type.lower() == 'torch':\n",
    "            net = cv2.dnn.readNetFromTorch(model_path)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_type} model from {Path(model_path).name}\")\n",
    "        \n",
    "        # Print network information\n",
    "        layer_names = net.getLayerNames()\n",
    "        print(f\"  Total layers: {len(layer_names)}\")\n",
    "        print(f\"  Input layers: {net.getUnconnectedOutLayers()}\")\n",
    "        \n",
    "        return net\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {model_type} model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Load model using generic readNet (auto-detect format)\n",
    "print(\"\\nGeneric model loading (auto-detect):\")\n",
    "print(\"cv2.dnn.readNet() automatically detects the framework based on file extensions\")\n",
    "print(\"Example: net = cv2.dnn.readNet('model.caffemodel', 'deploy.prototxt')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_network(net, verbose=False):\n",
    "    \"\"\"\n",
    "    Inspect DNN network structure\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    net : cv2.dnn.Net\n",
    "        Loaded network\n",
    "    verbose : bool\n",
    "        Print detailed layer information\n",
    "    \"\"\"\n",
    "    if net is None:\n",
    "        print(\"Network is None\")\n",
    "        return\n",
    "    \n",
    "    # Get layer information\n",
    "    layer_names = net.getLayerNames()\n",
    "    layer_ids = net.getUnconnectedOutLayers()\n",
    "    \n",
    "    print(f\"\\nNetwork Structure:\")\n",
    "    print(f\"  Total layers: {len(layer_names)}\")\n",
    "    print(f\"  Output layers: {len(layer_ids)}\")\n",
    "    \n",
    "    # Get output layer names\n",
    "    output_layers = [layer_names[i - 1] for i in layer_ids]\n",
    "    print(f\"  Output layer names: {output_layers}\")\n",
    "    \n",
    "    if verbose and len(layer_names) < 50:\n",
    "        print(f\"\\n  All layers:\")\n",
    "        for i, name in enumerate(layer_names[:20], 1):\n",
    "            print(f\"    {i:3d}. {name}\")\n",
    "        if len(layer_names) > 20:\n",
    "            print(f\"    ... ({len(layer_names) - 20} more layers)\")\n",
    "\n",
    "print(\"Network inspection utility defined.\")\n",
    "print(\"Use inspect_network(net, verbose=True) to see detailed layer information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 人臉檢測 (Face Detection with DNN) - 15%\n",
    "\n",
    "### 使用 ResNet-based SSD 進行人臉檢測\n",
    "\n",
    "OpenCV 提供基於 ResNet-10 的 SSD 人臉檢測模型，相比 Haar Cascade 有更高的準確度。\n",
    "\n",
    "### 模型特點\n",
    "- **架構**: Single Shot Detector (SSD) with ResNet-10 backbone\n",
    "- **輸入**: 300×300 RGB image\n",
    "- **輸出**: Face bounding boxes with confidence scores\n",
    "- **優勢**: 比 Haar Cascade 更準確，處理側臉和遮擋更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup face detection model paths\n",
    "FACE_MODEL_DIR = Path('../assets/models/face_detection')\n",
    "FACE_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "face_prototxt = FACE_MODEL_DIR / 'deploy.prototxt'\n",
    "face_model = FACE_MODEL_DIR / 'res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "\n",
    "# Check if models exist\n",
    "if face_prototxt.exists() and face_model.exists():\n",
    "    print(\"✓ Face detection models found\")\n",
    "    FACE_MODEL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"⚠ Face detection models not found\")\n",
    "    print(\"Please download:\")\n",
    "    print(\"  1. deploy.prototxt\")\n",
    "    print(\"  2. res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n",
    "    print(f\"  Save to: {FACE_MODEL_DIR}\")\n",
    "    print(\"\\nDownload from: https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\")\n",
    "    FACE_MODEL_AVAILABLE = False\n",
    "\n",
    "if FACE_MODEL_AVAILABLE:\n",
    "    # Load face detection model\n",
    "    face_net = load_model_safe('caffe', str(face_model), str(face_prototxt))\n",
    "    \n",
    "    if face_net is not None:\n",
    "        # Set default backend and target\n",
    "        face_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        face_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "        print(\"\\n✓ Face detection model ready\")\n",
    "        inspect_network(face_net, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN 人臉檢測實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_dnn(image, net, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect faces using DNN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image (BGR)\n",
    "    net : cv2.dnn.Net\n",
    "        Face detection network\n",
    "    confidence_threshold : float\n",
    "        Minimum confidence for detection (0.0-1.0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    detections : list\n",
    "        List of (x, y, w, h, confidence) tuples\n",
    "    elapsed : float\n",
    "        Detection time in seconds\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Prepare blob from image\n",
    "    # Mean values from ImageNet dataset\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)),\n",
    "        scalefactor=1.0,\n",
    "        size=(300, 300),\n",
    "        mean=(104.0, 177.0, 123.0),\n",
    "        swapRB=False,\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    net.setInput(blob)\n",
    "    detections_raw = net.forward()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Parse detections\n",
    "    detections = []\n",
    "    \n",
    "    for i in range(detections_raw.shape[2]):\n",
    "        confidence = detections_raw[0, 0, i, 2]\n",
    "        \n",
    "        if confidence > confidence_threshold:\n",
    "            # Get bounding box coordinates\n",
    "            box = detections_raw[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "            \n",
    "            # Convert to (x, y, w, h) format\n",
    "            x = max(0, x1)\n",
    "            y = max(0, y1)\n",
    "            width = min(w - x, x2 - x1)\n",
    "            height = min(h - y, y2 - y1)\n",
    "            \n",
    "            detections.append((x, y, width, height, confidence))\n",
    "    \n",
    "    return detections, elapsed\n",
    "\n",
    "\n",
    "def draw_faces_with_confidence(image, detections, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw face bounding boxes with confidence scores\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    detections : list\n",
    "        List of (x, y, w, h, confidence) tuples\n",
    "    color : tuple\n",
    "        Box color (B, G, R)\n",
    "    thickness : int\n",
    "        Line thickness\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Image with drawn boxes\n",
    "    \"\"\"\n",
    "    output = image.copy()\n",
    "    \n",
    "    for i, (x, y, w, h, conf) in enumerate(detections):\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Draw label with confidence\n",
    "        label = f\"Face {i+1}: {conf:.2f}\"\n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        \n",
    "        # Background for text\n",
    "        cv2.rectangle(output, \n",
    "                     (x, y - label_size[1] - 10), \n",
    "                     (x + label_size[0], y), \n",
    "                     color, -1)\n",
    "        \n",
    "        # Text\n",
    "        cv2.putText(output, label, (x, y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"Face detection functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試人臉檢測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FACE_MODEL_AVAILABLE and face_net is not None:\n",
    "    # Load test image\n",
    "    test_image_paths = [\n",
    "        '../assets/images/basic/baby.jpg',\n",
    "        '../assets/images/basic/baby01.jpg',\n",
    "        '../assets/images/faces/face01.jpg'\n",
    "    ]\n",
    "    \n",
    "    # Find first existing image\n",
    "    test_img = None\n",
    "    for path in test_image_paths:\n",
    "        if Path(path).exists():\n",
    "            test_img = cv2.imread(path)\n",
    "            print(f\"Loaded test image: {path}\")\n",
    "            break\n",
    "    \n",
    "    if test_img is None:\n",
    "        # Create synthetic image\n",
    "        test_img = np.ones((480, 640, 3), dtype=np.uint8) * 200\n",
    "        cv2.putText(test_img, \"No test image found\", (150, 240),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "        print(\"Created synthetic test image\")\n",
    "    \n",
    "    # Detect faces\n",
    "    detections, elapsed = detect_faces_dnn(test_img, face_net, confidence_threshold=0.5)\n",
    "    \n",
    "    # Draw results\n",
    "    result_img = draw_faces_with_confidence(test_img, detections)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'DNN Face Detection\\n'\n",
    "                     f'Detected: {len(detections)} faces, '\n",
    "                     f'Time: {elapsed*1000:.2f}ms',\n",
    "                     fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print details\n",
    "    print(f\"\\nDetection Results:\")\n",
    "    print(f\"  Faces detected: {len(detections)}\")\n",
    "    print(f\"  Inference time: {elapsed*1000:.2f}ms\")\n",
    "    print(f\"\\nDetection details:\")\n",
    "    for i, (x, y, w, h, conf) in enumerate(detections, 1):\n",
    "        print(f\"  Face {i}: ({x}, {y}, {w}, {h}), confidence={conf:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Face detection model not available. Skipping demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar Cascade vs DNN 比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FACE_MODEL_AVAILABLE and face_net is not None and test_img is not None:\n",
    "    # Load Haar Cascade for comparison\n",
    "    haar_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    )\n",
    "    \n",
    "    # Haar detection\n",
    "    gray = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    start_time = time.time()\n",
    "    haar_faces = haar_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "    haar_time = time.time() - start_time\n",
    "    \n",
    "    # Draw Haar results\n",
    "    haar_result = test_img.copy()\n",
    "    for (x, y, w, h) in haar_faces:\n",
    "        cv2.rectangle(haar_result, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Compare\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(haar_result, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f'Haar Cascade\\n'\n",
    "                     f'Detected: {len(haar_faces)} faces, '\n",
    "                     f'Time: {haar_time*1000:.2f}ms',\n",
    "                     fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'DNN (ResNet-SSD)\\n'\n",
    "                     f'Detected: {len(detections)} faces, '\n",
    "                     f'Time: {elapsed*1000:.2f}ms',\n",
    "                     fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Face Detection Comparison', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparison table\n",
    "    print(\"\\nMethod Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Method':<20} {'Faces':<10} {'Time (ms)':<15} {'Accuracy'}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Haar Cascade':<20} {len(haar_faces):<10} {haar_time*1000:<15.2f} {'Medium'}\")\n",
    "    print(f\"{'DNN (ResNet-SSD)':<20} {len(detections):<10} {elapsed*1000:<15.2f} {'High'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nKey differences:\")\n",
    "    print(\"  - DNN provides confidence scores for each detection\")\n",
    "    print(\"  - DNN handles side faces and occlusions better\")\n",
    "    print(\"  - DNN has fewer false positives\")\n",
    "    print(\"  - Haar is faster but less accurate\")\n",
    "else:\n",
    "    print(\"Comparison skipped: model or image not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 物體檢測 (Object Detection) - 20%\n",
    "\n",
    "### MobileNet-SSD 物體檢測\n",
    "\n",
    "MobileNet-SSD 是一個輕量級的物體檢測模型，適合實時應用。\n",
    "\n",
    "### 模型特點\n",
    "- **架構**: Single Shot Detector (SSD) with MobileNet backbone\n",
    "- **輸入**: 300×300 RGB image\n",
    "- **類別**: 20 object classes (PASCAL VOC)\n",
    "- **優勢**: 快速、輕量、適合移動設備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet-SSD class labels (PASCAL VOC)\n",
    "CLASSES = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "    \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
    "    \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\",\n",
    "    \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "# Color map for visualization\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "print(f\"MobileNet-SSD can detect {len(CLASSES) - 1} object classes:\")\n",
    "for i, cls in enumerate(CLASSES[1:], 1):\n",
    "    print(f\"  {i:2d}. {cls}\")\n",
    "\n",
    "# Setup object detection model paths\n",
    "OBJECT_MODEL_DIR = Path('../assets/models/object_detection')\n",
    "OBJECT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mobilenet_prototxt = OBJECT_MODEL_DIR / 'MobileNetSSD_deploy.prototxt'\n",
    "mobilenet_model = OBJECT_MODEL_DIR / 'MobileNetSSD_deploy.caffemodel'\n",
    "\n",
    "# Check if models exist\n",
    "if mobilenet_prototxt.exists() and mobilenet_model.exists():\n",
    "    print(\"\\n✓ MobileNet-SSD models found\")\n",
    "    OBJECT_MODEL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\n⚠ MobileNet-SSD models not found\")\n",
    "    print(\"Download from: https://github.com/chuanqi305/MobileNet-SSD\")\n",
    "    print(f\"Save to: {OBJECT_MODEL_DIR}\")\n",
    "    OBJECT_MODEL_AVAILABLE = False\n",
    "\n",
    "if OBJECT_MODEL_AVAILABLE:\n",
    "    # Load MobileNet-SSD model\n",
    "    object_net = load_model_safe('caffe', str(mobilenet_model), str(mobilenet_prototxt))\n",
    "    \n",
    "    if object_net is not None:\n",
    "        object_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        object_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "        print(\"\\n✓ Object detection model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 物體檢測實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image, net, confidence_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detect objects using MobileNet-SSD\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image (BGR)\n",
    "    net : cv2.dnn.Net\n",
    "        Object detection network\n",
    "    confidence_threshold : float\n",
    "        Minimum confidence for detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    detections : list\n",
    "        List of (class_id, class_name, confidence, x, y, w, h) tuples\n",
    "    elapsed : float\n",
    "        Detection time\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Prepare blob\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)),\n",
    "        scalefactor=0.007843,  # 1/127.5\n",
    "        size=(300, 300),\n",
    "        mean=127.5,\n",
    "        swapRB=True,\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    net.setInput(blob)\n",
    "    detections_raw = net.forward()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Parse detections\n",
    "    detections = []\n",
    "    \n",
    "    for i in range(detections_raw.shape[2]):\n",
    "        confidence = detections_raw[0, 0, i, 2]\n",
    "        \n",
    "        if confidence > confidence_threshold:\n",
    "            # Get class ID\n",
    "            class_id = int(detections_raw[0, 0, i, 1])\n",
    "            \n",
    "            # Get bounding box\n",
    "            box = detections_raw[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "            \n",
    "            x = max(0, x1)\n",
    "            y = max(0, y1)\n",
    "            width = min(w - x, x2 - x1)\n",
    "            height = min(h - y, y2 - y1)\n",
    "            \n",
    "            detections.append((\n",
    "                class_id,\n",
    "                CLASSES[class_id],\n",
    "                confidence,\n",
    "                x, y, width, height\n",
    "            ))\n",
    "    \n",
    "    return detections, elapsed\n",
    "\n",
    "\n",
    "def draw_detections(image, detections):\n",
    "    \"\"\"\n",
    "    Draw object detection results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    detections : list\n",
    "        List of detection tuples\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Image with drawn boxes and labels\n",
    "    \"\"\"\n",
    "    output = image.copy()\n",
    "    \n",
    "    for (class_id, class_name, confidence, x, y, w, h) in detections:\n",
    "        # Get color\n",
    "        color = COLORS[class_id].astype(int).tolist()\n",
    "        \n",
    "        # Draw box\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Prepare label\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        \n",
    "        # Draw label background\n",
    "        cv2.rectangle(output,\n",
    "                     (x, y - label_size[1] - 10),\n",
    "                     (x + label_size[0], y),\n",
    "                     color, -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(output, label, (x, y - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"Object detection functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試物體檢測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OBJECT_MODEL_AVAILABLE and object_net is not None:\n",
    "    # Load test image with objects\n",
    "    test_paths = [\n",
    "        '../assets/images/basic/assassin.jpg',\n",
    "        '../assets/images/objects/car.jpg',\n",
    "        '../assets/images/basic/1.jpg'\n",
    "    ]\n",
    "    \n",
    "    test_img = None\n",
    "    for path in test_paths:\n",
    "        if Path(path).exists():\n",
    "            test_img = cv2.imread(path)\n",
    "            print(f\"Loaded test image: {path}\")\n",
    "            break\n",
    "    \n",
    "    if test_img is None:\n",
    "        # Create demo image\n",
    "        test_img = np.ones((480, 640, 3), dtype=np.uint8) * 200\n",
    "        cv2.putText(test_img, \"Place test image here\", (150, 240),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "        print(\"Created demo image\")\n",
    "    \n",
    "    # Detect objects\n",
    "    detections, elapsed = detect_objects(test_img, object_net, confidence_threshold=0.3)\n",
    "    \n",
    "    # Draw results\n",
    "    result_img = draw_detections(test_img, detections)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'MobileNet-SSD Detection\\n'\n",
    "                     f'Detected: {len(detections)} objects, '\n",
    "                     f'Time: {elapsed*1000:.2f}ms',\n",
    "                     fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"\\nDetection Results:\")\n",
    "    print(f\"  Objects detected: {len(detections)}\")\n",
    "    print(f\"  Inference time: {elapsed*1000:.2f}ms\")\n",
    "    print(f\"\\nDetection details:\")\n",
    "    for i, (cid, cname, conf, x, y, w, h) in enumerate(detections, 1):\n",
    "        print(f\"  {i}. {cname}: confidence={conf:.3f}, bbox=({x}, {y}, {w}, {h})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Object detection model not available. Skipping demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次圖像檢測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OBJECT_MODEL_AVAILABLE and object_net is not None:\n",
    "    # Find multiple test images\n",
    "    image_dir = Path('../assets/images/basic')\n",
    "    \n",
    "    if image_dir.exists():\n",
    "        image_files = list(image_dir.glob('*.jpg'))[:4]\n",
    "        \n",
    "        if len(image_files) > 0:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            axes = axes.ravel()\n",
    "            \n",
    "            for idx, img_path in enumerate(image_files[:4]):\n",
    "                # Load image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                # Resize for display\n",
    "                img = cv2.resize(img, (640, 480))\n",
    "                \n",
    "                # Detect\n",
    "                dets, t = detect_objects(img, object_net, confidence_threshold=0.4)\n",
    "                \n",
    "                # Draw\n",
    "                result = draw_detections(img, dets)\n",
    "                \n",
    "                # Display\n",
    "                axes[idx].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "                axes[idx].set_title(f'{img_path.name}\\n'\n",
    "                                   f'{len(dets)} objects, {t*1000:.1f}ms',\n",
    "                                   fontsize=10)\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            plt.suptitle('Batch Object Detection', fontsize=14, y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No test images found\")\n",
    "    else:\n",
    "        print(f\"Image directory not found: {image_dir}\")\n",
    "else:\n",
    "    print(\"Skipping batch detection demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 圖像分類 (Image Classification) - 10%\n",
    "\n",
    "### 使用 GoogLeNet 進行圖像分類\n",
    "\n",
    "GoogLeNet (Inception v1) 是一個經典的圖像分類網絡，在 ImageNet 數據集上訓練。\n",
    "\n",
    "### 模型特點\n",
    "- **架構**: GoogLeNet (Inception v1)\n",
    "- **輸入**: 224×224 RGB image\n",
    "- **類別**: 1000 ImageNet classes\n",
    "- **應用**: 通用圖像分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Due to file size, classification models are typically not included\n",
    "# This is a demonstration of how to use them\n",
    "\n",
    "print(\"Image Classification with DNN\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPopular classification models:\")\n",
    "print(\"  1. GoogLeNet (Inception v1) - 7MB\")\n",
    "print(\"  2. ResNet-50 - 100MB\")\n",
    "print(\"  3. VGG-16 - 550MB\")\n",
    "print(\"  4. SqueezeNet - 5MB (lightweight)\")\n",
    "print(\"  5. MobileNet - 17MB (mobile-friendly)\")\n",
    "print(\"\\nAll models can classify 1000 ImageNet categories\")\n",
    "\n",
    "# Classification function template\n",
    "def classify_image(image, net, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Classify image using pre-trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    net : cv2.dnn.Net\n",
    "        Classification network\n",
    "    labels : list\n",
    "        List of class labels\n",
    "    top_k : int\n",
    "        Return top K predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : list\n",
    "        List of (class_name, probability) tuples\n",
    "    elapsed : float\n",
    "        Inference time\n",
    "    \"\"\"\n",
    "    # Prepare blob (ImageNet mean values)\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image,\n",
    "        scalefactor=1.0,\n",
    "        size=(224, 224),\n",
    "        mean=(104, 117, 123),\n",
    "        swapRB=False,\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    net.setInput(blob)\n",
    "    probs = net.forward()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Get top K predictions\n",
    "    probs = probs.flatten()\n",
    "    top_indices = probs.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    predictions = []\n",
    "    for idx in top_indices:\n",
    "        predictions.append((labels[idx], probs[idx]))\n",
    "    \n",
    "    return predictions, elapsed\n",
    "\n",
    "print(\"\\n✓ Classification function template defined\")\n",
    "print(\"\\nTo use classification:\")\n",
    "print(\"  1. Download a model (e.g., GoogLeNet from Caffe Model Zoo)\")\n",
    "print(\"  2. Download ImageNet class labels\")\n",
    "print(\"  3. Load model: net = cv2.dnn.readNetFromCaffe(prototxt, model)\")\n",
    "print(\"  4. Classify: predictions = classify_image(img, net, labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 語義分割 (Semantic Segmentation) - 10%\n",
    "\n",
    "### 使用 ENet 進行語義分割\n",
    "\n",
    "語義分割為圖像的每個像素分配類別標籤。\n",
    "\n",
    "### 分割模型類型\n",
    "1. **FCN (Fully Convolutional Network)** - 第一個端到端分割網絡\n",
    "2. **ENet** - 輕量級實時分割網絡\n",
    "3. **Mask R-CNN** - 實例分割 (結合檢測與分割)\n",
    "4. **DeepLab v3+** - 高精度語義分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Semantic Segmentation with DNN\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCommon segmentation tasks:\")\n",
    "print(\"  1. Semantic Segmentation - Classify each pixel\")\n",
    "print(\"  2. Instance Segmentation - Separate object instances\")\n",
    "print(\"  3. Panoptic Segmentation - Combine semantic + instance\")\n",
    "\n",
    "# Segmentation function template\n",
    "def segment_image(image, net, output_layer_name):\n",
    "    \"\"\"\n",
    "    Perform semantic segmentation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    net : cv2.dnn.Net\n",
    "        Segmentation network\n",
    "    output_layer_name : str\n",
    "        Name of output layer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    mask : np.ndarray\n",
    "        Segmentation mask (H, W) with class IDs\n",
    "    elapsed : float\n",
    "        Inference time\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Prepare blob\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image,\n",
    "        scalefactor=1.0/255.0,\n",
    "        size=(512, 512),\n",
    "        mean=(0, 0, 0),\n",
    "        swapRB=True,\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    net.setInput(blob)\n",
    "    output = net.forward(output_layer_name)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Output shape: (1, num_classes, H, W)\n",
    "    # Get class with max probability for each pixel\n",
    "    mask = np.argmax(output[0], axis=0)\n",
    "    \n",
    "    # Resize to original size\n",
    "    mask = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return mask, elapsed\n",
    "\n",
    "\n",
    "def visualize_segmentation(image, mask, num_classes=21, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Visualize segmentation mask\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Original image\n",
    "    mask : np.ndarray\n",
    "        Segmentation mask with class IDs\n",
    "    num_classes : int\n",
    "        Number of classes\n",
    "    alpha : float\n",
    "        Transparency of overlay\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Visualization\n",
    "    \"\"\"\n",
    "    # Create color map\n",
    "    colors = np.random.randint(0, 255, size=(num_classes, 3), dtype=np.uint8)\n",
    "    colors[0] = [0, 0, 0]  # Background is black\n",
    "    \n",
    "    # Map mask to colors\n",
    "    colored_mask = colors[mask]\n",
    "    \n",
    "    # Blend with original image\n",
    "    output = cv2.addWeighted(image, 1 - alpha, colored_mask, alpha, 0)\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"\\n✓ Segmentation function templates defined\")\n",
    "print(\"\\nPopular segmentation datasets:\")\n",
    "print(\"  - PASCAL VOC: 21 classes (person, car, dog, etc.)\")\n",
    "print(\"  - Cityscapes: Urban street scenes (19 classes)\")\n",
    "print(\"  - ADE20K: 150 diverse classes\")\n",
    "print(\"  - COCO: 80 object categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能優化 (Performance Optimization) - 10%\n",
    "\n",
    "### Backend 與 Target 選擇\n",
    "\n",
    "OpenCV DNN 支援多種計算後端和目標設備。\n",
    "\n",
    "### Backends\n",
    "- **DNN_BACKEND_DEFAULT**: 自動選擇\n",
    "- **DNN_BACKEND_OPENCV**: OpenCV 實作 (CPU)\n",
    "- **DNN_BACKEND_HALIDE**: Halide 優化\n",
    "- **DNN_BACKEND_CUDA**: NVIDIA CUDA (GPU)\n",
    "- **DNN_BACKEND_INFERENCE_ENGINE**: Intel OpenVINO\n",
    "\n",
    "### Targets\n",
    "- **DNN_TARGET_CPU**: CPU 執行\n",
    "- **DNN_TARGET_OPENCL**: OpenCL (GPU/CPU)\n",
    "- **DNN_TARGET_OPENCL_FP16**: OpenCL with FP16\n",
    "- **DNN_TARGET_CUDA**: CUDA (NVIDIA GPU)\n",
    "- **DNN_TARGET_CUDA_FP16**: CUDA with FP16\n",
    "- **DNN_TARGET_MYRIAD**: Intel Movidius VPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmark utility\n",
    "def benchmark_backend_target(net, image, backend, target, iterations=10):\n",
    "    \"\"\"\n",
    "    Benchmark DNN inference with specific backend and target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    net : cv2.dnn.Net\n",
    "        Network to benchmark\n",
    "    image : np.ndarray\n",
    "        Test image\n",
    "    backend : int\n",
    "        DNN backend ID\n",
    "    target : int\n",
    "        DNN target ID\n",
    "    iterations : int\n",
    "        Number of iterations for averaging\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    avg_time : float\n",
    "        Average inference time in milliseconds\n",
    "    success : bool\n",
    "        Whether benchmark succeeded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set backend and target\n",
    "        net.setPreferableBackend(backend)\n",
    "        net.setPreferableTarget(target)\n",
    "        \n",
    "        # Prepare blob\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)),\n",
    "            1.0, (300, 300), (104, 177, 123),\n",
    "            swapRB=False, crop=False\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        net.setInput(blob)\n",
    "        _ = net.forward()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(iterations):\n",
    "            start = time.time()\n",
    "            net.setInput(blob)\n",
    "            _ = net.forward()\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed * 1000)  # Convert to ms\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        return avg_time, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "        return 0, False\n",
    "\n",
    "print(\"Performance benchmarking utility defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能比較測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FACE_MODEL_AVAILABLE and face_net is not None and test_img is not None:\n",
    "    print(\"Benchmarking different backend/target combinations...\\n\")\n",
    "    \n",
    "    # Test configurations\n",
    "    configs = [\n",
    "        (\"CPU\", cv2.dnn.DNN_BACKEND_OPENCV, cv2.dnn.DNN_TARGET_CPU),\n",
    "        (\"OpenCL\", cv2.dnn.DNN_BACKEND_OPENCV, cv2.dnn.DNN_TARGET_OPENCL),\n",
    "        (\"OpenCL FP16\", cv2.dnn.DNN_BACKEND_OPENCV, cv2.dnn.DNN_TARGET_OPENCL_FP16),\n",
    "    ]\n",
    "    \n",
    "    # Add CUDA if available\n",
    "    try:\n",
    "        if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "            configs.append((\"CUDA\", cv2.dnn.DNN_BACKEND_CUDA, cv2.dnn.DNN_TARGET_CUDA))\n",
    "            configs.append((\"CUDA FP16\", cv2.dnn.DNN_BACKEND_CUDA, cv2.dnn.DNN_TARGET_CUDA_FP16))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, backend, target in configs:\n",
    "        print(f\"Testing {name}...\", end=\" \")\n",
    "        avg_time, success = benchmark_backend_target(\n",
    "            face_net, test_img, backend, target, iterations=10\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"✓ {avg_time:.2f}ms\")\n",
    "            results.append((name, avg_time))\n",
    "        else:\n",
    "            print(\"✗ Not available\")\n",
    "    \n",
    "    # Visualize results\n",
    "    if len(results) > 0:\n",
    "        names, times = zip(*results)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars = ax.barh(names, times, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Color fastest bar green\n",
    "        min_idx = np.argmin(times)\n",
    "        bars[min_idx].set_color('lightgreen')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (name, time_val) in enumerate(results):\n",
    "            ax.text(time_val + 0.5, i, f'{time_val:.2f}ms',\n",
    "                   va='center', fontsize=10)\n",
    "        \n",
    "        ax.set_xlabel('Inference Time (ms)', fontsize=12)\n",
    "        ax.set_title('Performance Comparison: Backend/Target', fontsize=14, pad=20)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Speedup analysis\n",
    "        baseline_time = results[0][1]  # CPU time\n",
    "        print(\"\\nSpeedup compared to CPU:\")\n",
    "        for name, time_val in results:\n",
    "            speedup = baseline_time / time_val\n",
    "            print(f\"  {name:<15}: {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"Skipping performance benchmark (model not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 優化建議\n",
    "\n",
    "#### 1. 選擇合適的 Backend/Target\n",
    "```python\n",
    "# CPU (default)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# NVIDIA GPU (if available)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "# Intel OpenCL\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
    "```\n",
    "\n",
    "#### 2. 模型量化 (Quantization)\n",
    "- 使用 FP16 或 INT8 精度\n",
    "- 減少模型大小和推理時間\n",
    "- 略微降低精度（通常可接受）\n",
    "\n",
    "#### 3. 批次處理 (Batching)\n",
    "- 使用 `blobFromImages()` 處理多張圖像\n",
    "- 提高 GPU 利用率\n",
    "\n",
    "#### 4. 輸入尺寸優化\n",
    "- 使用較小的輸入尺寸（如 300×300 vs 600×600）\n",
    "- 權衡精度與速度\n",
    "\n",
    "#### 5. 模型選擇\n",
    "- MobileNet: 移動設備\n",
    "- SqueezeNet: 邊緣設備\n",
    "- ResNet/VGG: 高精度應用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization comparison table\n",
    "print(\"\\nOptimization Strategies:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Strategy':<25} {'Speedup':<15} {'Accuracy Impact':<20} {'Use Case'}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'FP16 Precision':<25} {'1.5-2x':<15} {'Minimal (~0.5%)':<20} {'GPU inference'}\")\n",
    "print(f\"{'INT8 Quantization':<25} {'2-4x':<15} {'Small (~1-2%)':<20} {'Edge devices'}\")\n",
    "print(f\"{'Model Pruning':<25} {'1.5-3x':<15} {'Medium (~2-5%)':<20} {'Resource-limited'}\")\n",
    "print(f\"{'Input Downscaling':<25} {'2-4x':<15} {'Depends on task':<20} {'Real-time apps'}\")\n",
    "print(f\"{'Batch Processing':<25} {'1.5-2x':<15} {'None':<20} {'Offline processing'}\")\n",
    "print(f\"{'GPU Acceleration':<25} {'5-10x':<15} {'None':<20} {'Server deployment'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 實戰練習 (Hands-on Exercises) - 3%\n",
    "\n",
    "### 練習 1: 多模型整合\n",
    "\n",
    "結合人臉檢測和物體檢測，在同一張圖像上運行兩個模型。\n",
    "\n",
    "**任務**:\n",
    "1. 載入人臉檢測和物體檢測模型\n",
    "2. 對同一圖像進行兩種檢測\n",
    "3. 在結果圖上用不同顏色標記\n",
    "4. 統計檢測到的人臉和物體數量\n",
    "\n",
    "### 練習 2: 視訊流處理\n",
    "\n",
    "實作實時視訊物體檢測。\n",
    "\n",
    "**任務**:\n",
    "1. 讀取視訊文件或攝像頭\n",
    "2. 逐幀進行物體檢測\n",
    "3. 計算並顯示 FPS\n",
    "4. 測試不同 backend 的性能差異\n",
    "\n",
    "### 練習 3: 自定義後處理\n",
    "\n",
    "實作 Non-Maximum Suppression (NMS) 去除重複檢測。\n",
    "\n",
    "**任務**:\n",
    "1. 實作 IoU (Intersection over Union) 計算\n",
    "2. 實作 NMS 算法\n",
    "3. 對檢測結果應用 NMS\n",
    "4. 比較 NMS 前後的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise templates\n",
    "\n",
    "# Exercise 1: Multi-model integration\n",
    "def exercise1_multimodel():\n",
    "    \"\"\"\n",
    "    TODO: Implement multi-model detection\n",
    "    - Load face and object detection models\n",
    "    - Run both on same image\n",
    "    - Draw results with different colors\n",
    "    - Print statistics\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Exercise 2: Video stream processing\n",
    "def exercise2_video():\n",
    "    \"\"\"\n",
    "    TODO: Implement real-time video detection\n",
    "    - Open video file or webcam\n",
    "    - Process each frame\n",
    "    - Calculate and display FPS\n",
    "    - Test different backends\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Exercise 3: NMS implementation\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    TODO: Compute Intersection over Union\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    box1, box2 : tuple\n",
    "        Bounding boxes (x, y, w, h)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    iou : float\n",
    "        IoU score (0.0-1.0)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def apply_nms(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    TODO: Apply Non-Maximum Suppression\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    detections : list\n",
    "        List of (class_id, confidence, x, y, w, h) tuples\n",
    "    iou_threshold : float\n",
    "        IoU threshold for suppression\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    filtered : list\n",
    "        Filtered detections after NMS\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Exercise templates defined.\")\n",
    "print(\"\\nImplement the TODO sections to complete the exercises.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結與延伸 (Summary & Extensions) - 2%\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. **OpenCV DNN 優勢**\n",
    "   - 統一接口支援多框架模型\n",
    "   - 無需安裝深度學習框架\n",
    "   - 輕量級、易部署\n",
    "   - 支援多種硬體加速\n",
    "\n",
    "2. **支援的框架**\n",
    "   - Caffe: 人臉檢測、分類\n",
    "   - TensorFlow: 物體檢測\n",
    "   - PyTorch (via ONNX): 最新研究模型\n",
    "   - Darknet: YOLO 系列\n",
    "\n",
    "3. **應用場景**\n",
    "   - 人臉檢測: ResNet-SSD\n",
    "   - 物體檢測: MobileNet-SSD, YOLO\n",
    "   - 圖像分類: GoogLeNet, ResNet\n",
    "   - 語義分割: FCN, ENet\n",
    "\n",
    "4. **性能優化**\n",
    "   - Backend/Target 選擇\n",
    "   - FP16/INT8 量化\n",
    "   - 批次處理\n",
    "   - 輸入尺寸優化\n",
    "\n",
    "### 與傳統方法對比\n",
    "\n",
    "| 方法 | 準確度 | 速度 | 部署 | 適用場景 |\n",
    "|------|--------|------|------|----------|\n",
    "| **Haar Cascade** | 中 (85%) | 快 (10ms) | 簡單 | 實時應用、嵌入式 |\n",
    "| **HOG + SVM** | 中-高 (90%) | 中 (50ms) | 簡單 | 傳統 CV 系統 |\n",
    "| **DNN (CPU)** | 高 (95%+) | 中 (30-80ms) | 中等 | 準確度優先 |\n",
    "| **DNN (GPU)** | 高 (95%+) | 快 (5-15ms) | 複雜 | 實時 + 高精度 |\n",
    "\n",
    "### 延伸學習\n",
    "\n",
    "#### 1. 模型訓練與轉換\n",
    "- 使用 TensorFlow/PyTorch 訓練自定義模型\n",
    "- 轉換為 OpenCV DNN 支援格式\n",
    "- 模型量化與優化\n",
    "\n",
    "#### 2. YOLO 系列\n",
    "- YOLOv3/v4/v5 實戰\n",
    "- 實時多類別物體檢測\n",
    "- 自定義類別訓練\n",
    "\n",
    "#### 3. 實例分割\n",
    "- Mask R-CNN 實作\n",
    "- 像素級物體分割\n",
    "- 醫療影像應用\n",
    "\n",
    "#### 4. 跨平台部署\n",
    "- Android/iOS 移動部署\n",
    "- 邊緣設備優化 (Raspberry Pi, Jetson)\n",
    "- TensorFlow Lite / ONNX Runtime\n",
    "\n",
    "#### 5. 深度學習框架整合\n",
    "- OpenCV + TensorFlow\n",
    "- OpenCV + PyTorch\n",
    "- 混合推理管線\n",
    "\n",
    "### 實戰項目建議\n",
    "\n",
    "1. **智能監控系統**\n",
    "   - 多攝像頭人臉 + 物體檢測\n",
    "   - 異常行為檢測\n",
    "   - 實時警報系統\n",
    "\n",
    "2. **自動駕駛感知**\n",
    "   - 車輛、行人、交通標誌檢測\n",
    "   - 車道線分割\n",
    "   - 多傳感器融合\n",
    "\n",
    "3. **醫療影像分析**\n",
    "   - 病灶檢測與分割\n",
    "   - X光/CT/MRI 分析\n",
    "   - 輔助診斷系統\n",
    "\n",
    "4. **零售分析**\n",
    "   - 商品識別\n",
    "   - 顧客行為分析\n",
    "   - 庫存管理\n",
    "\n",
    "### 參考資源\n",
    "\n",
    "#### 官方文檔\n",
    "- OpenCV DNN Tutorial: https://docs.opencv.org/4.x/d2/d58/tutorial_table_of_content_dnn.html\n",
    "- OpenCV Model Zoo: https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "\n",
    "#### 模型資源\n",
    "- Caffe Model Zoo: https://github.com/BVLC/caffe/wiki/Model-Zoo\n",
    "- TensorFlow Model Garden: https://github.com/tensorflow/models\n",
    "- ONNX Model Zoo: https://github.com/onnx/models\n",
    "- PyTorch Hub: https://pytorch.org/hub/\n",
    "\n",
    "#### 論文與教程\n",
    "- SSD: Single Shot MultiBox Detector (2016)\n",
    "- YOLO: You Only Look Once (2015-2020)\n",
    "- Mask R-CNN (2017)\n",
    "- EfficientDet (2020)\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "完成本模組後，建議繼續學習:\n",
    "- **5.2.2 YOLO 系列實戰** - YOLOv3/v4/v5 詳細教學\n",
    "- **5.2.3 模型訓練與轉換** - 自定義模型開發\n",
    "- **5.3.1 實例分割** - Mask R-CNN 實作\n",
    "- **5.3.2 姿態估計** - OpenPose 人體骨架檢測\n",
    "\n",
    "**實戰建議**: \n",
    "1. 下載並測試不同模型\n",
    "2. 比較各模型的速度與精度\n",
    "3. 實作自己的檢測應用\n",
    "4. 嘗試模型融合提升性能\n",
    "\n",
    "---\n",
    "\n",
    "**模組完成標記**: ✅ WBS 5.2.1 OpenCV DNN Module Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
