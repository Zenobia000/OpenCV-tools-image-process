{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.2 å¯¦æ™‚ç‰©é«”æª¢æ¸¬ (Real-time Object Detection)\n",
    "\n",
    "**WBS 5.2.2**: YOLO ç³»åˆ—èˆ‡å¯¦æ™‚æª¢æ¸¬æŠ€è¡“\n",
    "\n",
    "æœ¬æ¨¡çµ„æ·±å…¥æ¢è¨å¯¦æ™‚ç‰©é«”æª¢æ¸¬æŠ€è¡“ï¼Œé‡é»å­¸ç¿’ YOLO (You Only Look Once) ç³»åˆ—ç®—æ³•ï¼Œå¾åŸºç¤åŸç†åˆ°ç”Ÿç”¢ç´šéƒ¨ç½²ã€‚\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£å¯¦æ™‚ç‰©é«”æª¢æ¸¬çš„æ ¸å¿ƒè¦æ±‚èˆ‡æŒ‘æˆ°\n",
    "- æŒæ¡ YOLO ç³»åˆ—ç®—æ³•çš„æ¶æ§‹æ¼”é€²\n",
    "- å¯¦ä½œ YOLOv3/v4 å®Œæ•´æª¢æ¸¬æµç¨‹\n",
    "- å­¸ç¿’æ€§èƒ½å„ªåŒ–èˆ‡å¤šç·šç¨‹è™•ç†æŠ€è¡“\n",
    "- å¯¦ç¾ç”Ÿç”¢ç´šå¯¦æ™‚è¦–è¨Šæª¢æ¸¬ç³»çµ±\n",
    "\n",
    "## å‰ç½®çŸ¥è­˜\n",
    "- OpenCV DNN æ¨¡çµ„ (WBS 5.2.1)\n",
    "- åœ–åƒé è™•ç†æŠ€è¡“ (Stage 3)\n",
    "- Python å¤šç·šç¨‹åŸºç¤\n",
    "- åŸºç¤æ·±åº¦å­¸ç¿’æ¦‚å¿µ\n",
    "\n",
    "## èª²ç¨‹å¤§ç¶±\n",
    "1. å¯¦æ™‚æª¢æ¸¬åŸºç¤ (Real-time Detection Basics) - 5%\n",
    "2. YOLO ç³»åˆ—ä»‹ç´¹ (YOLO Series Introduction) - 15%\n",
    "3. YOLOv3/v4 å¯¦ä½œ (YOLOv3/v4 Implementation) - 20%\n",
    "4. æ€§èƒ½å„ªåŒ–æŠ€å·§ (Performance Optimization) - 15%\n",
    "5. å¤šç·šç¨‹è™•ç† (Multi-threading) - 10%\n",
    "6. æ‰¹æ¬¡æ¨ç† (Batch Inference) - 10%\n",
    "7. å¯¦æ™‚è¦–è¨Šæª¢æ¸¬ (Real-time Video Detection) - 15%\n",
    "8. æª¢æ¸¬çµæœè¿½è¹¤ (Object Tracking) - 5%\n",
    "9. å¯¦æˆ°ç·´ç¿’ (Hands-on Exercises) - 3%\n",
    "10. ç¸½çµèˆ‡éƒ¨ç½² (Summary & Deployment) - 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request\n",
    "from collections import defaultdict, deque\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Configure matplotlib for Chinese display\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Disable warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "try:\n",
    "    cuda_devices = cv2.cuda.getCudaEnabledDeviceCount()\n",
    "    print(f\"CUDA devices available: {cuda_devices}\")\n",
    "except:\n",
    "    print(\"CUDA not available - will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¦æ™‚æª¢æ¸¬åŸºç¤ (Real-time Detection Basics) - 5%\n",
    "\n",
    "### ä»€éº¼æ˜¯å¯¦æ™‚ç‰©é«”æª¢æ¸¬ï¼Ÿ\n",
    "\n",
    "**å¯¦æ™‚ç‰©é«”æª¢æ¸¬**æ˜¯æŒ‡èƒ½å¤ åœ¨è¦–è¨Šæµä¸­ä»¥è¶³å¤ é«˜çš„å¹€ç‡ï¼ˆé€šå¸¸ â‰¥30 FPSï¼‰æª¢æ¸¬ä¸¦å®šä½å¤šå€‹ç‰©é«”çš„æŠ€è¡“ã€‚\n",
    "\n",
    "### æ ¸å¿ƒè¦æ±‚\n",
    "\n",
    "#### 1. é€Ÿåº¦è¦æ±‚ (Speed Requirements)\n",
    "- **å¯¦æ™‚æ¨™æº–**: â‰¥30 FPS (æ¯å¹€ â‰¤33ms)\n",
    "- **æµæš¢é«”é©—**: â‰¥20 FPS (æ¯å¹€ â‰¤50ms)\n",
    "- **å¯æ¥å—**: â‰¥10 FPS (æ¯å¹€ â‰¤100ms)\n",
    "- **ä¸æµæš¢**: <10 FPS\n",
    "\n",
    "#### 2. æº–ç¢ºåº¦è¦æ±‚ (Accuracy Requirements)\n",
    "- **mAP (mean Average Precision)**: é€šç”¨è©•ä¼°æŒ‡æ¨™\n",
    "- **IoU Threshold**: é€šå¸¸ 0.5 æˆ– 0.75\n",
    "- **Recall**: æª¢æ¸¬åˆ°çš„ç‰©é«”æ¯”ä¾‹\n",
    "- **Precision**: æ­£ç¢ºæª¢æ¸¬çš„æ¯”ä¾‹\n",
    "\n",
    "#### 3. å»¶é²è¦æ±‚ (Latency Requirements)\n",
    "- **ç«¯åˆ°ç«¯å»¶é²**: å¾åœ–åƒè¼¸å…¥åˆ°çµæœè¼¸å‡º\n",
    "- **è™•ç†å»¶é²**: æ¨¡å‹æ¨ç†æ™‚é–“\n",
    "- **é€šä¿¡å»¶é²**: æ•¸æ“šå‚³è¼¸æ™‚é–“\n",
    "\n",
    "### æ€§èƒ½æ¬Šè¡¡ (Speed-Accuracy Trade-off)\n",
    "\n",
    "```\n",
    "High Accuracy (Slower)           Real-time (Faster)\n",
    "        â†“                              â†“\n",
    "  Mask R-CNN  â†’  Faster R-CNN  â†’  SSD  â†’  YOLO\n",
    "    5 FPS          7 FPS        25 FPS   45+ FPS\n",
    "    mAP: 39%       mAP: 37%     mAP: 31%  mAP: 33%\n",
    "```\n",
    "\n",
    "### å¯¦æ™‚æª¢æ¸¬æŒ‘æˆ°\n",
    "\n",
    "1. **è¨ˆç®—è³‡æºé™åˆ¶**: CPU vs GPU vs é‚Šç·£è¨­å‚™\n",
    "2. **æ¨¡å‹å¤§å°**: å…§å­˜ä½”ç”¨èˆ‡è¼‰å…¥æ™‚é–“\n",
    "3. **å¤šå°ºåº¦æª¢æ¸¬**: å°ç‰©é«”èˆ‡å¤§ç‰©é«”åŒæ™‚æª¢æ¸¬\n",
    "4. **é®æ“‹èˆ‡æ“æ“ **: é‡ç–Šç‰©é«”çš„åˆ†é›¢\n",
    "5. **å…‰ç…§èˆ‡è¦–è§’è®ŠåŒ–**: é­¯æ£’æ€§è¦æ±‚\n",
    "\n",
    "### æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "âœ… **é©åˆå¯¦æ™‚æª¢æ¸¬**:\n",
    "- è‡ªå‹•é§•é§› (è»Šè¼›ã€è¡Œäººã€äº¤é€šæ¨™èªŒ)\n",
    "- æ™ºèƒ½ç›£æ§ (å…¥ä¾µæª¢æ¸¬ã€ç•°å¸¸è¡Œç‚º)\n",
    "- æ©Ÿå™¨äººè¦–è¦º (å°èˆªã€æŠ“å–)\n",
    "- AR/VR æ‡‰ç”¨ (ç‰©é«”è­˜åˆ¥èˆ‡è¿½è¹¤)\n",
    "- å·¥æ¥­æª¢æ¸¬ (ç¼ºé™·æª¢æ¸¬)\n",
    "\n",
    "âŒ **ä¸é©åˆå¯¦æ™‚æª¢æ¸¬**:\n",
    "- é†«ç™‚å½±åƒåˆ†æ (é«˜ç²¾åº¦å„ªå…ˆ)\n",
    "- è¡›æ˜Ÿåœ–åƒåˆ†æ (é›¢ç·šè™•ç†)\n",
    "- è—è¡“å“ä¿®å¾© (è³ªé‡å„ªå…ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance requirements comparison\n",
    "performance_data = {\n",
    "    \"Application\": [\n",
    "        \"Autonomous Driving\",\n",
    "        \"Smart Surveillance\",\n",
    "        \"Robot Navigation\",\n",
    "        \"AR/VR\",\n",
    "        \"Industrial Inspection\",\n",
    "        \"Face Recognition\"\n",
    "    ],\n",
    "    \"Required FPS\": [30, 20, 25, 60, 15, 30],\n",
    "    \"Accuracy Priority\": [\"High\", \"Medium\", \"Medium\", \"Medium\", \"Very High\", \"High\"],\n",
    "    \"Latency Budget (ms)\": [33, 50, 40, 16, 67, 33]\n",
    "}\n",
    "\n",
    "print(\"Real-time Detection Requirements by Application\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Application':<25} {'Required FPS':<15} {'Accuracy':<20} {'Latency (ms)'}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "for i in range(len(performance_data[\"Application\"])):\n",
    "    print(f\"{performance_data['Application'][i]:<25} \"\n",
    "          f\"{performance_data['Required FPS'][i]:<15} \"\n",
    "          f\"{performance_data['Accuracy Priority'][i]:<20} \"\n",
    "          f\"{performance_data['Latency Budget (ms)'][i]}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# Visualize FPS requirements\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "apps = performance_data[\"Application\"]\n",
    "fps = performance_data[\"Required FPS\"]\n",
    "\n",
    "colors = ['#FF6B6B' if f >= 30 else '#4ECDC4' if f >= 20 else '#95E1D3' for f in fps]\n",
    "bars = ax.barh(apps, fps, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add FPS threshold lines\n",
    "ax.axvline(x=30, color='red', linestyle='--', linewidth=2, label='Real-time (30 FPS)', alpha=0.7)\n",
    "ax.axvline(x=20, color='orange', linestyle='--', linewidth=2, label='Smooth (20 FPS)', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for i, (app, f) in enumerate(zip(apps, fps)):\n",
    "    ax.text(f + 1, i, f'{f} FPS', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Frames Per Second (FPS)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('FPS Requirements by Application Domain', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight: Most real-time applications require â‰¥20 FPS for smooth operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YOLO ç³»åˆ—ä»‹ç´¹ (YOLO Series Introduction) - 15%\n",
    "\n",
    "### YOLO: You Only Look Once\n",
    "\n",
    "**YOLO** æ˜¯ä¸€ç¨®é©å‘½æ€§çš„å–®éšæ®µç‰©é«”æª¢æ¸¬ç®—æ³•ï¼Œç”± Joseph Redmon æ–¼ 2015 å¹´æå‡ºã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°‡ç‰©é«”æª¢æ¸¬è¦–ç‚º**å›æ­¸å•é¡Œ**ï¼Œç›´æ¥å¾åœ–åƒåƒç´ é æ¸¬é‚Šç•Œæ¡†å’Œé¡åˆ¥æ¦‚ç‡ã€‚\n",
    "\n",
    "### æ ¸å¿ƒå‰µæ–°\n",
    "\n",
    "#### 1. å–®éšæ®µæª¢æ¸¬ (Single-Shot Detection)\n",
    "- **å‚³çµ±å…©éšæ®µ**: Region Proposal â†’ Classification (R-CNN ç³»åˆ—)\n",
    "- **YOLO å–®éšæ®µ**: ä¸€æ¬¡å‰å‘å‚³æ’­å®Œæˆæª¢æ¸¬\n",
    "- **å„ªå‹¢**: é€Ÿåº¦å¿«ï¼Œç«¯åˆ°ç«¯è¨“ç·´\n",
    "\n",
    "#### 2. å…¨å±€ä¸Šä¸‹æ–‡ (Global Context)\n",
    "- çœ‹æ•´å¼µåœ–åƒï¼Œè€Œéå±€éƒ¨å€åŸŸ\n",
    "- æ¸›å°‘èƒŒæ™¯èª¤æª¢æ¸¬\n",
    "- æ›´å¥½çš„ç‰©é«”é—œä¿‚ç†è§£\n",
    "\n",
    "#### 3. çµ±ä¸€æ¶æ§‹ (Unified Architecture)\n",
    "```\n",
    "Input Image â†’ CNN Backbone â†’ Detection Head â†’ Outputs\n",
    "                                               â†“\n",
    "                          [Bounding Boxes + Class Probabilities]\n",
    "```\n",
    "\n",
    "### YOLO ç‰ˆæœ¬æ¼”é€²\n",
    "\n",
    "#### YOLOv1 (2015) - é–‹å‰µè€…\n",
    "- **æ¶æ§‹**: 24 convolutional layers + 2 fully connected layers\n",
    "- **è¼¸å…¥**: 448Ã—448\n",
    "- **é€Ÿåº¦**: 45 FPS (base), 155 FPS (fast)\n",
    "- **mAP**: 63.4% (VOC 2007)\n",
    "- **ç¼ºé»**: å°ç‰©é«”æª¢æ¸¬å·®ï¼Œå®šä½ç²¾åº¦ä½\n",
    "\n",
    "#### YOLOv2 / YOLO9000 (2016) - æ”¹é€²ç‰ˆ\n",
    "- **å‰µæ–°**: \n",
    "  - Batch Normalization\n",
    "  - Anchor Boxes (å€Ÿé‘’ Faster R-CNN)\n",
    "  - Darknet-19 backbone (19 layers)\n",
    "  - Multi-scale training\n",
    "- **é€Ÿåº¦**: 40-90 FPS\n",
    "- **mAP**: 78.6% (VOC 2007)\n",
    "- **ç‰¹é»**: å¯æª¢æ¸¬ 9000+ é¡åˆ¥\n",
    "\n",
    "#### YOLOv3 (2018) - å¤šå°ºåº¦æª¢æ¸¬\n",
    "- **æ¶æ§‹**: Darknet-53 (53 layers)\n",
    "- **å‰µæ–°**:\n",
    "  - 3 å€‹å°ºåº¦çš„ç‰¹å¾µåœ– (13Ã—13, 26Ã—26, 52Ã—52)\n",
    "  - æ¯å€‹å°ºåº¦ 3 å€‹ anchor boxes\n",
    "  - Residual connections\n",
    "  - æ”¹é€²çš„å°ç‰©é«”æª¢æ¸¬\n",
    "- **é€Ÿåº¦**: 20-30 FPS (416Ã—416)\n",
    "- **mAP**: 57.9% (COCO)\n",
    "- **ç‹€æ…‹**: æœ¬æ¨¡çµ„é‡é»\n",
    "\n",
    "#### YOLOv4 (2020) - æ€§èƒ½å·”å³°\n",
    "- **æ¶æ§‹**: CSPDarknet53 + SPP + PANet\n",
    "- **å‰µæ–°**:\n",
    "  - CSPNet (Cross Stage Partial Network)\n",
    "  - Mish activation\n",
    "  - DropBlock regularization\n",
    "  - Mosaic data augmentation\n",
    "  - CIoU loss\n",
    "- **é€Ÿåº¦**: 65 FPS (Tesla V100)\n",
    "- **mAP**: 43.5% (COCO)\n",
    "- **ç‰¹é»**: æœ€ä½³é€Ÿåº¦/ç²¾åº¦æ¬Šè¡¡\n",
    "\n",
    "#### YOLOv5 (2020) - PyTorch é‡å¯«\n",
    "- **ç‰¹é»**: PyTorch å¯¦ç¾ï¼Œæ˜“ç”¨æ€§é«˜\n",
    "- **ç‰ˆæœ¬**: YOLOv5s/m/l/x (ä¸åŒå¤§å°)\n",
    "- **é€Ÿåº¦**: 140 FPS (YOLOv5s)\n",
    "- **mAP**: 37.4-50.7% (COCO)\n",
    "- **ç”Ÿæ…‹**: éƒ¨ç½²å·¥å…·è±å¯Œ\n",
    "\n",
    "#### YOLOv6-v8 (2022-2023) - æŒçºŒæ¼”é€²\n",
    "- **YOLOv6**: å·¥æ¥­ç•Œå„ªåŒ– (ç¾åœ˜)\n",
    "- **YOLOv7**: æ¶æ§‹å‰µæ–° (Edge TPU å„ªåŒ–)\n",
    "- **YOLOv8**: Ultralytics æœ€æ–°ç‰ˆ\n",
    "  - æ”¯æŒæª¢æ¸¬ã€åˆ†å‰²ã€åˆ†é¡ã€å§¿æ…‹ä¼°è¨ˆ\n",
    "  - çµ±ä¸€ API è¨­è¨ˆ\n",
    "  - mAP: 53.9% (YOLOv8x)\n",
    "\n",
    "### YOLO vs å…¶ä»–æª¢æ¸¬å™¨\n",
    "\n",
    "| Model | Type | FPS | mAP (COCO) | Year |\n",
    "|-------|------|-----|------------|------|\n",
    "| **R-CNN** | Two-stage | 0.05 | - | 2014 |\n",
    "| **Fast R-CNN** | Two-stage | 0.5 | - | 2015 |\n",
    "| **Faster R-CNN** | Two-stage | 7 | 42.7% | 2015 |\n",
    "| **YOLOv1** | One-stage | 45 | - | 2015 |\n",
    "| **SSD300** | One-stage | 46 | 25.1% | 2016 |\n",
    "| **YOLOv2** | One-stage | 67 | 21.6% | 2016 |\n",
    "| **YOLOv3** | One-stage | 30 | 33.0% | 2018 |\n",
    "| **YOLOv4** | One-stage | 65 | 43.5% | 2020 |\n",
    "| **EfficientDet** | One-stage | 30 | 51.0% | 2020 |\n",
    "| **YOLOv8** | One-stage | 80+ | 53.9% | 2023 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO evolution visualization\n",
    "yolo_evolution = {\n",
    "    \"Version\": [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v8\"],\n",
    "    \"Year\": [2015, 2016, 2018, 2020, 2020, 2023],\n",
    "    \"FPS\": [45, 67, 30, 65, 140, 80],\n",
    "    \"mAP\": [0.0, 21.6, 33.0, 43.5, 37.4, 53.9],  # COCO dataset\n",
    "    \"Backbone\": [\"Custom\", \"Darknet-19\", \"Darknet-53\", \"CSPDarknet53\", \"CSPDarknet\", \"CSPDarknet\"],\n",
    "    \"Key Innovation\": [\n",
    "        \"Single-stage detection\",\n",
    "        \"Anchor boxes\",\n",
    "        \"Multi-scale FPN\",\n",
    "        \"CSPNet + Bag of Freebies\",\n",
    "        \"PyTorch implementation\",\n",
    "        \"Unified architecture\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Speed evolution\n",
    "versions = yolo_evolution[\"Version\"]\n",
    "fps = yolo_evolution[\"FPS\"]\n",
    "years = yolo_evolution[\"Year\"]\n",
    "\n",
    "colors_fps = plt.cm.viridis(np.linspace(0, 1, len(versions)))\n",
    "ax1.plot(years, fps, marker='o', linewidth=3, markersize=12, color='blue', alpha=0.7)\n",
    "for i, (year, f, ver) in enumerate(zip(years, fps, versions)):\n",
    "    ax1.scatter(year, f, s=300, c=[colors_fps[i]], edgecolor='black', linewidth=2, zorder=5)\n",
    "    ax1.text(year, f + 8, f'YOLO{ver}\\n{f} FPS', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Speed (FPS)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('YOLO Speed Evolution', fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(True, alpha=0.3, linestyle=':')\n",
    "ax1.set_ylim(0, 160)\n",
    "\n",
    "# Plot 2: Accuracy evolution\n",
    "map_scores = yolo_evolution[\"mAP\"]\n",
    "# Filter out v1 (no COCO mAP)\n",
    "valid_indices = [i for i, m in enumerate(map_scores) if m > 0]\n",
    "valid_years = [years[i] for i in valid_indices]\n",
    "valid_map = [map_scores[i] for i in valid_indices]\n",
    "valid_versions = [versions[i] for i in valid_indices]\n",
    "\n",
    "colors_map = plt.cm.plasma(np.linspace(0, 1, len(valid_versions)))\n",
    "ax2.plot(valid_years, valid_map, marker='s', linewidth=3, markersize=12, color='red', alpha=0.7)\n",
    "for i, (year, m, ver) in enumerate(zip(valid_years, valid_map, valid_versions)):\n",
    "    ax2.scatter(year, m, s=300, c=[colors_map[i]], edgecolor='black', linewidth=2, zorder=5)\n",
    "    ax2.text(year, m + 2.5, f'YOLO{ver}\\n{m:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('mAP on COCO (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('YOLO Accuracy Evolution', fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.grid(True, alpha=0.3, linestyle=':')\n",
    "ax2.set_ylim(0, 60)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\nYOLO Version Comparison\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Version':<10} {'Year':<8} {'FPS':<8} {'mAP (COCO)':<15} {'Key Innovation':<50}\")\n",
    "print(\"=\" * 100)\n",
    "for i in range(len(yolo_evolution[\"Version\"])):\n",
    "    ver = yolo_evolution[\"Version\"][i]\n",
    "    year = yolo_evolution[\"Year\"][i]\n",
    "    fps = yolo_evolution[\"FPS\"][i]\n",
    "    map_score = yolo_evolution[\"mAP\"][i]\n",
    "    innovation = yolo_evolution[\"Key Innovation\"][i]\n",
    "    \n",
    "    map_str = f\"{map_score:.1f}%\" if map_score > 0 else \"N/A\"\n",
    "    print(f\"YOLO{ver:<5} {year:<8} {fps:<8} {map_str:<15} {innovation:<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nğŸ’¡ Trend Analysis:\")\n",
    "print(\"  - Speed: 3.1x improvement (v1 to v5)\")\n",
    "print(\"  - Accuracy: 2.5x improvement (v2 to v8 on COCO)\")\n",
    "print(\"  - Architecture: Increasingly complex backbones with better feature extraction\")\n",
    "print(\"  - Focus: Balancing speed and accuracy for practical deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO å·¥ä½œåŸç†\n",
    "\n",
    "#### 1. ç¶²æ ¼åŠƒåˆ† (Grid Division)\n",
    "å°‡è¼¸å…¥åœ–åƒåŠƒåˆ†ç‚º SÃ—S ç¶²æ ¼ï¼ˆå¦‚ 13Ã—13ï¼‰\n",
    "\n",
    "#### 2. Anchor Boxes\n",
    "æ¯å€‹ç¶²æ ¼å–®å…ƒé æ¸¬ B å€‹é‚Šç•Œæ¡†ï¼ˆé€šå¸¸ B=3ï¼‰\n",
    "\n",
    "#### 3. é æ¸¬è¼¸å‡º\n",
    "æ¯å€‹é‚Šç•Œæ¡†é æ¸¬ï¼š\n",
    "- **ä½ç½®**: (x, y, w, h) - ä¸­å¿ƒåæ¨™å’Œå¯¬é«˜\n",
    "- **ä¿¡å¿ƒåº¦**: objectness score - åŒ…å«ç‰©é«”çš„æ¦‚ç‡\n",
    "- **é¡åˆ¥**: class probabilities - å„é¡åˆ¥çš„æ¦‚ç‡\n",
    "\n",
    "#### 4. è¼¸å‡ºå¼µé‡\n",
    "```\n",
    "Shape: S Ã— S Ã— B Ã— (5 + C)\n",
    "      â†“   â†“   â†“   â†“    â†“\n",
    "    Grid Anchors (x,y,w,h,conf) + Classes\n",
    "```\n",
    "\n",
    "#### 5. å¾Œè™•ç†\n",
    "- **Confidence Filtering**: éæ¿¾ä½ä¿¡å¿ƒåº¦æª¢æ¸¬\n",
    "- **NMS (Non-Maximum Suppression)**: ç§»é™¤é‡è¤‡æª¢æ¸¬\n",
    "\n",
    "### ç‚ºä»€éº¼é¸æ“‡ YOLOv3/v4ï¼Ÿ\n",
    "\n",
    "âœ… **YOLOv3 å„ªå‹¢**:\n",
    "- OpenCV DNN å®Œç¾æ”¯æ´\n",
    "- é è¨“ç·´æ¨¡å‹è±å¯Œï¼ˆCOCO 80 classesï¼‰\n",
    "- é€Ÿåº¦èˆ‡ç²¾åº¦å¹³è¡¡è‰¯å¥½\n",
    "- æ–‡æª”å’Œç¤¾ç¾¤æ”¯æ´å®Œå–„\n",
    "- é©åˆç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\n",
    "\n",
    "âœ… **YOLOv4 å„ªå‹¢**:\n",
    "- æ€§èƒ½æå‡é¡¯è‘—ï¼ˆ+10% mAPï¼‰\n",
    "- ä»ä¿æŒå¯¦æ™‚é€Ÿåº¦\n",
    "- æ›´å¥½çš„å°ç‰©é«”æª¢æ¸¬\n",
    "- å…ˆé€²çš„è¨“ç·´æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO architecture visualization\n",
    "print(\"YOLOv3 Architecture Overview\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInput: 416Ã—416Ã—3 RGB Image\")\n",
    "print(\"  â†“\")\n",
    "print(\"Darknet-53 Backbone (53 convolutional layers)\")\n",
    "print(\"  â†“\")\n",
    "print(\"Feature Pyramid Network (FPN)\")\n",
    "print(\"  â”œâ”€ Scale 1: 13Ã—13 grid (large objects)\")\n",
    "print(\"  â”‚   â””â”€ 3 anchors Ã— (5 + 80 classes) = 255 channels\")\n",
    "print(\"  â”œâ”€ Scale 2: 26Ã—26 grid (medium objects)\")\n",
    "print(\"  â”‚   â””â”€ 3 anchors Ã— (5 + 80 classes) = 255 channels\")\n",
    "print(\"  â””â”€ Scale 3: 52Ã—52 grid (small objects)\")\n",
    "print(\"      â””â”€ 3 anchors Ã— (5 + 80 classes) = 255 channels\")\n",
    "print(\"  â†“\")\n",
    "print(\"Post-processing\")\n",
    "print(\"  â”œâ”€ Confidence filtering (threshold: 0.5)\")\n",
    "print(\"  â””â”€ Non-Maximum Suppression (NMS threshold: 0.4)\")\n",
    "print(\"  â†“\")\n",
    "print(\"Output: Detected objects with [class, confidence, x, y, w, h]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate total predictions\n",
    "scales = [(13, 13), (26, 26), (52, 52)]\n",
    "anchors_per_scale = 3\n",
    "total_predictions = sum(w * h * anchors_per_scale for w, h in scales)\n",
    "\n",
    "print(f\"\\nTotal anchor boxes per image: {total_predictions:,}\")\n",
    "print(f\"  - 13Ã—13 scale: {13*13*3:,} predictions\")\n",
    "print(f\"  - 26Ã—26 scale: {26*26*3:,} predictions\")\n",
    "print(f\"  - 52Ã—52 scale: {52*52*3:,} predictions\")\n",
    "print(f\"\\nAfter NMS: Typically 1-100 final detections per image\")\n",
    "\n",
    "# Visualize multi-scale detection\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "scales_info = [(13, \"Large Objects\"), (26, \"Medium Objects\"), (52, \"Small Objects\")]\n",
    "\n",
    "for ax, (grid_size, obj_type) in zip(axes, scales_info):\n",
    "    # Create grid visualization\n",
    "    img = np.ones((grid_size, grid_size, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Draw grid\n",
    "    cell_size = 10\n",
    "    img_large = cv2.resize(img, (grid_size * cell_size, grid_size * cell_size), \n",
    "                          interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Draw grid lines\n",
    "    for i in range(grid_size + 1):\n",
    "        cv2.line(img_large, (0, i * cell_size), (grid_size * cell_size, i * cell_size), \n",
    "                (200, 200, 200), 1)\n",
    "        cv2.line(img_large, (i * cell_size, 0), (i * cell_size, grid_size * cell_size), \n",
    "                (200, 200, 200), 1)\n",
    "    \n",
    "    # Highlight a few cells\n",
    "    sample_cells = np.random.choice(range(1, grid_size-1), size=min(5, grid_size-2), replace=False)\n",
    "    for cell in sample_cells:\n",
    "        cv2.rectangle(img_large, \n",
    "                     (cell * cell_size, cell * cell_size),\n",
    "                     ((cell + 1) * cell_size, (cell + 1) * cell_size),\n",
    "                     (100, 149, 237), -1)\n",
    "    \n",
    "    ax.imshow(img_large)\n",
    "    ax.set_title(f'{grid_size}Ã—{grid_size} Grid\\n{obj_type}\\n'\n",
    "                f'{grid_size*grid_size*3:,} predictions',\n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('YOLOv3 Multi-Scale Detection', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Multi-scale detection allows YOLO to detect objects of various sizes effectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YOLOv3/v4 å¯¦ä½œ (Implementation) - 20%\n",
    "\n",
    "### æº–å‚™ YOLO æ¨¡å‹æ–‡ä»¶\n",
    "\n",
    "è¦é‹è¡Œ YOLOv3ï¼Œæˆ‘å€‘éœ€è¦ä»¥ä¸‹æ–‡ä»¶ï¼š\n",
    "\n",
    "1. **yolov3.cfg** - ç¶²çµ¡æ¶æ§‹é…ç½®\n",
    "2. **yolov3.weights** - é è¨“ç·´æ¬Šé‡ (~240MB)\n",
    "3. **coco.names** - COCO æ•¸æ“šé›†é¡åˆ¥åç¨±\n",
    "\n",
    "### COCO æ•¸æ“šé›† 80 é¡\n",
    "\n",
    "COCO (Common Objects in Context) åŒ…å« 80 å€‹å¸¸è¦‹ç‰©é«”é¡åˆ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup YOLO model directory\n",
    "YOLO_MODEL_DIR = Path('../assets/models/yolo')\n",
    "YOLO_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# COCO class names (80 classes)\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Save class names to file\n",
    "coco_names_path = YOLO_MODEL_DIR / 'coco.names'\n",
    "with open(coco_names_path, 'w') as f:\n",
    "    f.write('\\n'.join(COCO_CLASSES))\n",
    "\n",
    "print(f\"âœ… Saved COCO class names to {coco_names_path}\")\n",
    "print(f\"\\nCOCO Dataset: {len(COCO_CLASSES)} classes\")\n",
    "print(\"\\nCategories:\")\n",
    "categories = {\n",
    "    \"People & Animals\": COCO_CLASSES[0:1] + COCO_CLASSES[14:24],\n",
    "    \"Vehicles\": COCO_CLASSES[1:9],\n",
    "    \"Outdoor Objects\": COCO_CLASSES[9:14],\n",
    "    \"Accessories\": COCO_CLASSES[24:33],\n",
    "    \"Sports\": COCO_CLASSES[33:39],\n",
    "    \"Kitchen\": COCO_CLASSES[39:56],\n",
    "    \"Furniture\": COCO_CLASSES[56:62] + [COCO_CLASSES[63]],\n",
    "    \"Electronics\": COCO_CLASSES[62:63] + COCO_CLASSES[64:70],\n",
    "    \"Appliances\": COCO_CLASSES[70:75],\n",
    "    \"Indoor Objects\": COCO_CLASSES[75:80]\n",
    "}\n",
    "\n",
    "for category, items in categories.items():\n",
    "    print(f\"  {category}: {len(items)} classes\")\n",
    "    print(f\"    {', '.join(items[:5])}{'...' if len(items) > 5 else ''}\")\n",
    "\n",
    "# Generate colors for each class\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(COCO_CLASSES), 3), dtype=np.uint8)\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(COLORS)} unique colors for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for YOLOv3 model files\n",
    "yolov3_cfg = YOLO_MODEL_DIR / 'yolov3.cfg'\n",
    "yolov3_weights = YOLO_MODEL_DIR / 'yolov3.weights'\n",
    "\n",
    "# Alternative: YOLOv3-tiny (faster but less accurate)\n",
    "yolov3_tiny_cfg = YOLO_MODEL_DIR / 'yolov3-tiny.cfg'\n",
    "yolov3_tiny_weights = YOLO_MODEL_DIR / 'yolov3-tiny.weights'\n",
    "\n",
    "# Alternative: YOLOv4\n",
    "yolov4_cfg = YOLO_MODEL_DIR / 'yolov4.cfg'\n",
    "yolov4_weights = YOLO_MODEL_DIR / 'yolov4.weights'\n",
    "\n",
    "print(\"Checking for YOLO model files...\\n\")\n",
    "\n",
    "models_status = [\n",
    "    (\"YOLOv3\", yolov3_cfg, yolov3_weights, \"237 MB\"),\n",
    "    (\"YOLOv3-tiny\", yolov3_tiny_cfg, yolov3_tiny_weights, \"33 MB\"),\n",
    "    (\"YOLOv4\", yolov4_cfg, yolov4_weights, \"245 MB\")\n",
    "]\n",
    "\n",
    "available_models = []\n",
    "\n",
    "for model_name, cfg, weights, size in models_status:\n",
    "    cfg_exists = cfg.exists()\n",
    "    weights_exists = weights.exists()\n",
    "    \n",
    "    status = \"âœ…\" if (cfg_exists and weights_exists) else \"âš ï¸\"\n",
    "    print(f\"{status} {model_name}:\")\n",
    "    print(f\"   Config (.cfg):   {cfg.name:<25} {'Found' if cfg_exists else 'Not found'}\")\n",
    "    print(f\"   Weights (.weights): {weights.name:<22} {'Found' if weights_exists else 'Not found'} ({size})\")\n",
    "    print()\n",
    "    \n",
    "    if cfg_exists and weights_exists:\n",
    "        available_models.append((model_name, cfg, weights))\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(\"\\nâš ï¸  No YOLO models found. Please download:\")\n",
    "    print(\"\\nğŸ“¥ Download YOLOv3 (recommended):\")\n",
    "    print(\"   Config:  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\")\n",
    "    print(\"   Weights: https://pjreddie.com/media/files/yolov3.weights\")\n",
    "    print(\"\\nğŸ“¥ Download YOLOv3-tiny (faster, for testing):\")\n",
    "    print(\"   Config:  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg\")\n",
    "    print(\"   Weights: https://pjreddie.com/media/files/yolov3-tiny.weights\")\n",
    "    print(\"\\nğŸ“¥ Download YOLOv4 (best performance):\")\n",
    "    print(\"   Config:  https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg\")\n",
    "    print(\"   Weights: https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\")\n",
    "    print(f\"\\nSave files to: {YOLO_MODEL_DIR}/\")\n",
    "    YOLO_MODEL_AVAILABLE = False\n",
    "else:\n",
    "    print(f\"\\nâœ… Found {len(available_models)} YOLO model(s) ready to use\")\n",
    "    YOLO_MODEL_AVAILABLE = True\n",
    "    \n",
    "    # Use the first available model\n",
    "    DEFAULT_MODEL_NAME, DEFAULT_CFG, DEFAULT_WEIGHTS = available_models[0]\n",
    "    print(f\"\\nğŸ¯ Will use {DEFAULT_MODEL_NAME} for demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥ YOLO æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(cfg_path, weights_path, backend=cv2.dnn.DNN_BACKEND_OPENCV, \n",
    "                   target=cv2.dnn.DNN_TARGET_CPU):\n",
    "    \"\"\"\n",
    "    Load YOLO model using OpenCV DNN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cfg_path : str or Path\n",
    "        Path to .cfg file\n",
    "    weights_path : str or Path\n",
    "        Path to .weights file\n",
    "    backend : int\n",
    "        DNN backend (default: OpenCV)\n",
    "    target : int\n",
    "        DNN target device (default: CPU)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    net : cv2.dnn_Net\n",
    "        Loaded YOLO network\n",
    "    output_layers : list\n",
    "        Names of output layers\n",
    "    \"\"\"\n",
    "    print(f\"Loading YOLO model...\")\n",
    "    print(f\"  Config: {Path(cfg_path).name}\")\n",
    "    print(f\"  Weights: {Path(weights_path).name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load network\n",
    "    net = cv2.dnn.readNetFromDarknet(str(cfg_path), str(weights_path))\n",
    "    \n",
    "    # Set backend and target\n",
    "    net.setPreferableBackend(backend)\n",
    "    net.setPreferableTarget(target)\n",
    "    \n",
    "    # Get output layer names\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… Model loaded in {load_time:.2f}s\")\n",
    "    print(f\"   Total layers: {len(layer_names)}\")\n",
    "    print(f\"   Output layers: {output_layers}\")\n",
    "    \n",
    "    return net, output_layers\n",
    "\n",
    "\n",
    "# Load model if available\n",
    "if YOLO_MODEL_AVAILABLE:\n",
    "    yolo_net, yolo_output_layers = load_yolo_model(DEFAULT_CFG, DEFAULT_WEIGHTS)\n",
    "    print(f\"\\nğŸ¯ {DEFAULT_MODEL_NAME} ready for inference\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Skipping model loading - no YOLO models available\")\n",
    "    yolo_net = None\n",
    "    yolo_output_layers = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO æª¢æ¸¬å‡½æ•¸\n",
    "\n",
    "å¯¦ç¾å®Œæ•´çš„ YOLO æª¢æ¸¬æµç¨‹ï¼š\n",
    "1. åœ–åƒé è™•ç†ï¼ˆblobFromImageï¼‰\n",
    "2. å‰å‘å‚³æ’­\n",
    "3. è§£æè¼¸å‡º\n",
    "4. ä¿¡å¿ƒåº¦éæ¿¾\n",
    "5. Non-Maximum Suppression (NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_yolo(image, net, output_layers, classes=COCO_CLASSES,\n",
    "                       confidence_threshold=0.5, nms_threshold=0.4, \n",
    "                       input_size=(416, 416)):\n",
    "    \"\"\"\n",
    "    Detect objects using YOLO\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image (BGR)\n",
    "    net : cv2.dnn_Net\n",
    "        YOLO network\n",
    "    output_layers : list\n",
    "        Output layer names\n",
    "    classes : list\n",
    "        Class names\n",
    "    confidence_threshold : float\n",
    "        Minimum confidence (0.0-1.0)\n",
    "    nms_threshold : float\n",
    "        NMS IoU threshold (0.0-1.0)\n",
    "    input_size : tuple\n",
    "        Network input size (width, height)\n",
    "        Common sizes: (320, 320), (416, 416), (608, 608)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    detections : list\n",
    "        List of (class_id, class_name, confidence, x, y, w, h) tuples\n",
    "    inference_time : float\n",
    "        Inference time in seconds\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Prepare blob from image\n",
    "    # YOLO expects normalized input (1/255 scaling)\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image,\n",
    "        scalefactor=1/255.0,\n",
    "        size=input_size,\n",
    "        mean=(0, 0, 0),\n",
    "        swapRB=True,  # BGR to RGB\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    net.setInput(blob)\n",
    "    start_time = time.time()\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Parse detections\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            # Detection format: [x, y, w, h, objectness, class1_prob, class2_prob, ...]\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > confidence_threshold:\n",
    "                # YOLO returns center coordinates and dimensions\n",
    "                # Scale to image size\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Convert to top-left corner coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply Non-Maximum Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    \n",
    "    # Prepare final detections\n",
    "    detections = []\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            class_id = class_ids[i]\n",
    "            confidence = confidences[i]\n",
    "            class_name = classes[class_id] if class_id < len(classes) else \"unknown\"\n",
    "            \n",
    "            detections.append((class_id, class_name, confidence, x, y, w, h))\n",
    "    \n",
    "    return detections, inference_time\n",
    "\n",
    "\n",
    "def draw_yolo_detections(image, detections, colors=COLORS, thickness=2, font_scale=0.5):\n",
    "    \"\"\"\n",
    "    Draw YOLO detection results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    detections : list\n",
    "        Detection results from detect_objects_yolo()\n",
    "    colors : np.ndarray\n",
    "        Color array for each class\n",
    "    thickness : int\n",
    "        Box line thickness\n",
    "    font_scale : float\n",
    "        Label font scale\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Image with drawn detections\n",
    "    \"\"\"\n",
    "    output = image.copy()\n",
    "    \n",
    "    for (class_id, class_name, confidence, x, y, w, h) in detections:\n",
    "        # Get color for this class\n",
    "        color = colors[class_id].tolist() if class_id < len(colors) else [0, 255, 0]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Prepare label\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        \n",
    "        # Get label size\n",
    "        (label_w, label_h), baseline = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1\n",
    "        )\n",
    "        \n",
    "        # Draw label background\n",
    "        cv2.rectangle(output,\n",
    "                     (x, y - label_h - baseline - 5),\n",
    "                     (x + label_w, y),\n",
    "                     color, -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(output, label,\n",
    "                   (x, y - baseline - 2),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"âœ… YOLO detection functions defined\")\n",
    "print(\"\\nFunctions:\")\n",
    "print(\"  - detect_objects_yolo(): Complete detection pipeline\")\n",
    "print(\"  - draw_yolo_detections(): Visualize results with bounding boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦ YOLO æª¢æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if YOLO_MODEL_AVAILABLE and yolo_net is not None:\n",
    "    # Load test image\n",
    "    test_image_paths = [\n",
    "        '../assets/images/basic/assassin.jpg',\n",
    "        '../assets/images/basic/1.jpg',\n",
    "        '../assets/images/basic/2.jpg',\n",
    "        '../assets/images/objects/street.jpg'\n",
    "    ]\n",
    "    \n",
    "    test_img = None\n",
    "    for path in test_image_paths:\n",
    "        if Path(path).exists():\n",
    "            test_img = cv2.imread(path)\n",
    "            print(f\"ğŸ“· Loaded test image: {path}\")\n",
    "            print(f\"   Size: {test_img.shape[1]}Ã—{test_img.shape[0]}\")\n",
    "            break\n",
    "    \n",
    "    if test_img is None:\n",
    "        # Create demo image\n",
    "        test_img = np.ones((480, 640, 3), dtype=np.uint8) * 200\n",
    "        cv2.putText(test_img, \"No test image found\", (150, 240),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "        print(\"âš ï¸  Created placeholder image\")\n",
    "    \n",
    "    # Detect objects\n",
    "    print(\"\\nğŸ” Running YOLO detection...\")\n",
    "    detections, inf_time = detect_objects_yolo(\n",
    "        test_img, yolo_net, yolo_output_layers,\n",
    "        confidence_threshold=0.5,\n",
    "        nms_threshold=0.4,\n",
    "        input_size=(416, 416)\n",
    "    )\n",
    "    \n",
    "    # Draw results\n",
    "    result_img = draw_yolo_detections(test_img, detections)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    fps = 1.0 / inf_time if inf_time > 0 else 0\n",
    "    axes[1].set_title(f'{DEFAULT_MODEL_NAME} Detection\\n'\n",
    "                     f'Objects: {len(detections)} | '\n",
    "                     f'Time: {inf_time*1000:.1f}ms | '\n",
    "                     f'FPS: {fps:.1f}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"\\nğŸ“Š Detection Results:\")\n",
    "    print(f\"   Inference time: {inf_time*1000:.2f}ms\")\n",
    "    print(f\"   Throughput: {fps:.2f} FPS\")\n",
    "    print(f\"   Objects detected: {len(detections)}\\n\")\n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        print(\"   Detected objects:\")\n",
    "        # Group by class\n",
    "        class_counts = defaultdict(list)\n",
    "        for (cid, cname, conf, x, y, w, h) in detections:\n",
    "            class_counts[cname].append(conf)\n",
    "        \n",
    "        for cname, confs in sorted(class_counts.items()):\n",
    "            avg_conf = np.mean(confs)\n",
    "            print(f\"     - {cname}: {len(confs)} instance(s), avg conf: {avg_conf:.3f}\")\n",
    "    else:\n",
    "        print(\"   No objects detected. Try:\")\n",
    "        print(\"     - Lowering confidence_threshold (e.g., 0.3)\")\n",
    "        print(\"     - Using a different test image\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  Skipping YOLO detection demo - model not available\")\n",
    "    print(\"    Please download YOLO model files first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
