{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.2 實時物體檢測 (Real-time Object Detection)\n",
    "\n",
    "**WBS 5.2.2**: YOLO 系列與實時檢測技術\n",
    "\n",
    "本模組深入探討實時物體檢測技術，重點學習 YOLO (You Only Look Once) 系列算法，從基礎原理到生產級部署。\n",
    "\n",
    "## 學習目標\n",
    "- 理解實時物體檢測的核心要求與挑戰\n",
    "- 掌握 YOLO 系列算法的架構演進\n",
    "- 實作 YOLOv3/v4 完整檢測流程\n",
    "- 學習性能優化與多線程處理技術\n",
    "- 實現生產級實時視訊檢測系統\n",
    "\n",
    "## 前置知識\n",
    "- OpenCV DNN 模組 (WBS 5.2.1)\n",
    "- 圖像預處理技術 (Stage 3)\n",
    "- Python 多線程基礎\n",
    "- 基礎深度學習概念\n",
    "\n",
    "## 課程大綱\n",
    "1. 實時檢測基礎 (Real-time Detection Basics) - 5%\n",
    "2. YOLO 系列介紹 (YOLO Series Introduction) - 15%\n",
    "3. YOLOv3/v4 實作 (YOLOv3/v4 Implementation) - 20%\n",
    "4. 性能優化技巧 (Performance Optimization) - 15%\n",
    "5. 多線程處理 (Multi-threading) - 10%\n",
    "6. 批次推理 (Batch Inference) - 10%\n",
    "7. 實時視訊檢測 (Real-time Video Detection) - 15%\n",
    "8. 檢測結果追蹤 (Object Tracking) - 5%\n",
    "9. 實戰練習 (Hands-on Exercises) - 3%\n",
    "10. 總結與部署 (Summary & Deployment) - 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request\n",
    "from collections import defaultdict, deque\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Configure matplotlib for Chinese display\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Disable warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "try:\n",
    "    cuda_devices = cv2.cuda.getCudaEnabledDeviceCount()\n",
    "    print(f\"CUDA devices available: {cuda_devices}\")\n",
    "except:\n",
    "    print(\"CUDA not available - will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 實時檢測基礎 (Real-time Detection Basics) - 5%\n",
    "\n",
    "### 什麼是實時物體檢測？\n",
    "\n",
    "**實時物體檢測**是指能夠在視訊流中以足夠高的幀率（通常 ≥30 FPS）檢測並定位多個物體的技術。\n",
    "\n",
    "### 核心要求\n",
    "\n",
    "#### 1. 速度要求 (Speed Requirements)\n",
    "- **實時標準**: ≥30 FPS (每幀 ≤33ms)\n",
    "- **流暢體驗**: ≥20 FPS (每幀 ≤50ms)\n",
    "- **可接受**: ≥10 FPS (每幀 ≤100ms)\n",
    "- **不流暢**: <10 FPS\n",
    "\n",
    "#### 2. 準確度要求 (Accuracy Requirements)\n",
    "- **mAP (mean Average Precision)**: 通用評估指標\n",
    "- **IoU Threshold**: 通常 0.5 或 0.75\n",
    "- **Recall**: 檢測到的物體比例\n",
    "- **Precision**: 正確檢測的比例\n",
    "\n",
    "#### 3. 延遲要求 (Latency Requirements)\n",
    "- **端到端延遲**: 從圖像輸入到結果輸出\n",
    "- **處理延遲**: 模型推理時間\n",
    "- **通信延遲**: 數據傳輸時間\n",
    "\n",
    "### 性能權衡 (Speed-Accuracy Trade-off)\n",
    "\n",
    "```\n",
    "High Accuracy (Slower)           Real-time (Faster)\n",
    "        ↓                              ↓\n",
    "  Mask R-CNN  →  Faster R-CNN  →  SSD  →  YOLO\n",
    "    5 FPS          7 FPS        25 FPS   45+ FPS\n",
    "    mAP: 39%       mAP: 37%     mAP: 31%  mAP: 33%\n",
    "```\n",
    "\n",
    "### 實時檢測挑戰\n",
    "\n",
    "1. **計算資源限制**: CPU vs GPU vs 邊緣設備\n",
    "2. **模型大小**: 內存佔用與載入時間\n",
    "3. **多尺度檢測**: 小物體與大物體同時檢測\n",
    "4. **遮擋與擁擠**: 重疊物體的分離\n",
    "5. **光照與視角變化**: 魯棒性要求\n",
    "\n",
    "### 應用場景\n",
    "\n",
    "✅ **適合實時檢測**:\n",
    "- 自動駕駛 (車輛、行人、交通標誌)\n",
    "- 智能監控 (入侵檢測、異常行為)\n",
    "- 機器人視覺 (導航、抓取)\n",
    "- AR/VR 應用 (物體識別與追蹤)\n",
    "- 工業檢測 (缺陷檢測)\n",
    "\n",
    "❌ **不適合實時檢測**:\n",
    "- 醫療影像分析 (高精度優先)\n",
    "- 衛星圖像分析 (離線處理)\n",
    "- 藝術品修復 (質量優先)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance requirements comparison\n",
    "performance_data = {\n",
    "    \"Application\": [\n",
    "        \"Autonomous Driving\",\n",
    "        \"Smart Surveillance\",\n",
    "        \"Robot Navigation\",\n",
    "        \"AR/VR\",\n",
    "        \"Industrial Inspection\",\n",
    "        \"Face Recognition\"\n",
    "    ],\n",
    "    \"Required FPS\": [30, 20, 25, 60, 15, 30],\n",
    "    \"Accuracy Priority\": [\"High\", \"Medium\", \"Medium\", \"Medium\", \"Very High\", \"High\"],\n",
    "    \"Latency Budget (ms)\": [33, 50, 40, 16, 67, 33]\n",
    "}\n",
    "\n",
    "print(\"Real-time Detection Requirements by Application\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Application':<25} {'Required FPS':<15} {'Accuracy':<20} {'Latency (ms)'}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "for i in range(len(performance_data[\"Application\"])):\n",
    "    print(f\"{performance_data['Application'][i]:<25} \"\n",
    "          f\"{performance_data['Required FPS'][i]:<15} \"\n",
    "          f\"{performance_data['Accuracy Priority'][i]:<20} \"\n",
    "          f\"{performance_data['Latency Budget (ms)'][i]}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# Visualize FPS requirements\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "apps = performance_data[\"Application\"]\n",
    "fps = performance_data[\"Required FPS\"]\n",
    "\n",
    "colors = ['#FF6B6B' if f >= 30 else '#4ECDC4' if f >= 20 else '#95E1D3' for f in fps]\n",
    "bars = ax.barh(apps, fps, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add FPS threshold lines\n",
    "ax.axvline(x=30, color='red', linestyle='--', linewidth=2, label='Real-time (30 FPS)', alpha=0.7)\n",
    "ax.axvline(x=20, color='orange', linestyle='--', linewidth=2, label='Smooth (20 FPS)', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for i, (app, f) in enumerate(zip(apps, fps)):\n",
    "    ax.text(f + 1, i, f'{f} FPS', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Frames Per Second (FPS)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('FPS Requirements by Application Domain', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Insight: Most real-time applications require ≥20 FPS for smooth operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YOLO 系列介紹 (YOLO Series Introduction) - 15%\n",
    "\n",
    "### YOLO: You Only Look Once\n",
    "\n",
    "**YOLO** 是一種革命性的單階段物體檢測算法，由 Joseph Redmon 於 2015 年提出。核心思想是將物體檢測視為**回歸問題**，直接從圖像像素預測邊界框和類別概率。\n",
    "\n",
    "### 核心創新\n",
    "\n",
    "#### 1. 單階段檢測 (Single-Shot Detection)\n",
    "- **傳統兩階段**: Region Proposal → Classification (R-CNN 系列)\n",
    "- **YOLO 單階段**: 一次前向傳播完成檢測\n",
    "- **優勢**: 速度快，端到端訓練\n",
    "\n",
    "#### 2. 全局上下文 (Global Context)\n",
    "- 看整張圖像，而非局部區域\n",
    "- 減少背景誤檢測\n",
    "- 更好的物體關係理解\n",
    "\n",
    "#### 3. 統一架構 (Unified Architecture)\n",
    "```\n",
    "Input Image → CNN Backbone → Detection Head → Outputs\n",
    "                                               ↓\n",
    "                          [Bounding Boxes + Class Probabilities]\n",
    "```\n",
    "\n",
    "### YOLO 版本演進\n",
    "\n",
    "#### YOLOv1 (2015) - 開創者\n",
    "- **架構**: 24 convolutional layers + 2 fully connected layers\n",
    "- **輸入**: 448×448\n",
    "- **速度**: 45 FPS (base), 155 FPS (fast)\n",
    "- **mAP**: 63.4% (VOC 2007)\n",
    "- **缺點**: 小物體檢測差，定位精度低\n",
    "\n",
    "#### YOLOv2 / YOLO9000 (2016) - 改進版\n",
    "- **創新**: \n",
    "  - Batch Normalization\n",
    "  - Anchor Boxes (借鑒 Faster R-CNN)\n",
    "  - Darknet-19 backbone (19 layers)\n",
    "  - Multi-scale training\n",
    "- **速度**: 40-90 FPS\n",
    "- **mAP**: 78.6% (VOC 2007)\n",
    "- **特點**: 可檢測 9000+ 類別\n",
    "\n",
    "#### YOLOv3 (2018) - 多尺度檢測\n",
    "- **架構**: Darknet-53 (53 layers)\n",
    "- **創新**:\n",
    "  - 3 個尺度的特徵圖 (13×13, 26×26, 52×52)\n",
    "  - 每個尺度 3 個 anchor boxes\n",
    "  - Residual connections\n",
    "  - 改進的小物體檢測\n",
    "- **速度**: 20-30 FPS (416×416)\n",
    "- **mAP**: 57.9% (COCO)\n",
    "- **狀態**: 本模組重點\n",
    "\n",
    "#### YOLOv4 (2020) - 性能巔峰\n",
    "- **架構**: CSPDarknet53 + SPP + PANet\n",
    "- **創新**:\n",
    "  - CSPNet (Cross Stage Partial Network)\n",
    "  - Mish activation\n",
    "  - DropBlock regularization\n",
    "  - Mosaic data augmentation\n",
    "  - CIoU loss\n",
    "- **速度**: 65 FPS (Tesla V100)\n",
    "- **mAP**: 43.5% (COCO)\n",
    "- **特點**: 最佳速度/精度權衡\n",
    "\n",
    "#### YOLOv5 (2020) - PyTorch 重寫\n",
    "- **特點**: PyTorch 實現，易用性高\n",
    "- **版本**: YOLOv5s/m/l/x (不同大小)\n",
    "- **速度**: 140 FPS (YOLOv5s)\n",
    "- **mAP**: 37.4-50.7% (COCO)\n",
    "- **生態**: 部署工具豐富\n",
    "\n",
    "#### YOLOv6-v8 (2022-2023) - 持續演進\n",
    "- **YOLOv6**: 工業界優化 (美團)\n",
    "- **YOLOv7**: 架構創新 (Edge TPU 優化)\n",
    "- **YOLOv8**: Ultralytics 最新版\n",
    "  - 支持檢測、分割、分類、姿態估計\n",
    "  - 統一 API 設計\n",
    "  - mAP: 53.9% (YOLOv8x)\n",
    "\n",
    "### YOLO vs 其他檢測器\n",
    "\n",
    "| Model | Type | FPS | mAP (COCO) | Year |\n",
    "|-------|------|-----|------------|------|\n",
    "| **R-CNN** | Two-stage | 0.05 | - | 2014 |\n",
    "| **Fast R-CNN** | Two-stage | 0.5 | - | 2015 |\n",
    "| **Faster R-CNN** | Two-stage | 7 | 42.7% | 2015 |\n",
    "| **YOLOv1** | One-stage | 45 | - | 2015 |\n",
    "| **SSD300** | One-stage | 46 | 25.1% | 2016 |\n",
    "| **YOLOv2** | One-stage | 67 | 21.6% | 2016 |\n",
    "| **YOLOv3** | One-stage | 30 | 33.0% | 2018 |\n",
    "| **YOLOv4** | One-stage | 65 | 43.5% | 2020 |\n",
    "| **EfficientDet** | One-stage | 30 | 51.0% | 2020 |\n",
    "| **YOLOv8** | One-stage | 80+ | 53.9% | 2023 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO evolution visualization\n",
    "yolo_evolution = {\n",
    "    \"Version\": [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v8\"],\n",
    "    \"Year\": [2015, 2016, 2018, 2020, 2020, 2023],\n",
    "    \"FPS\": [45, 67, 30, 65, 140, 80],\n",
    "    \"mAP\": [0.0, 21.6, 33.0, 43.5, 37.4, 53.9],  # COCO dataset\n",
    "    \"Backbone\": [\"Custom\", \"Darknet-19\", \"Darknet-53\", \"CSPDarknet53\", \"CSPDarknet\", \"CSPDarknet\"],\n",
    "    \"Key Innovation\": [\n",
    "        \"Single-stage detection\",\n",
    "        \"Anchor boxes\",\n",
    "        \"Multi-scale FPN\",\n",
    "        \"CSPNet + Bag of Freebies\",\n",
    "        \"PyTorch implementation\",\n",
    "        \"Unified architecture\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Speed evolution\n",
    "versions = yolo_evolution[\"Version\"]\n",
    "fps = yolo_evolution[\"FPS\"]\n",
    "years = yolo_evolution[\"Year\"]\n",
    "\n",
    "colors_fps = plt.cm.viridis(np.linspace(0, 1, len(versions)))\n",
    "ax1.plot(years, fps, marker='o', linewidth=3, markersize=12, color='blue', alpha=0.7)\n",
    "for i, (year, f, ver) in enumerate(zip(years, fps, versions)):\n",
    "    ax1.scatter(year, f, s=300, c=[colors_fps[i]], edgecolor='black', linewidth=2, zorder=5)\n",
    "    ax1.text(year, f + 8, f'YOLO{ver}\\n{f} FPS', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Speed (FPS)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('YOLO Speed Evolution', fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(True, alpha=0.3, linestyle=':')\n",
    "ax1.set_ylim(0, 160)\n",
    "\n",
    "# Plot 2: Accuracy evolution\n",
    "map_scores = yolo_evolution[\"mAP\"]\n",
    "# Filter out v1 (no COCO mAP)\n",
    "valid_indices = [i for i, m in enumerate(map_scores) if m > 0]\n",
    "valid_years = [years[i] for i in valid_indices]\n",
    "valid_map = [map_scores[i] for i in valid_indices]\n",
    "valid_versions = [versions[i] for i in valid_indices]\n",
    "\n",
    "colors_map = plt.cm.plasma(np.linspace(0, 1, len(valid_versions)))\n",
    "ax2.plot(valid_years, valid_map, marker='s', linewidth=3, markersize=12, color='red', alpha=0.7)\n",
    "for i, (year, m, ver) in enumerate(zip(valid_years, valid_map, valid_versions)):\n",
    "    ax2.scatter(year, m, s=300, c=[colors_map[i]], edgecolor='black', linewidth=2, zorder=5)\n",
    "    ax2.text(year, m + 2.5, f'YOLO{ver}\\n{m:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('mAP on COCO (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('YOLO Accuracy Evolution', fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.grid(True, alpha=0.3, linestyle=':')\n",
    "ax2.set_ylim(0, 60)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\nYOLO Version Comparison\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Version':<10} {'Year':<8} {'FPS':<8} {'mAP (COCO)':<15} {'Key Innovation':<50}\")\n",
    "print(\"=\" * 100)\n",
    "for i in range(len(yolo_evolution[\"Version\"])):\n",
    "    ver = yolo_evolution[\"Version\"][i]\n",
    "    year = yolo_evolution[\"Year\"][i]\n",
    "    fps = yolo_evolution[\"FPS\"][i]\n",
    "    map_score = yolo_evolution[\"mAP\"][i]\n",
    "    innovation = yolo_evolution[\"Key Innovation\"][i]\n",
    "    \n",
    "    map_str = f\"{map_score:.1f}%\" if map_score > 0 else \"N/A\"\n",
    "    print(f\"YOLO{ver:<5} {year:<8} {fps:<8} {map_str:<15} {innovation:<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n💡 Trend Analysis:\")\n",
    "print(\"  - Speed: 3.1x improvement (v1 to v5)\")\n",
    "print(\"  - Accuracy: 2.5x improvement (v2 to v8 on COCO)\")\n",
    "print(\"  - Architecture: Increasingly complex backbones with better feature extraction\")\n",
    "print(\"  - Focus: Balancing speed and accuracy for practical deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 工作原理\n",
    "\n",
    "#### 1. 網格劃分 (Grid Division)\n",
    "將輸入圖像劃分為 S×S 網格（如 13×13）\n",
    "\n",
    "#### 2. Anchor Boxes\n",
    "每個網格單元預測 B 個邊界框（通常 B=3）\n",
    "\n",
    "#### 3. 預測輸出\n",
    "每個邊界框預測：\n",
    "- **位置**: (x, y, w, h) - 中心坐標和寬高\n",
    "- **信心度**: objectness score - 包含物體的概率\n",
    "- **類別**: class probabilities - 各類別的概率\n",
    "\n",
    "#### 4. 輸出張量\n",
    "```\n",
    "Shape: S × S × B × (5 + C)\n",
    "      ↓   ↓   ↓   ↓    ↓\n",
    "    Grid Anchors (x,y,w,h,conf) + Classes\n",
    "```\n",
    "\n",
    "#### 5. 後處理\n",
    "- **Confidence Filtering**: 過濾低信心度檢測\n",
    "- **NMS (Non-Maximum Suppression)**: 移除重複檢測\n",
    "\n",
    "### 為什麼選擇 YOLOv3/v4？\n",
    "\n",
    "✅ **YOLOv3 優勢**:\n",
    "- OpenCV DNN 完美支援\n",
    "- 預訓練模型豐富（COCO 80 classes）\n",
    "- 速度與精度平衡良好\n",
    "- 文檔和社群支援完善\n",
    "- 適合生產環境部署\n",
    "\n",
    "✅ **YOLOv4 優勢**:\n",
    "- 性能提升顯著（+10% mAP）\n",
    "- 仍保持實時速度\n",
    "- 更好的小物體檢測\n",
    "- 先進的訓練技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO architecture visualization\n",
    "print(\"YOLOv3 Architecture Overview\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInput: 416×416×3 RGB Image\")\n",
    "print(\"  ↓\")\n",
    "print(\"Darknet-53 Backbone (53 convolutional layers)\")\n",
    "print(\"  ↓\")\n",
    "print(\"Feature Pyramid Network (FPN)\")\n",
    "print(\"  ├─ Scale 1: 13×13 grid (large objects)\")\n",
    "print(\"  │   └─ 3 anchors × (5 + 80 classes) = 255 channels\")\n",
    "print(\"  ├─ Scale 2: 26×26 grid (medium objects)\")\n",
    "print(\"  │   └─ 3 anchors × (5 + 80 classes) = 255 channels\")\n",
    "print(\"  └─ Scale 3: 52×52 grid (small objects)\")\n",
    "print(\"      └─ 3 anchors × (5 + 80 classes) = 255 channels\")\n",
    "print(\"  ↓\")\n",
    "print(\"Post-processing\")\n",
    "print(\"  ├─ Confidence filtering (threshold: 0.5)\")\n",
    "print(\"  └─ Non-Maximum Suppression (NMS threshold: 0.4)\")\n",
    "print(\"  ↓\")\n",
    "print(\"Output: Detected objects with [class, confidence, x, y, w, h]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate total predictions\n",
    "scales = [(13, 13), (26, 26), (52, 52)]\n",
    "anchors_per_scale = 3\n",
    "total_predictions = sum(w * h * anchors_per_scale for w, h in scales)\n",
    "\n",
    "print(f\"\\nTotal anchor boxes per image: {total_predictions:,}\")\n",
    "print(f\"  - 13×13 scale: {13*13*3:,} predictions\")\n",
    "print(f\"  - 26×26 scale: {26*26*3:,} predictions\")\n",
    "print(f\"  - 52×52 scale: {52*52*3:,} predictions\")\n",
    "print(f\"\\nAfter NMS: Typically 1-100 final detections per image\")\n",
    "\n",
    "# Visualize multi-scale detection\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "scales_info = [(13, \"Large Objects\"), (26, \"Medium Objects\"), (52, \"Small Objects\")]\n",
    "\n",
    "for ax, (grid_size, obj_type) in zip(axes, scales_info):\n",
    "    # Create grid visualization\n",
    "    img = np.ones((grid_size, grid_size, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Draw grid\n",
    "    cell_size = 10\n",
    "    img_large = cv2.resize(img, (grid_size * cell_size, grid_size * cell_size), \n",
    "                          interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Draw grid lines\n",
    "    for i in range(grid_size + 1):\n",
    "        cv2.line(img_large, (0, i * cell_size), (grid_size * cell_size, i * cell_size), \n",
    "                (200, 200, 200), 1)\n",
    "        cv2.line(img_large, (i * cell_size, 0), (i * cell_size, grid_size * cell_size), \n",
    "                (200, 200, 200), 1)\n",
    "    \n",
    "    # Highlight a few cells\n",
    "    sample_cells = np.random.choice(range(1, grid_size-1), size=min(5, grid_size-2), replace=False)\n",
    "    for cell in sample_cells:\n",
    "        cv2.rectangle(img_large, \n",
    "                     (cell * cell_size, cell * cell_size),\n",
    "                     ((cell + 1) * cell_size, (cell + 1) * cell_size),\n",
    "                     (100, 149, 237), -1)\n",
    "    \n",
    "    ax.imshow(img_large)\n",
    "    ax.set_title(f'{grid_size}×{grid_size} Grid\\n{obj_type}\\n'\n",
    "                f'{grid_size*grid_size*3:,} predictions',\n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('YOLOv3 Multi-Scale Detection', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Multi-scale detection allows YOLO to detect objects of various sizes effectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YOLOv3/v4 實作 (Implementation) - 20%\n",
    "\n",
    "### 準備 YOLO 模型文件\n",
    "\n",
    "要運行 YOLOv3，我們需要以下文件：\n",
    "\n",
    "1. **yolov3.cfg** - 網絡架構配置\n",
    "2. **yolov3.weights** - 預訓練權重 (~240MB)\n",
    "3. **coco.names** - COCO 數據集類別名稱\n",
    "\n",
    "### COCO 數據集 80 類\n",
    "\n",
    "COCO (Common Objects in Context) 包含 80 個常見物體類別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup YOLO model directory\n",
    "YOLO_MODEL_DIR = Path('../assets/models/yolo')\n",
    "YOLO_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# COCO class names (80 classes)\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Save class names to file\n",
    "coco_names_path = YOLO_MODEL_DIR / 'coco.names'\n",
    "with open(coco_names_path, 'w') as f:\n",
    "    f.write('\\n'.join(COCO_CLASSES))\n",
    "\n",
    "print(f\"✅ Saved COCO class names to {coco_names_path}\")\n",
    "print(f\"\\nCOCO Dataset: {len(COCO_CLASSES)} classes\")\n",
    "print(\"\\nCategories:\")\n",
    "categories = {\n",
    "    \"People & Animals\": COCO_CLASSES[0:1] + COCO_CLASSES[14:24],\n",
    "    \"Vehicles\": COCO_CLASSES[1:9],\n",
    "    \"Outdoor Objects\": COCO_CLASSES[9:14],\n",
    "    \"Accessories\": COCO_CLASSES[24:33],\n",
    "    \"Sports\": COCO_CLASSES[33:39],\n",
    "    \"Kitchen\": COCO_CLASSES[39:56],\n",
    "    \"Furniture\": COCO_CLASSES[56:62] + [COCO_CLASSES[63]],\n",
    "    \"Electronics\": COCO_CLASSES[62:63] + COCO_CLASSES[64:70],\n",
    "    \"Appliances\": COCO_CLASSES[70:75],\n",
    "    \"Indoor Objects\": COCO_CLASSES[75:80]\n",
    "}\n",
    "\n",
    "for category, items in categories.items():\n",
    "    print(f\"  {category}: {len(items)} classes\")\n",
    "    print(f\"    {', '.join(items[:5])}{'...' if len(items) > 5 else ''}\")\n",
    "\n",
    "# Generate colors for each class\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(COCO_CLASSES), 3), dtype=np.uint8)\n",
    "\n",
    "print(f\"\\n✅ Generated {len(COLORS)} unique colors for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for YOLOv3 model files\n",
    "yolov3_cfg = YOLO_MODEL_DIR / 'yolov3.cfg'\n",
    "yolov3_weights = YOLO_MODEL_DIR / 'yolov3.weights'\n",
    "\n",
    "# Alternative: YOLOv3-tiny (faster but less accurate)\n",
    "yolov3_tiny_cfg = YOLO_MODEL_DIR / 'yolov3-tiny.cfg'\n",
    "yolov3_tiny_weights = YOLO_MODEL_DIR / 'yolov3-tiny.weights'\n",
    "\n",
    "# Alternative: YOLOv4\n",
    "yolov4_cfg = YOLO_MODEL_DIR / 'yolov4.cfg'\n",
    "yolov4_weights = YOLO_MODEL_DIR / 'yolov4.weights'\n",
    "\n",
    "print(\"Checking for YOLO model files...\\n\")\n",
    "\n",
    "models_status = [\n",
    "    (\"YOLOv3\", yolov3_cfg, yolov3_weights, \"237 MB\"),\n",
    "    (\"YOLOv3-tiny\", yolov3_tiny_cfg, yolov3_tiny_weights, \"33 MB\"),\n",
    "    (\"YOLOv4\", yolov4_cfg, yolov4_weights, \"245 MB\")\n",
    "]\n",
    "\n",
    "available_models = []\n",
    "\n",
    "for model_name, cfg, weights, size in models_status:\n",
    "    cfg_exists = cfg.exists()\n",
    "    weights_exists = weights.exists()\n",
    "    \n",
    "    status = \"✅\" if (cfg_exists and weights_exists) else \"⚠️\"\n",
    "    print(f\"{status} {model_name}:\")\n",
    "    print(f\"   Config (.cfg):   {cfg.name:<25} {'Found' if cfg_exists else 'Not found'}\")\n",
    "    print(f\"   Weights (.weights): {weights.name:<22} {'Found' if weights_exists else 'Not found'} ({size})\")\n",
    "    print()\n",
    "    \n",
    "    if cfg_exists and weights_exists:\n",
    "        available_models.append((model_name, cfg, weights))\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(\"\\n⚠️  No YOLO models found. Please download:\")\n",
    "    print(\"\\n📥 Download YOLOv3 (recommended):\")\n",
    "    print(\"   Config:  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\")\n",
    "    print(\"   Weights: https://pjreddie.com/media/files/yolov3.weights\")\n",
    "    print(\"\\n📥 Download YOLOv3-tiny (faster, for testing):\")\n",
    "    print(\"   Config:  https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg\")\n",
    "    print(\"   Weights: https://pjreddie.com/media/files/yolov3-tiny.weights\")\n",
    "    print(\"\\n📥 Download YOLOv4 (best performance):\")\n",
    "    print(\"   Config:  https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg\")\n",
    "    print(\"   Weights: https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\")\n",
    "    print(f\"\\nSave files to: {YOLO_MODEL_DIR}/\")\n",
    "    YOLO_MODEL_AVAILABLE = False\n",
    "else:\n",
    "    print(f\"\\n✅ Found {len(available_models)} YOLO model(s) ready to use\")\n",
    "    YOLO_MODEL_AVAILABLE = True\n",
    "    \n",
    "    # Use the first available model\n",
    "    DEFAULT_MODEL_NAME, DEFAULT_CFG, DEFAULT_WEIGHTS = available_models[0]\n",
    "    print(f\"\\n🎯 Will use {DEFAULT_MODEL_NAME} for demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入 YOLO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(cfg_path, weights_path, backend=cv2.dnn.DNN_BACKEND_OPENCV, \n",
    "                   target=cv2.dnn.DNN_TARGET_CPU):\n",
    "    \"\"\"\n",
    "    Load YOLO model using OpenCV DNN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cfg_path : str or Path\n",
    "        Path to .cfg file\n",
    "    weights_path : str or Path\n",
    "        Path to .weights file\n",
    "    backend : int\n",
    "        DNN backend (default: OpenCV)\n",
    "    target : int\n",
    "        DNN target device (default: CPU)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    net : cv2.dnn_Net\n",
    "        Loaded YOLO network\n",
    "    output_layers : list\n",
    "        Names of output layers\n",
    "    \"\"\"\n",
    "    print(f\"Loading YOLO model...\")\n",
    "    print(f\"  Config: {Path(cfg_path).name}\")\n",
    "    print(f\"  Weights: {Path(weights_path).name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load network\n",
    "    net = cv2.dnn.readNetFromDarknet(str(cfg_path), str(weights_path))\n",
    "    \n",
    "    # Set backend and target\n",
    "    net.setPreferableBackend(backend)\n",
    "    net.setPreferableTarget(target)\n",
    "    \n",
    "    # Get output layer names\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"✅ Model loaded in {load_time:.2f}s\")\n",
    "    print(f\"   Total layers: {len(layer_names)}\")\n",
    "    print(f\"   Output layers: {output_layers}\")\n",
    "    \n",
    "    return net, output_layers\n",
    "\n",
    "\n",
    "# Load model if available\n",
    "if YOLO_MODEL_AVAILABLE:\n",
    "    yolo_net, yolo_output_layers = load_yolo_model(DEFAULT_CFG, DEFAULT_WEIGHTS)\n",
    "    print(f\"\\n🎯 {DEFAULT_MODEL_NAME} ready for inference\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Skipping model loading - no YOLO models available\")\n",
    "    yolo_net = None\n",
    "    yolo_output_layers = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 檢測函數\n",
    "\n",
    "實現完整的 YOLO 檢測流程：\n",
    "1. 圖像預處理（blobFromImage）\n",
    "2. 前向傳播\n",
    "3. 解析輸出\n",
    "4. 信心度過濾\n",
    "5. Non-Maximum Suppression (NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_yolo(image, net, output_layers, classes=COCO_CLASSES,\n",
    "                       confidence_threshold=0.5, nms_threshold=0.4, \n",
    "                       input_size=(416, 416)):\n",
    "    \"\"\"\n",
    "    Detect objects using YOLO\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image (BGR)\n",
    "    net : cv2.dnn_Net\n",
    "        YOLO network\n",
    "    output_layers : list\n",
    "        Output layer names\n",
    "    classes : list\n",
    "        Class names\n",
    "    confidence_threshold : float\n",
    "        Minimum confidence (0.0-1.0)\n",
    "    nms_threshold : float\n",
    "        NMS IoU threshold (0.0-1.0)\n",
    "    input_size : tuple\n",
    "        Network input size (width, height)\n",
    "        Common sizes: (320, 320), (416, 416), (608, 608)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    detections : list\n",
    "        List of (class_id, class_name, confidence, x, y, w, h) tuples\n",
    "    inference_time : float\n",
    "        Inference time in seconds\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Prepare blob from image\n",
    "    # YOLO expects normalized input (1/255 scaling)\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image,\n",
    "        scalefactor=1/255.0,\n",
    "        size=input_size,\n",
    "        mean=(0, 0, 0),\n",
    "        swapRB=True,  # BGR to RGB\n",
    "        crop=False\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    net.setInput(blob)\n",
    "    start_time = time.time()\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Parse detections\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            # Detection format: [x, y, w, h, objectness, class1_prob, class2_prob, ...]\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > confidence_threshold:\n",
    "                # YOLO returns center coordinates and dimensions\n",
    "                # Scale to image size\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Convert to top-left corner coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply Non-Maximum Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    \n",
    "    # Prepare final detections\n",
    "    detections = []\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            class_id = class_ids[i]\n",
    "            confidence = confidences[i]\n",
    "            class_name = classes[class_id] if class_id < len(classes) else \"unknown\"\n",
    "            \n",
    "            detections.append((class_id, class_name, confidence, x, y, w, h))\n",
    "    \n",
    "    return detections, inference_time\n",
    "\n",
    "\n",
    "def draw_yolo_detections(image, detections, colors=COLORS, thickness=2, font_scale=0.5):\n",
    "    \"\"\"\n",
    "    Draw YOLO detection results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : np.ndarray\n",
    "        Input image\n",
    "    detections : list\n",
    "        Detection results from detect_objects_yolo()\n",
    "    colors : np.ndarray\n",
    "        Color array for each class\n",
    "    thickness : int\n",
    "        Box line thickness\n",
    "    font_scale : float\n",
    "        Label font scale\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    output : np.ndarray\n",
    "        Image with drawn detections\n",
    "    \"\"\"\n",
    "    output = image.copy()\n",
    "    \n",
    "    for (class_id, class_name, confidence, x, y, w, h) in detections:\n",
    "        # Get color for this class\n",
    "        color = colors[class_id].tolist() if class_id < len(colors) else [0, 255, 0]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Prepare label\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        \n",
    "        # Get label size\n",
    "        (label_w, label_h), baseline = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1\n",
    "        )\n",
    "        \n",
    "        # Draw label background\n",
    "        cv2.rectangle(output,\n",
    "                     (x, y - label_h - baseline - 5),\n",
    "                     (x + label_w, y),\n",
    "                     color, -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(output, label,\n",
    "                   (x, y - baseline - 2),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"✅ YOLO detection functions defined\")\n",
    "print(\"\\nFunctions:\")\n",
    "print(\"  - detect_objects_yolo(): Complete detection pipeline\")\n",
    "print(\"  - draw_yolo_detections(): Visualize results with bounding boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試 YOLO 檢測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if YOLO_MODEL_AVAILABLE and yolo_net is not None:\n",
    "    # Load test image\n",
    "    test_image_paths = [\n",
    "        '../assets/images/basic/assassin.jpg',\n",
    "        '../assets/images/basic/1.jpg',\n",
    "        '../assets/images/basic/2.jpg',\n",
    "        '../assets/images/objects/street.jpg'\n",
    "    ]\n",
    "    \n",
    "    test_img = None\n",
    "    for path in test_image_paths:\n",
    "        if Path(path).exists():\n",
    "            test_img = cv2.imread(path)\n",
    "            print(f\"📷 Loaded test image: {path}\")\n",
    "            print(f\"   Size: {test_img.shape[1]}×{test_img.shape[0]}\")\n",
    "            break\n",
    "    \n",
    "    if test_img is None:\n",
    "        # Create demo image\n",
    "        test_img = np.ones((480, 640, 3), dtype=np.uint8) * 200\n",
    "        cv2.putText(test_img, \"No test image found\", (150, 240),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "        print(\"⚠️  Created placeholder image\")\n",
    "    \n",
    "    # Detect objects\n",
    "    print(\"\\n🔍 Running YOLO detection...\")\n",
    "    detections, inf_time = detect_objects_yolo(\n",
    "        test_img, yolo_net, yolo_output_layers,\n",
    "        confidence_threshold=0.5,\n",
    "        nms_threshold=0.4,\n",
    "        input_size=(416, 416)\n",
    "    )\n",
    "    \n",
    "    # Draw results\n",
    "    result_img = draw_yolo_detections(test_img, detections)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    fps = 1.0 / inf_time if inf_time > 0 else 0\n",
    "    axes[1].set_title(f'{DEFAULT_MODEL_NAME} Detection\\n'\n",
    "                     f'Objects: {len(detections)} | '\n",
    "                     f'Time: {inf_time*1000:.1f}ms | '\n",
    "                     f'FPS: {fps:.1f}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"\\n📊 Detection Results:\")\n",
    "    print(f\"   Inference time: {inf_time*1000:.2f}ms\")\n",
    "    print(f\"   Throughput: {fps:.2f} FPS\")\n",
    "    print(f\"   Objects detected: {len(detections)}\\n\")\n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        print(\"   Detected objects:\")\n",
    "        # Group by class\n",
    "        class_counts = defaultdict(list)\n",
    "        for (cid, cname, conf, x, y, w, h) in detections:\n",
    "            class_counts[cname].append(conf)\n",
    "        \n",
    "        for cname, confs in sorted(class_counts.items()):\n",
    "            avg_conf = np.mean(confs)\n",
    "            print(f\"     - {cname}: {len(confs)} instance(s), avg conf: {avg_conf:.3f}\")\n",
    "    else:\n",
    "        print(\"   No objects detected. Try:\")\n",
    "        print(\"     - Lowering confidence_threshold (e.g., 0.3)\")\n",
    "        print(\"     - Using a different test image\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Skipping YOLO detection demo - model not available\")\n",
    "    print(\"    Please download YOLO model files first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
