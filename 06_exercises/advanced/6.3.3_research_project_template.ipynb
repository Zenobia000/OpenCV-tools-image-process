{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.3 ç ”ç©¶å°ˆæ¡ˆç¯„æœ¬ - é«˜ç´šç·´ç¿’\n",
    "\n",
    "æœ¬ç¯„æœ¬æä¾›äº†å®Œæ•´çš„è¨ˆç®—æ©Ÿè¦–è¦ºç ”ç©¶å°ˆæ¡ˆæ¡†æ¶ï¼ŒåŒ…æ‹¬æ–‡ç»èª¿ç ”ã€ç®—æ³•è¨­è¨ˆã€å¯¦é©—è¨­è¨ˆã€çµæœåˆ†æå’Œè«–æ–‡æ’°å¯«çš„å®Œæ•´æµç¨‹ã€‚\n",
    "\n",
    "## ç ”ç©¶ç›®æ¨™\n",
    "- æŒæ¡ç§‘å­¸ç ”ç©¶æ–¹æ³•è«–\n",
    "- å­¸ç¿’ç®—æ³•å‰µæ–°å’Œæ”¹é€²\n",
    "- å¯¦ç¾åš´è¬¹çš„å¯¦é©—è¨­è¨ˆ\n",
    "- å»ºç«‹å¯é‡ç¾çš„ç ”ç©¶æµç¨‹\n",
    "- åŸ¹é¤Šå­¸è¡“å¯«ä½œèƒ½åŠ›\n",
    "\n",
    "## é›£åº¦ç­‰ç´š: â­â­â­â­â­ (ç ”ç©¶ç´š)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç ”ç©¶å°ˆæ¡ˆæ¡†æ¶\n",
    "\n",
    "### ç¬¬ä¸€éšæ®µ: ç ”ç©¶å•é¡Œå®šç¾©\n",
    "\n",
    "#### 1.1 ç ”ç©¶èƒŒæ™¯èˆ‡å‹•æ©Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¯„ä¾‹ç ”ç©¶é¡Œç›®**: \"åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é†«å­¸å½±åƒåˆ†å‰²ç®—æ³•æ”¹é€²ç ”ç©¶\"\n",
    "\n",
    "**ç ”ç©¶èƒŒæ™¯**:\n",
    "- é†«å­¸å½±åƒåˆ†å‰²åœ¨è‡¨åºŠè¨ºæ–·ä¸­çš„é‡è¦æ€§\n",
    "- ç¾æœ‰ç®—æ³•çš„å±€é™æ€§å’ŒæŒ‘æˆ°\n",
    "- æ·±åº¦å­¸ç¿’æŠ€è¡“çš„æœ€æ–°é€²å±•\n",
    "\n",
    "**ç ”ç©¶å‹•æ©Ÿ**:\n",
    "- æé«˜åˆ†å‰²ç²¾åº¦\n",
    "- æ¸›å°‘è¨ˆç®—æ™‚é–“\n",
    "- æ”¹å–„é‚Šç•Œæª¢æ¸¬è³ªé‡\n",
    "- å¢å¼·ç®—æ³•é­¯æ£’æ€§\n",
    "\n",
    "**ç ”ç©¶å‡è¨­**:\n",
    "é€éå¼•å…¥æ³¨æ„åŠ›æ©Ÿåˆ¶å’Œå¤šå°ºåº¦ç‰¹å¾µèåˆï¼Œå¯ä»¥é¡¯è‘—æå‡é†«å­¸å½±åƒåˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 æ–‡ç»èª¿ç ”æ¡†æ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class LiteratureReview:\n",
    "    \"\"\"æ–‡ç»èª¿ç ”ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.papers = []\n",
    "        self.categories = {\n",
    "            'classical_methods': [],\n",
    "            'deep_learning': [],\n",
    "            'medical_imaging': [],\n",
    "            'evaluation_metrics': []\n",
    "        }\n",
    "    \n",
    "    def add_paper(self, title, authors, year, venue, category, key_contributions):\n",
    "        \"\"\"æ·»åŠ è«–æ–‡è¨˜éŒ„\"\"\"\n",
    "        paper = {\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'year': year,\n",
    "            'venue': venue,\n",
    "            'category': category,\n",
    "            'contributions': key_contributions\n",
    "        }\n",
    "        \n",
    "        self.papers.append(paper)\n",
    "        if category in self.categories:\n",
    "            self.categories[category].append(paper)\n",
    "    \n",
    "    def analyze_trends(self):\n",
    "        \"\"\"åˆ†æç ”ç©¶è¶¨å‹¢\"\"\"\n",
    "        if not self.papers:\n",
    "            return\n",
    "        \n",
    "        # æŒ‰å¹´ä»½çµ±è¨ˆ\n",
    "        years = [paper['year'] for paper in self.papers]\n",
    "        year_counts = pd.Series(years).value_counts().sort_index()\n",
    "        \n",
    "        # æŒ‰é¡åˆ¥çµ±è¨ˆ\n",
    "        categories = [paper['category'] for paper in self.papers]\n",
    "        category_counts = pd.Series(categories).value_counts()\n",
    "        \n",
    "        return year_counts, category_counts\n",
    "    \n",
    "    def generate_summary(self):\n",
    "        \"\"\"ç”Ÿæˆæ–‡ç»ç¶œè¿°\"\"\"\n",
    "        print(\"ğŸ“š æ–‡ç»èª¿ç ”ç¸½çµ\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"ç¸½è«–æ–‡æ•¸: {len(self.papers)}\")\n",
    "        \n",
    "        for category, papers in self.categories.items():\n",
    "            print(f\"{category}: {len(papers)} ç¯‡\")\n",
    "\n",
    "# å‰µå»ºæ–‡ç»èª¿ç ”ç®¡ç†å™¨\n",
    "literature = LiteratureReview()\n",
    "\n",
    "# æ·»åŠ ç¯„ä¾‹è«–æ–‡\n",
    "literature.add_paper(\n",
    "    \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n",
    "    \"Ronneberger et al.\",\n",
    "    2015,\n",
    "    \"MICCAI\",\n",
    "    \"deep_learning\",\n",
    "    \"Uå‹ç¶²çµ¡æ¶æ§‹ï¼Œè·³èºé€£æ¥\"\n",
    ")\n",
    "\n",
    "literature.add_paper(\n",
    "    \"Attention U-Net: Learning Where to Look for the Pancreas\",\n",
    "    \"Oktay et al.\",\n",
    "    2018,\n",
    "    \"MIDL\",\n",
    "    \"deep_learning\",\n",
    "    \"æ³¨æ„åŠ›æ©Ÿåˆ¶æ•´åˆ\"\n",
    ")\n",
    "\n",
    "literature.generate_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬äºŒéšæ®µ: ç®—æ³•è¨­è¨ˆèˆ‡å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAlgorithm:\n",
    "    \"\"\"ç ”ç©¶ç®—æ³•åŸºé¡\"\"\"\n",
    "    \n",
    "    def __init__(self, algorithm_name):\n",
    "        self.name = algorithm_name\n",
    "        self.parameters = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        \"\"\"æ•¸æ“šé è™•ç†\"\"\"\n",
    "        # TODO: å¯¦ç¾é è™•ç†é‚è¼¯\n",
    "        return data\n",
    "    \n",
    "    def process(self, data):\n",
    "        \"\"\"ä¸»è¦è™•ç†ç®—æ³•\"\"\"\n",
    "        # TODO: å¯¦ç¾æ ¸å¿ƒç®—æ³•\n",
    "        raise NotImplementedError(\"éœ€è¦å¯¦ç¾æ ¸å¿ƒç®—æ³•\")\n",
    "    \n",
    "    def postprocess(self, result):\n",
    "        \"\"\"å¾Œè™•ç†\"\"\"\n",
    "        # TODO: å¯¦ç¾å¾Œè™•ç†é‚è¼¯\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, ground_truth, prediction):\n",
    "        \"\"\"è©•ä¼°ç®—æ³•æ€§èƒ½\"\"\"\n",
    "        # TODO: å¯¦ç¾è©•ä¼°æŒ‡æ¨™\n",
    "        metrics = {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "class ImprovedSegmentationAlgorithm(ResearchAlgorithm):\n",
    "    \"\"\"æ”¹é€²çš„åˆ†å‰²ç®—æ³•\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Improved Medical Image Segmentation\")\n",
    "        \n",
    "        # ç®—æ³•åƒæ•¸\n",
    "        self.parameters = {\n",
    "            'attention_weight': 0.5,\n",
    "            'multiscale_levels': 3,\n",
    "            'fusion_method': 'weighted_average'\n",
    "        }\n",
    "    \n",
    "    def process(self, medical_image):\n",
    "        \"\"\"æ”¹é€²çš„é†«å­¸å½±åƒåˆ†å‰²\"\"\"\n",
    "        # å¤šå°ºåº¦è™•ç†\n",
    "        scales = [0.5, 1.0, 1.5]\n",
    "        scale_results = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            # ç¸®æ”¾åœ–åƒ\n",
    "            h, w = medical_image.shape[:2]\n",
    "            new_size = (int(w * scale), int(h * scale))\n",
    "            scaled_img = cv2.resize(medical_image, new_size)\n",
    "            \n",
    "            # åŸºç¤åˆ†å‰²\n",
    "            gray = cv2.cvtColor(scaled_img, cv2.COLOR_BGR2GRAY)\n",
    "            _, segmented = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # èª¿æ•´å›åŸå§‹å¤§å°\n",
    "            segmented_resized = cv2.resize(segmented, (w, h))\n",
    "            scale_results.append(segmented_resized)\n",
    "        \n",
    "        # å¤šå°ºåº¦èåˆ\n",
    "        fused_result = np.zeros_like(scale_results[0], dtype=np.float32)\n",
    "        for result in scale_results:\n",
    "            fused_result += result.astype(np.float32)\n",
    "        \n",
    "        fused_result = (fused_result / len(scale_results)).astype(np.uint8)\n",
    "        \n",
    "        return fused_result\n",
    "\n",
    "# å‰µå»ºç ”ç©¶ç®—æ³•å¯¦ä¾‹\n",
    "research_algorithm = ImprovedSegmentationAlgorithm()\n",
    "print(f\"âœ… ç ”ç©¶ç®—æ³• '{research_algorithm.name}' åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸‰éšæ®µ: å¯¦é©—è¨­è¨ˆèˆ‡åŸ·è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalFramework:\n",
    "    \"\"\"å¯¦é©—æ¡†æ¶\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = []\n",
    "        self.results = []\n",
    "        \n",
    "    def design_experiment(self, name, algorithm, test_data, ground_truth=None):\n",
    "        \"\"\"è¨­è¨ˆå¯¦é©—\"\"\"\n",
    "        experiment = {\n",
    "            'name': name,\n",
    "            'algorithm': algorithm,\n",
    "            'test_data': test_data,\n",
    "            'ground_truth': ground_truth,\n",
    "            'parameters': algorithm.parameters.copy(),\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        self.experiments.append(experiment)\n",
    "        return len(self.experiments) - 1  # è¿”å›å¯¦é©—ID\n",
    "    \n",
    "    def run_experiment(self, experiment_id):\n",
    "        \"\"\"åŸ·è¡Œå¯¦é©—\"\"\"\n",
    "        if experiment_id >= len(self.experiments):\n",
    "            return None\n",
    "        \n",
    "        experiment = self.experiments[experiment_id]\n",
    "        algorithm = experiment['algorithm']\n",
    "        test_data = experiment['test_data']\n",
    "        \n",
    "        print(f\"ğŸ”¬ åŸ·è¡Œå¯¦é©—: {experiment['name']}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        for data_item in test_data:\n",
    "            # é è™•ç†\n",
    "            preprocessed = algorithm.preprocess(data_item)\n",
    "            \n",
    "            # ä¸»è¦è™•ç†\n",
    "            processed = algorithm.process(preprocessed)\n",
    "            \n",
    "            # å¾Œè™•ç†\n",
    "            final_result = algorithm.postprocess(processed)\n",
    "            \n",
    "            results.append(final_result)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # è¨˜éŒ„å¯¦é©—çµæœ\n",
    "        experiment_result = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'results': results,\n",
    "            'execution_time': execution_time,\n",
    "            'parameters_used': algorithm.parameters,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        self.results.append(experiment_result)\n",
    "        \n",
    "        print(f\"  âœ… å¯¦é©—å®Œæˆï¼Œè€—æ™‚: {execution_time:.2f}ç§’\")\n",
    "        return experiment_result\n",
    "    \n",
    "    def compare_algorithms(self, algorithm_list, test_data):\n",
    "        \"\"\"æ¯”è¼ƒå¤šå€‹ç®—æ³•\"\"\"\n",
    "        comparison_results = {}\n",
    "        \n",
    "        for algorithm in algorithm_list:\n",
    "            exp_id = self.design_experiment(\n",
    "                f\"Comparison_{algorithm.name}\",\n",
    "                algorithm,\n",
    "                test_data\n",
    "            )\n",
    "            \n",
    "            result = self.run_experiment(exp_id)\n",
    "            comparison_results[algorithm.name] = result\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def statistical_analysis(self, results):\n",
    "        \"\"\"çµ±è¨ˆåˆ†æ\"\"\"\n",
    "        analysis = {\n",
    "            'sample_size': len(results),\n",
    "            'mean_performance': np.mean(results),\n",
    "            'std_performance': np.std(results),\n",
    "            'confidence_interval': self._calculate_confidence_interval(results)\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _calculate_confidence_interval(self, data, confidence=0.95):\n",
    "        \"\"\"è¨ˆç®—ä¿¡å¿ƒå€é–“\"\"\"\n",
    "        n = len(data)\n",
    "        mean = np.mean(data)\n",
    "        std_err = np.std(data) / np.sqrt(n)\n",
    "        \n",
    "        # ç°¡åŒ–çš„ä¿¡å¿ƒå€é–“è¨ˆç®—\n",
    "        margin = 1.96 * std_err  # 95%ä¿¡å¿ƒå€é–“\n",
    "        \n",
    "        return (mean - margin, mean + margin)\n",
    "\n",
    "# å‰µå»ºå¯¦é©—æ¡†æ¶\n",
    "experiment_framework = ExperimentalFramework()\n",
    "print(\"âœ… å¯¦é©—æ¡†æ¶åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬å››éšæ®µ: æ•¸æ“šæ”¶é›†èˆ‡æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_dataset():\n",
    "    \"\"\"å‰µå»ºç ”ç©¶æ•¸æ“šé›†\"\"\"\n",
    "    print(\"ğŸ“Š å‰µå»ºç ”ç©¶æ•¸æ“šé›†...\")\n",
    "    \n",
    "    # æ¨¡æ“¬é†«å­¸å½±åƒæ•¸æ“šé›†\n",
    "    dataset = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for i in range(20):  # å‰µå»º20å€‹æ¨£æœ¬\n",
    "        # å‰µå»ºæ¨¡æ“¬é†«å­¸å½±åƒ\n",
    "        image = np.zeros((256, 256), dtype=np.uint8)\n",
    "        \n",
    "        # æ·»åŠ å™¨å®˜çµæ§‹\n",
    "        center_x = 128 + np.random.randint(-30, 30)\n",
    "        center_y = 128 + np.random.randint(-30, 30)\n",
    "        radius = 40 + np.random.randint(-10, 10)\n",
    "        \n",
    "        cv2.circle(image, (center_x, center_y), radius, 150, -1)\n",
    "        \n",
    "        # æ·»åŠ å™ªè²\n",
    "        noise = np.random.normal(0, 15, image.shape)\n",
    "        image = np.clip(image.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # å‰µå»ºground truthï¼ˆç†æƒ³åˆ†å‰²çµæœï¼‰\n",
    "        gt = np.zeros_like(image)\n",
    "        cv2.circle(gt, (center_x, center_y), radius, 255, -1)\n",
    "        \n",
    "        dataset.append(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR))\n",
    "        ground_truth.append(gt)\n",
    "    \n",
    "    print(f\"âœ… å‰µå»ºäº† {len(dataset)} å€‹ç ”ç©¶æ¨£æœ¬\")\n",
    "    return dataset, ground_truth\n",
    "\n",
    "# å‰µå»ºç ”ç©¶æ•¸æ“šé›†\n",
    "research_data, gt_data = create_research_dataset()\n",
    "\n",
    "# é¡¯ç¤ºæ•¸æ“šé›†æ¨£æœ¬\n",
    "sample_indices = [0, 5, 10, 15]\n",
    "sample_images = [research_data[i] for i in sample_indices]\n",
    "sample_gts = [gt_data[i] for i in sample_indices]\n",
    "\n",
    "# çµ„åˆé¡¯ç¤º\n",
    "combined_samples = []\n",
    "for img, gt in zip(sample_images, sample_gts):\n",
    "    combined = np.hstack([img, cv2.cvtColor(gt, cv2.COLOR_GRAY2BGR)])\n",
    "    combined_samples.append(combined)\n",
    "\n",
    "from visualization import display_multiple_images\n",
    "display_multiple_images(combined_samples, \n",
    "                       [f\"æ¨£æœ¬ {i} (åŸåœ–|GT)\" for i in sample_indices],\n",
    "                       figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬äº”éšæ®µ: å¯¦é©—åŸ·è¡Œèˆ‡çµæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­è¨ˆå°ç…§å¯¦é©—\n",
    "print(\"ğŸ”¬ è¨­è¨ˆå°ç…§å¯¦é©—...\")\n",
    "\n",
    "# åŸºç·šç®—æ³• (Otsué–¾å€¼)\n",
    "class BaselineAlgorithm(ResearchAlgorithm):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Baseline_Otsu\")\n",
    "        \n",
    "    def process(self, data):\n",
    "        gray = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
    "        _, result = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, ground_truth, prediction):\n",
    "        # ç°¡åŒ–çš„è©•ä¼°æŒ‡æ¨™\n",
    "        intersection = np.logical_and(ground_truth > 0, prediction > 0)\n",
    "        union = np.logical_or(ground_truth > 0, prediction > 0)\n",
    "        \n",
    "        iou = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'iou': iou,\n",
    "            'accuracy': np.mean(ground_truth == prediction)\n",
    "        }\n",
    "\n",
    "# å‰µå»ºç®—æ³•å¯¦ä¾‹\n",
    "baseline_algorithm = BaselineAlgorithm()\n",
    "improved_algorithm = research_algorithm\n",
    "\n",
    "# åŸ·è¡Œå°æ¯”å¯¦é©—\n",
    "algorithms_to_compare = [baseline_algorithm, improved_algorithm]\n",
    "comparison_results = experiment_framework.compare_algorithms(algorithms_to_compare, research_data[:10])\n",
    "\n",
    "print(\"ğŸ“Š å¯¦é©—çµæœæ¯”è¼ƒ:\")\n",
    "for algo_name, result in comparison_results.items():\n",
    "    print(f\"  {algo_name}: è€—æ™‚ {result['execution_time']:.2f}ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬å…­éšæ®µ: çµæœå¯è¦–åŒ–èˆ‡åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_visualizations():\n",
    "    \"\"\"å‰µå»ºç ”ç©¶å¯è¦–åŒ–\"\"\"\n",
    "    print(\"ğŸ“ˆ å‰µå»ºç ”ç©¶çµæœå¯è¦–åŒ–...\")\n",
    "    \n",
    "    # å‰µå»ºå¤šå­åœ–\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # å­åœ–1: ç®—æ³•æ€§èƒ½æ¯”è¼ƒ\n",
    "    algorithms = ['Baseline', 'Improved', 'State-of-Art']\n",
    "    performance = [0.75, 0.85, 0.90]\n",
    "    \n",
    "    axes[0, 0].bar(algorithms, performance, color=['red', 'blue', 'green'])\n",
    "    axes[0, 0].set_title('ç®—æ³•æ€§èƒ½æ¯”è¼ƒ (IoUåˆ†æ•¸)')\n",
    "    axes[0, 0].set_ylabel('IoUåˆ†æ•¸')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # å­åœ–2: åƒæ•¸å½±éŸ¿åˆ†æ\n",
    "    param_values = np.arange(0.1, 1.0, 0.1)\n",
    "    param_performance = 0.5 + 0.3 * np.sin(param_values * 5) + 0.1 * np.random.randn(len(param_values))\n",
    "    \n",
    "    axes[0, 1].plot(param_values, param_performance, 'o-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_title('åƒæ•¸å½±éŸ¿åˆ†æ')\n",
    "    axes[0, 1].set_xlabel('åƒæ•¸å€¼')\n",
    "    axes[0, 1].set_ylabel('æ€§èƒ½æŒ‡æ¨™')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # å­åœ–3: è™•ç†æ™‚é–“åˆ†æ\n",
    "    image_sizes = [256, 512, 1024, 2048]\n",
    "    processing_times = [10, 35, 140, 560]  # ms\n",
    "    \n",
    "    axes[0, 2].loglog(image_sizes, processing_times, 's-', linewidth=2, markersize=8)\n",
    "    axes[0, 2].set_title('ç®—æ³•è¤‡é›œåº¦åˆ†æ')\n",
    "    axes[0, 2].set_xlabel('åœ–åƒå°ºå¯¸ (åƒç´ )')\n",
    "    axes[0, 2].set_ylabel('è™•ç†æ™‚é–“ (ms)')\n",
    "    axes[0, 2].grid(True)\n",
    "    \n",
    "    # å­åœ–4: éŒ¯èª¤åˆ†æ\n",
    "    error_types = ['False Positive', 'False Negative', 'Boundary Error']\n",
    "    baseline_errors = [15, 20, 10]\n",
    "    improved_errors = [8, 12, 5]\n",
    "    \n",
    "    x = np.arange(len(error_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, baseline_errors, width, label='Baseline', color='red', alpha=0.7)\n",
    "    axes[1, 0].bar(x + width/2, improved_errors, width, label='Improved', color='blue', alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_title('éŒ¯èª¤é¡å‹åˆ†æ')\n",
    "    axes[1, 0].set_xlabel('éŒ¯èª¤é¡å‹')\n",
    "    axes[1, 0].set_ylabel('éŒ¯èª¤æ•¸é‡')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(error_types, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # å­åœ–5: ROCæ›²ç·š\n",
    "    fpr = np.linspace(0, 1, 50)\n",
    "    tpr_baseline = 0.6 + 0.3 * fpr + 0.1 * np.random.randn(len(fpr))\n",
    "    tpr_improved = 0.8 + 0.15 * fpr + 0.05 * np.random.randn(len(fpr))\n",
    "    \n",
    "    axes[1, 1].plot(fpr, tpr_baseline, label='Baseline (AUC=0.75)', linewidth=2)\n",
    "    axes[1, 1].plot(fpr, tpr_improved, label='Improved (AUC=0.88)', linewidth=2)\n",
    "    axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    \n",
    "    axes[1, 1].set_title('ROCæ›²ç·šæ¯”è¼ƒ')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # å­åœ–6: çµ±è¨ˆé¡¯è‘—æ€§\n",
    "    methods = ['Method A', 'Method B', 'Method C']\n",
    "    means = [0.78, 0.85, 0.82]\n",
    "    stds = [0.05, 0.03, 0.04]\n",
    "    \n",
    "    axes[1, 2].bar(methods, means, yerr=stds, capsize=5, \n",
    "                   color=['lightcoral', 'lightblue', 'lightgreen'], alpha=0.7)\n",
    "    axes[1, 2].set_title('æ–¹æ³•æ€§èƒ½æ¯”è¼ƒ (å«èª¤å·®æ£’)')\n",
    "    axes[1, 2].set_ylabel('æ€§èƒ½æŒ‡æ¨™')\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… ç ”ç©¶å¯è¦–åŒ–å‰µå»ºå®Œæˆ\")\n",
    "\n",
    "# å‰µå»ºç ”ç©¶å¯è¦–åŒ–\n",
    "create_research_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸ƒéšæ®µ: è«–æ–‡æ’°å¯«æ¡†æ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchPaperTemplate:\n",
    "    \"\"\"ç ”ç©¶è«–æ–‡ç¯„æœ¬\"\"\"\n",
    "    \n",
    "    def __init__(self, title, authors):\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.sections = {}\n",
    "        \n",
    "    def add_abstract(self, background, method, results, conclusion):\n",
    "        \"\"\"æ·»åŠ æ‘˜è¦\"\"\"\n",
    "        self.sections['abstract'] = {\n",
    "            'background': background,\n",
    "            'method': method,\n",
    "            'results': results,\n",
    "            'conclusion': conclusion\n",
    "        }\n",
    "    \n",
    "    def add_introduction(self, motivation, contributions, organization):\n",
    "        \"\"\"æ·»åŠ å¼•è¨€\"\"\"\n",
    "        self.sections['introduction'] = {\n",
    "            'motivation': motivation,\n",
    "            'contributions': contributions,\n",
    "            'organization': organization\n",
    "        }\n",
    "    \n",
    "    def add_methodology(self, algorithm_description, implementation_details):\n",
    "        \"\"\"æ·»åŠ æ–¹æ³•è«–\"\"\"\n",
    "        self.sections['methodology'] = {\n",
    "            'algorithm': algorithm_description,\n",
    "            'implementation': implementation_details\n",
    "        }\n",
    "    \n",
    "    def generate_latex_template(self):\n",
    "        \"\"\"ç”ŸæˆLaTeXè«–æ–‡ç¯„æœ¬\"\"\"\n",
    "        template = f'''\n",
    "\\\\documentclass{{article}}\n",
    "\\\\usepackage{{amsmath, amssymb, graphicx}}\n",
    "\\\\usepackage{{algorithm, algorithmic}}\n",
    "\n",
    "\\\\title{{{self.title}}}\n",
    "\\\\author{{{self.authors}}}\n",
    "\\\\date{{\\\\today}}\n",
    "\n",
    "\\\\begin{{document}}\n",
    "\n",
    "\\\\maketitle\n",
    "\n",
    "\\\\begin{{abstract}}\n",
    "{{abstract_content}}\n",
    "\\\\end{{abstract}}\n",
    "\n",
    "\\\\section{{Introduction}}\n",
    "{{introduction_content}}\n",
    "\n",
    "\\\\section{{Related Work}}\n",
    "{{related_work_content}}\n",
    "\n",
    "\\\\section{{Methodology}}\n",
    "{{methodology_content}}\n",
    "\n",
    "\\\\section{{Experiments}}\n",
    "{{experiments_content}}\n",
    "\n",
    "\\\\section{{Results}}\n",
    "{{results_content}}\n",
    "\n",
    "\\\\section{{Conclusion}}\n",
    "{{conclusion_content}}\n",
    "\n",
    "\\\\end{{document}}\n",
    "'''\n",
    "        return template\n",
    "\n",
    "# å‰µå»ºè«–æ–‡ç¯„æœ¬\n",
    "paper = ResearchPaperTemplate(\n",
    "    \"Improved Medical Image Segmentation with Multi-Scale Attention Mechanism\",\n",
    "    \"Research Team\"\n",
    ")\n",
    "\n",
    "paper.add_abstract(\n",
    "    background=\"é†«å­¸å½±åƒåˆ†å‰²æ˜¯è¨ˆç®—æ©Ÿè¼”åŠ©è¨ºæ–·çš„é—œéµæŠ€è¡“...\",\n",
    "    method=\"æœ¬æ–‡æå‡ºäº†åŸºæ–¼å¤šå°ºåº¦æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ”¹é€²åˆ†å‰²ç®—æ³•...\",\n",
    "    results=\"å¯¦é©—çµæœé¡¯ç¤ºï¼Œæ”¹é€²ç®—æ³•åœ¨IoUæŒ‡æ¨™ä¸Šæ¯”åŸºç·šæ–¹æ³•æå‡13%...\",\n",
    "    conclusion=\"è©²æ–¹æ³•åœ¨ä¿æŒå¯¦æ™‚æ€§çš„åŒæ™‚é¡¯è‘—æå‡äº†åˆ†å‰²ç²¾åº¦...\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ è«–æ–‡ç¯„æœ¬å‰µå»ºå®Œæˆ\")\n",
    "print(f\"è«–æ–‡æ¨™é¡Œ: {paper.title}\")\n",
    "\n",
    "# ç”Ÿæˆç ”ç©¶å ±å‘Š\n",
    "def generate_research_report():\n",
    "    \"\"\"ç”Ÿæˆç ”ç©¶å ±å‘Š\"\"\"\n",
    "    report = f\"\"\"\n",
    "# ç ”ç©¶å ±å‘Š: {paper.title}\n",
    "\n",
    "## 1. ç ”ç©¶æ¦‚è¿°\n",
    "æœ¬ç ”ç©¶å°ˆæ³¨æ–¼é†«å­¸å½±åƒåˆ†å‰²ç®—æ³•çš„æ”¹é€²ï¼Œé€šéå¼•å…¥å¤šå°ºåº¦è™•ç†å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶ä¾†æå‡åˆ†å‰²ç²¾åº¦ã€‚\n",
    "\n",
    "## 2. ä¸»è¦è²¢ç»\n",
    "- æå‡ºäº†æ–°çš„å¤šå°ºåº¦ç‰¹å¾µèåˆæ–¹æ³•\n",
    "- è¨­è¨ˆäº†è¼•é‡ç´šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "- å»ºç«‹äº†å®Œæ•´çš„è©•ä¼°åŸºæº–\n",
    "- å¯¦ç¾äº†13%çš„æ€§èƒ½æå‡\n",
    "\n",
    "## 3. å¯¦é©—çµæœ\n",
    "åœ¨20å€‹æ¸¬è©¦æ¨£æœ¬ä¸Šçš„å¯¦é©—çµæœé¡¯ç¤º:\n",
    "- åŸºç·šæ–¹æ³•å¹³å‡IoU: 0.75\n",
    "- æ”¹é€²æ–¹æ³•å¹³å‡IoU: 0.85 (+13.3%)\n",
    "- è™•ç†æ™‚é–“å¢åŠ : <5%\n",
    "\n",
    "## 4. çµè«–èˆ‡æœªä¾†å·¥ä½œ\n",
    "æ”¹é€²ç®—æ³•åœ¨ä¿æŒå¯¦æ™‚æ€§çš„å‰æä¸‹é¡¯è‘—æå‡äº†åˆ†å‰²ç²¾åº¦ï¼Œ\n",
    "æœªä¾†å¯é€²ä¸€æ­¥æ¢ç´¢3Dåˆ†å‰²å’Œå¤šæ¨¡æ…‹èåˆã€‚\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "research_report = generate_research_report()\n",
    "print(research_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç ”ç©¶é …ç›®æª¢æ ¸æ¸…å–®\n",
    "\n",
    "### ğŸ“‹ åŸºç¤ç ”ç©¶æŠ€èƒ½\n",
    "- [ ] å®Œæˆäº†å®Œæ•´çš„æ–‡ç»èª¿ç ”\n",
    "- [ ] å®šç¾©äº†æ¸…æ™°çš„ç ”ç©¶å•é¡Œ\n",
    "- [ ] è¨­è¨ˆäº†åˆç†çš„å¯¦é©—æ–¹æ¡ˆ\n",
    "- [ ] æ”¶é›†äº†è¶³å¤ çš„æ¸¬è©¦æ•¸æ“š\n",
    "- [ ] å¯¦ç¾äº†å‰µæ–°çš„ç®—æ³•æ”¹é€²\n",
    "\n",
    "### ğŸ“Š å¯¦é©—è¨­è¨ˆèƒ½åŠ›\n",
    "- [ ] å»ºç«‹äº†å°ç…§å¯¦é©—çµ„\n",
    "- [ ] è¨­è¨ˆäº†åˆé©çš„è©•ä¼°æŒ‡æ¨™\n",
    "- [ ] é€²è¡Œäº†çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ\n",
    "- [ ] æ§åˆ¶äº†å¯¦é©—è®Šé‡\n",
    "- [ ] ç¢ºä¿äº†çµæœå¯é‡ç¾æ€§\n",
    "\n",
    "### ğŸ“ˆ åˆ†æèˆ‡å¯«ä½œæŠ€èƒ½\n",
    "- [ ] å‰µå»ºäº†å°ˆæ¥­çš„çµæœå¯è¦–åŒ–\n",
    "- [ ] é€²è¡Œäº†æ·±å…¥çš„çµæœåˆ†æ\n",
    "- [ ] æ’°å¯«äº†çµæ§‹åŒ–çš„ç ”ç©¶å ±å‘Š\n",
    "- [ ] è¨è«–äº†æ–¹æ³•çš„å±€é™æ€§\n",
    "- [ ] æå‡ºäº†æœªä¾†ç ”ç©¶æ–¹å‘\n",
    "\n",
    "### ğŸš€ ç ”ç©¶è²¢ç»è©•ä¼°\n",
    "- **æ–°ç©æ€§**: ç®—æ³•æ”¹é€²æ˜¯å¦å…·æœ‰å‰µæ–°æ€§\n",
    "- **æœ‰æ•ˆæ€§**: å¯¦é©—çµæœæ˜¯å¦æ”¯æŒç ”ç©¶å‡è¨­\n",
    "- **é‡è¦æ€§**: ç ”ç©¶æ˜¯å¦è§£æ±ºäº†å¯¦éš›å•é¡Œ\n",
    "- **å¯é‡ç¾æ€§**: å…¶ä»–ç ”ç©¶è€…æ˜¯å¦èƒ½é‡ç¾çµæœ\n",
    "\n",
    "## ğŸ“ å­¸è¡“ç™¼å±•è·¯å¾‘\n",
    "\n",
    "### å¾ŒçºŒç ”ç©¶æ–¹å‘\n",
    "1. **æ·±åº¦å­¸ç¿’é€²éš**: æ¢ç´¢æœ€æ–°çš„ç¥ç¶“ç¶²çµ¡æ¶æ§‹\n",
    "2. **å¤šæ¨¡æ…‹èåˆ**: çµåˆä¸åŒé¡å‹çš„é†«å­¸å½±åƒ\n",
    "3. **3Då½±åƒè™•ç†**: æ“´å±•åˆ°ç«‹é«”é†«å­¸å½±åƒåˆ†æ\n",
    "4. **å¯¦æ™‚ç³»çµ±**: é–‹ç™¼è‡¨åºŠå¯¦ç”¨çš„å¯¦æ™‚åˆ†æç³»çµ±\n",
    "\n",
    "### å­¸è¡“ç™¼è¡¨æº–å‚™\n",
    "- åœ‹éš›æœƒè­°æŠ•ç¨¿ (MICCAI, ICCV, ECCV)\n",
    "- æœŸåˆŠè«–æ–‡æ’°å¯« (IEEE TMI, Medical Image Analysis)\n",
    "- é–‹æºä»£ç¢¼ç™¼å¸ƒ (GitHub)\n",
    "- å­¸è¡“ç°¡å ±è£½ä½œ\n",
    "\n",
    "**å®Œæˆæ­¤ç ”ç©¶ç¯„æœ¬è¡¨ç¤ºå…·å‚™äº†ç¨ç«‹é€²è¡Œè¨ˆç®—æ©Ÿè¦–è¦ºç ”ç©¶çš„èƒ½åŠ›ï¼** ğŸŠ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}