{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2 åœ–åƒæ‹¼æ¥å°ˆæ¡ˆ - ä¸­ç´šç·´ç¿’\n",
    "\n",
    "æœ¬ç·´ç¿’å°ˆæ³¨æ–¼åœ–åƒæ‹¼æ¥æŠ€è¡“çš„å¯¦éš›æ‡‰ç”¨ï¼Œé€šéå‰µå»ºå…¨æ™¯åœ–åƒä¾†æŒæ¡ç‰¹å¾µåŒ¹é…ã€å¹¾ä½•è®Šæ›å’Œåœ–åƒèåˆæŠ€è¡“ã€‚\n",
    "\n",
    "## ç·´ç¿’ç›®æ¨™\n",
    "- æŒæ¡åœ–åƒæ‹¼æ¥çš„å®Œæ•´æµç¨‹\n",
    "- å¯¦ç¾è‡ªå‹•ç‰¹å¾µåŒ¹é…å’Œé…æº–\n",
    "- å­¸ç¿’åœ–åƒèåˆå’Œæ¥ç¸«æ¶ˆé™¤æŠ€è¡“\n",
    "- è™•ç†å…‰ç…§å’Œæ›å…‰å·®ç•°\n",
    "- å„ªåŒ–æ‹¼æ¥å“è³ªå’Œæ€§èƒ½\n",
    "\n",
    "## é›£åº¦ç­‰ç´š: â­â­â­ (ä¸­ç´š)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "from image_utils import load_image, resize_image\n",
    "from visualization import display_image, display_multiple_images\n",
    "from performance import time_function\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŒ‘æˆ°1: å¯¦ç¾åŸºç¤åœ–åƒæ‹¼æ¥å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageStitcher:\n",
    "    \"\"\"åœ–åƒæ‹¼æ¥å™¨é¡\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ‹¼æ¥å™¨\"\"\"\n",
    "        self.detector = cv2.SIFT_create()\n",
    "        \n",
    "        # å‰µå»ºFLANNåŒ¹é…å™¨\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        self.matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        \n",
    "        print(\"âœ… åœ–åƒæ‹¼æ¥å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"æå–åœ–åƒç‰¹å¾µ\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "            \n",
    "        kp, desc = self.detector.detectAndCompute(gray, None)\n",
    "        return kp, desc\n",
    "    \n",
    "    def match_features(self, desc1, desc2, ratio_threshold=0.75):\n",
    "        \"\"\"åŒ¹é…ç‰¹å¾µé»\"\"\"\n",
    "        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for match_pair in matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < ratio_threshold * n.distance:\n",
    "                    good_matches.append(m)\n",
    "        \n",
    "        return good_matches\n",
    "    \n",
    "    def find_homography(self, kp1, kp2, matches):\n",
    "        \"\"\"è¨ˆç®—å–®æ‡‰æ€§çŸ©é™£\"\"\"\n",
    "        if len(matches) < 4:\n",
    "            return None, None\n",
    "        \n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        \n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, \n",
    "                                    cv2.RANSAC, 5.0)\n",
    "        \n",
    "        return H, mask\n",
    "    \n",
    "    def stitch_two_images(self, img1, img2):\n",
    "        \"\"\"æ‹¼æ¥å…©å¼µåœ–åƒ\"\"\"\n",
    "        print(\"ğŸ”„ é–‹å§‹æ‹¼æ¥å…©å¼µåœ–åƒ...\")\n",
    "        \n",
    "        # æå–ç‰¹å¾µ\n",
    "        kp1, desc1 = self.extract_features(img1)\n",
    "        kp2, desc2 = self.extract_features(img2)\n",
    "        \n",
    "        print(f\"  åœ–åƒ1ç‰¹å¾µé»: {len(kp1)}\")\n",
    "        print(f\"  åœ–åƒ2ç‰¹å¾µé»: {len(kp2)}\")\n",
    "        \n",
    "        if desc1 is None or desc2 is None:\n",
    "            print(\"âŒ ç„¡æ³•æå–ç‰¹å¾µ\")\n",
    "            return None\n",
    "        \n",
    "        # åŒ¹é…ç‰¹å¾µ\n",
    "        matches = self.match_features(desc1, desc2)\n",
    "        print(f\"  åŒ¹é…ç‰¹å¾µé»: {len(matches)}\")\n",
    "        \n",
    "        if len(matches) < 10:\n",
    "            print(\"âŒ åŒ¹é…é»ä¸è¶³\")\n",
    "            return None\n",
    "        \n",
    "        # è¨ˆç®—å–®æ‡‰æ€§çŸ©é™£\n",
    "        H, mask = self.find_homography(kp1, kp2, matches)\n",
    "        \n",
    "        if H is None:\n",
    "            print(\"âŒ ç„¡æ³•è¨ˆç®—å–®æ‡‰æ€§çŸ©é™£\")\n",
    "            return None\n",
    "        \n",
    "        inliers = np.sum(mask) if mask is not None else 0\n",
    "        print(f\"  å…§é»æ•¸é‡: {inliers}/{len(matches)}\")\n",
    "        \n",
    "        # è¨ˆç®—æ‹¼æ¥ç•«å¸ƒå¤§å°\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        \n",
    "        # è®Šæ›img1çš„è§’é»åˆ°img2çš„åº§æ¨™ç³»\n",
    "        corners1 = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]]).reshape(-1, 1, 2)\n",
    "        transformed_corners = cv2.perspectiveTransform(corners1, H)\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰è§’é»\n",
    "        all_corners = np.concatenate([transformed_corners, \n",
    "                                     np.float32([[0, 0], [w2, 0], [w2, h2], [0, h2]]).reshape(-1, 1, 2)])\n",
    "        \n",
    "        # è¨ˆç®—é‚Šç•Œ\n",
    "        x_min, y_min = np.int32(all_corners.min(axis=0).ravel())\n",
    "        x_max, y_max = np.int32(all_corners.max(axis=0).ravel())\n",
    "        \n",
    "        # èª¿æ•´è®Šæ›çŸ©é™£ä»¥è™•ç†è² åº§æ¨™\n",
    "        translation = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
    "        H_adjusted = translation @ H\n",
    "        \n",
    "        # è¨ˆç®—è¼¸å‡ºå°ºå¯¸\n",
    "        output_width = x_max - x_min\n",
    "        output_height = y_max - y_min\n",
    "        \n",
    "        print(f\"  è¼¸å‡ºå°ºå¯¸: {output_width}x{output_height}\")\n",
    "        \n",
    "        # å‰µå»ºæ‹¼æ¥çµæœ\n",
    "        stitched = cv2.warpPerspective(img1, H_adjusted, (output_width, output_height))\n",
    "        \n",
    "        # å°‡ç¬¬äºŒå¼µåœ–åƒæ”¾ç½®åœ¨æ­£ç¢ºä½ç½®\n",
    "        stitched[-y_min:-y_min+h2, -x_min:-x_min+w2] = img2\n",
    "        \n",
    "        print(\"âœ… åŸºç¤æ‹¼æ¥å®Œæˆ\")\n",
    "        return stitched\n",
    "\n",
    "# å‰µå»ºæ‹¼æ¥å™¨\n",
    "stitcher = ImageStitcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŒ‘æˆ°2: é«˜ç´šåœ–åƒèåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedImageStitcher(ImageStitcher):\n",
    "    \"\"\"é€²éšåœ–åƒæ‹¼æ¥å™¨ï¼ŒåŒ…å«é«˜ç´šèåˆæŠ€è¡“\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def blend_images(self, img1, img2, mask):\n",
    "        \"\"\"åœ–åƒèåˆè™•ç†\"\"\"\n",
    "        # å¤šé »å¸¶èåˆ\n",
    "        return self.multiband_blending(img1, img2, mask)\n",
    "    \n",
    "    def multiband_blending(self, img1, img2, mask, levels=4):\n",
    "        \"\"\"å¤šé »å¸¶èåˆç®—æ³•\"\"\"\n",
    "        # å»ºç«‹é«˜æ–¯é‡‘å­—å¡”\n",
    "        GP1 = [img1.copy()]\n",
    "        GP2 = [img2.copy()]\n",
    "        \n",
    "        for i in range(levels):\n",
    "            GP1.append(cv2.pyrDown(GP1[i]))\n",
    "            GP2.append(cv2.pyrDown(GP2[i]))\n",
    "        \n",
    "        # å»ºç«‹æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”\n",
    "        LP1 = [GP1[levels]]\n",
    "        LP2 = [GP2[levels]]\n",
    "        \n",
    "        for i in range(levels, 0, -1):\n",
    "            size = (GP1[i-1].shape[1], GP1[i-1].shape[0])\n",
    "            L1 = cv2.subtract(GP1[i-1], cv2.pyrUp(GP1[i], dstsize=size))\n",
    "            L2 = cv2.subtract(GP2[i-1], cv2.pyrUp(GP2[i], dstsize=size))\n",
    "            LP1.append(L1)\n",
    "            LP2.append(L2)\n",
    "        \n",
    "        # èåˆæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”\n",
    "        LS = []\n",
    "        for l1, l2 in zip(LP1, LP2):\n",
    "            ls = l1 * 0.5 + l2 * 0.5  # ç°¡åŒ–èåˆ\n",
    "            LS.append(ls)\n",
    "        \n",
    "        # é‡å»ºåœ–åƒ\n",
    "        result = LS[0]\n",
    "        for i in range(1, len(LS)):\n",
    "            size = (LS[i].shape[1], LS[i].shape[0])\n",
    "            result = cv2.add(cv2.pyrUp(result, dstsize=size), LS[i])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def stitch_multiple_images(self, images):\n",
    "        \"\"\"æ‹¼æ¥å¤šå¼µåœ–åƒ\"\"\"\n",
    "        if len(images) < 2:\n",
    "            return images[0] if images else None\n",
    "        \n",
    "        print(f\"ğŸ”„ é–‹å§‹æ‹¼æ¥ {len(images)} å¼µåœ–åƒ...\")\n",
    "        \n",
    "        result = images[0]\n",
    "        \n",
    "        for i in range(1, len(images)):\n",
    "            print(f\"  æ‹¼æ¥ç¬¬ {i+1} å¼µåœ–åƒ...\")\n",
    "            \n",
    "            stitched = self.stitch_two_images(result, images[i])\n",
    "            \n",
    "            if stitched is not None:\n",
    "                result = stitched\n",
    "                print(f\"  âœ… ç¬¬ {i+1} å¼µåœ–åƒæ‹¼æ¥æˆåŠŸ\")\n",
    "            else:\n",
    "                print(f\"  âŒ ç¬¬ {i+1} å¼µåœ–åƒæ‹¼æ¥å¤±æ•—\")\n",
    "                break\n",
    "        \n",
    "        return result\n",
    "\n",
    "# å‰µå»ºé€²éšæ‹¼æ¥å™¨\n",
    "advanced_stitcher = AdvancedImageStitcher()\n",
    "print(\"âœ… é€²éšåœ–åƒæ‹¼æ¥å™¨åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç·´ç¿’ä»»å‹™: å‰µå»ºå…¨æ™¯åœ–åƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_images():\n",
    "    \"\"\"å‰µå»ºæ¸¬è©¦åœ–åƒåºåˆ—\"\"\"\n",
    "    # å‰µå»ºæ¨¡æ“¬çš„é‡ç–Šåœ–åƒåºåˆ—\n",
    "    base_image = np.random.randint(100, 200, (300, 400, 3), dtype=np.uint8)\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # å‰µå»ºé‡ç–Šå€åŸŸ\n",
    "        img = base_image.copy()\n",
    "        \n",
    "        # æ·»åŠ ç¨ç‰¹ç‰¹å¾µ\n",
    "        cv2.rectangle(img, (50 + i*80, 50), (150 + i*80, 150), (255, 0, 0), -1)\n",
    "        cv2.circle(img, (200 + i*60, 200), 30, (0, 255, 0), -1)\n",
    "        cv2.putText(img, f\"Image {i+1}\", (100 + i*70, 250), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦åœ–åƒ\n",
    "test_images = create_test_images()\n",
    "\n",
    "print(f\"ğŸ“¸ å‰µå»ºäº† {len(test_images)} å¼µæ¸¬è©¦åœ–åƒ\")\n",
    "\n",
    "# é¡¯ç¤ºæ¸¬è©¦åœ–åƒ\n",
    "display_multiple_images(test_images, \n",
    "                       [f\"åœ–åƒ {i+1}\" for i in range(len(test_images))],\n",
    "                       figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œåœ–åƒæ‹¼æ¥\n",
    "print(\"ğŸ¬ é–‹å§‹å‰µå»ºå…¨æ™¯åœ–åƒ...\")\n",
    "\n",
    "# ä½¿ç”¨OpenCVå…§å»ºçš„Stitcher (æ¯”è¼ƒåŸºæº–)\n",
    "try:\n",
    "    opencv_stitcher = cv2.Stitcher.create()\n",
    "    status, opencv_panorama = opencv_stitcher.stitch(test_images)\n",
    "    \n",
    "    if status == cv2.Stitcher_OK:\n",
    "        print(\"âœ… OpenCV StitcheræˆåŠŸ\")\n",
    "    else:\n",
    "        print(f\"âŒ OpenCV Stitcherå¤±æ•—: {status}\")\n",
    "        opencv_panorama = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ OpenCV Stitcherä¸å¯ç”¨: {e}\")\n",
    "    opencv_panorama = None\n",
    "\n",
    "# ä½¿ç”¨è‡ªå®šç¾©æ‹¼æ¥å™¨\n",
    "start_time = time.time()\n",
    "custom_panorama = advanced_stitcher.stitch_multiple_images(test_images)\n",
    "custom_time = (time.time() - start_time) * 1000\n",
    "\n",
    "if custom_panorama is not None:\n",
    "    print(f\"âœ… è‡ªå®šç¾©æ‹¼æ¥å™¨æˆåŠŸï¼Œè€—æ™‚: {custom_time:.1f}ms\")\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœæ¯”è¼ƒ\n",
    "    results_to_show = []\n",
    "    titles = []\n",
    "    \n",
    "    # é¡¯ç¤ºåŸå§‹åœ–åƒçµ„åˆ\n",
    "    combined_original = np.hstack(test_images)\n",
    "    results_to_show.append(combined_original)\n",
    "    titles.append(\"åŸå§‹åœ–åƒåºåˆ—\")\n",
    "    \n",
    "    # é¡¯ç¤ºè‡ªå®šç¾©çµæœ\n",
    "    results_to_show.append(custom_panorama)\n",
    "    titles.append(f\"è‡ªå®šç¾©æ‹¼æ¥çµæœ\\n{custom_time:.1f}ms\")\n",
    "    \n",
    "    # é¡¯ç¤ºOpenCVçµæœï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "    if opencv_panorama is not None:\n",
    "        results_to_show.append(opencv_panorama)\n",
    "        titles.append(\"OpenCV Stitcherçµæœ\")\n",
    "    \n",
    "    display_multiple_images(results_to_show, titles, figsize=(15, 10))\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ è‡ªå®šç¾©æ‹¼æ¥å™¨å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŒ‘æˆ°3: å“è³ªè©•ä¼°å’Œå„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stitching_quality(panorama, original_images):\n",
    "    \"\"\"è©•ä¼°æ‹¼æ¥å“è³ª\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # éŠ³åº¦è©•ä¼°\n",
    "    gray = cv2.cvtColor(panorama, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    metrics['sharpness'] = laplacian.var()\n",
    "    \n",
    "    # å°æ¯”åº¦è©•ä¼°\n",
    "    metrics['contrast'] = np.std(gray)\n",
    "    \n",
    "    # æ¥ç¸«æª¢æ¸¬\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    seam_pixels = np.sum(edges > 0)\n",
    "    metrics['seam_visibility'] = seam_pixels / (panorama.shape[0] * panorama.shape[1])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# è©•ä¼°æ‹¼æ¥å“è³ª\n",
    "if custom_panorama is not None:\n",
    "    quality_metrics = evaluate_stitching_quality(custom_panorama, test_images)\n",
    "    \n",
    "    print(\"ğŸ“Š æ‹¼æ¥å“è³ªè©•ä¼°:\")\n",
    "    print(f\"  éŠ³åº¦: {quality_metrics['sharpness']:.1f}\")\n",
    "    print(f\"  å°æ¯”åº¦: {quality_metrics['contrast']:.1f}\")\n",
    "    print(f\"  æ¥ç¸«å¯è¦‹åº¦: {quality_metrics['seam_visibility']:.4f}\")\n",
    "    \n",
    "    # å“è³ªè©•ç´š\n",
    "    if quality_metrics['sharpness'] > 100 and quality_metrics['seam_visibility'] < 0.1:\n",
    "        grade = \"å„ªç§€\"\n",
    "    elif quality_metrics['sharpness'] > 50:\n",
    "        grade = \"è‰¯å¥½\"\n",
    "    else:\n",
    "        grade = \"éœ€æ”¹å–„\"\n",
    "    \n",
    "    print(f\"ğŸ† æ‹¼æ¥å“è³ª: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµèˆ‡å»¶ä¼¸\n",
    "\n",
    "### å®ŒæˆæŠ€èƒ½æª¢æ ¸\n",
    "- [ ] å¯¦ç¾äº†åŸºç¤çš„å…©åœ–åƒæ‹¼æ¥\n",
    "- [ ] å®Œæˆäº†å¤šåœ–åƒåºåˆ—æ‹¼æ¥\n",
    "- [ ] ç†è§£äº†ç‰¹å¾µåŒ¹é…å’Œå¹¾ä½•è®Šæ›\n",
    "- [ ] å¯¦ç¾äº†å“è³ªè©•ä¼°æ©Ÿåˆ¶\n",
    "- [ ] èƒ½è™•ç†æ‹¼æ¥å¤±æ•—çš„æƒ…æ³\n",
    "\n",
    "### é€²éšæŒ‘æˆ¦æ–¹å‘\n",
    "- å¯¦ç¾æ›´å¥½çš„åœ–åƒèåˆç®—æ³•\n",
    "- æ·»åŠ æ›å…‰è£œå„ŸåŠŸèƒ½\n",
    "- è™•ç†é­šçœ¼é¡é ­ç•¸è®Š\n",
    "- å¯¦ç¾æŸ±é¢æŠ•å½±æ‹¼æ¥\n",
    "- å„ªåŒ–å¤§åœ–åƒçš„è™•ç†æ€§èƒ½"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}