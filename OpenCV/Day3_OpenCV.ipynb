{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11. 特徵擷取(SIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-1: 擷取原理\n",
    "\n",
    "* 要辨識某物體的條件就是先掌握其特徵！由於我們要辨識的是某個物件而非整張相片，因此需要提取所謂稱為「Local features」的特徵，作法是先在影像中`選取重要的特徵點`，接著以其為 base 取得周圍的特徵（即local features），這些來自不同相片的 local features 會透過稍後會說明的 Feature matching 功能來比對是否有相同的物件。\n",
    "\n",
    "* 特徵點 → 局部特徵 → Feature matching 這些特性可用 `edges、corners、blobs (斑點)` 等組合來描述 Keypoint detection、Feature extraction 以及 Feature matching\n",
    "\n",
    "> * Keypoint detection ：在圖片中取得感興趣的關鍵點（可能為 edges、corners 或 blobs）。\n",
    "> * Feature extraction ：針對各關鍵點提取該區域的 features（我們稱為 local features）。\n",
    "> * 關鍵點篩選並進行 Feature matching。\n",
    "\n",
    "> https://gilscvblog.com/2013/08/18/a-short-introduction-to-descriptors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-2: Keypoint Descriptor\n",
    "### FAST – （Features from Accelerated Segment Test）演算法 FastFeatureDetector\n",
    "> 原理簡單, 執行速度相當快, 主要用於偵測 corners，亦可偵測 blob。適用於要求速度的 real-time analysis, 適用於`速度慢或較低階的執行環境`。使用度極高，尤其在需要即時的 realtime 環境。\n",
    "\n",
    "> * FAST 方法認為，一個以 p 為中心、半徑為 r 的圓形，若它位於一個所謂的 corner上，那麼該圓的圓周上必有連續 n 個點，其強度值`大於或小於中心點 p `加上一個指定的 threshold 門檻的強度值。如果是的話，該中心點 p 便被認為是 keypoint。\n",
    "\n",
    "> * 考慮是否應該將中心像素 p 視為關鍵點。中心像素 p 具有灰度強度值 p ＝ 32。為了使該像素成為關鍵點，必須在圓的邊界上具有 n = 12 / 16 (一般 `3/4`) 個連續像素，`這些像素要不就比 p + t 亮，要不就比 p – t 暗`。在此示例中，假設 t = 16。\n",
    "\n",
    "><img src=\"./image/KpFast01.png\"  style='width:90%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img src=\"./image/KpFast03.png\"  style='width:50%'></img>\n",
    "\n",
    "> FAST演算法提取角點的步驟：\n",
    "> * 在圖像中選擇圖元 p，假設其灰度值為：Ip\n",
    "> * 設置一個閾值 T，例如：Ip 的20%\n",
    "> * 選擇 p 周圍半徑為 3 的圓上的 16 個圖元，作為比較圖元\n",
    "> * 假設選取的圓上有`連續`的 N 個圖元`大於Ip+T或者小於 Ip−T`，那麼可以認為圖元 p 就是一個特徵點。（N 通常取12，即為 FAST-12；常用的還有 FAST-9, FAST-11）。\n",
    "\n",
    "> 為了獲得更快的效果，也採用了而外的加速辦法。首先對候選點的周圍每個90 度的點：1，9，5，13 進行測試（先測試1 和19, 如果它們符合閾值要求再測試5 和13）。如果p 是角點，那麼這四個點中至少有3 個要符合閾值要求。如果不是的話肯定不是角點，就放棄吧。\n",
    "\n",
    "> 缺點 :\n",
    "> * 檢測到的特徵點過多並且會出現 `紮堆` 的現象。這可以在第一遍檢測完成後，使用非最大值抑制（Non-maximal suppression），在`一定區域內僅保留回應極大值的角點`，避免角點集中的情況。\n",
    "> * FAST 提取到的角點`沒有方向和尺度`資訊\n",
    "> * SIFT 和 SURF 演算法都包含有各自的特徵點描述子的計算方法，而 FAST 不包含特徵點描述子的計算，僅僅只有特徵點的提取方法，這就需要一個特徵點描述方法來描述 FAST 提取到的特徵點，以方便特徵點的匹配\n",
    "\n",
    "https://docs.opencv.org/3.4/df/d74/classcv_1_1FastFeatureDetector.html\n",
    "\n",
    "## FAST\n",
    "> kp：關鍵點信息，包括位置，尺度，方向信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "  \n",
    "# load the image and convert it to grayscale\n",
    "image = cv2.imread('./image/lenaColor.png')\n",
    "# image = cv2.imread('./image/fruits.png')\n",
    "\n",
    "orig = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "fast = cv2.FastFeatureDetector_create(threshold=None, nonmaxSuppression=True)  # Fast feature detector\n",
    "kps = fast.detect(gray, None)\n",
    "\n",
    "print(f'keypoint counts\\t\\t: {len(kps)}\\n'\n",
    "      f'type\\t\\t\\t: {type(kps)}\\n'\n",
    "      f'Threshold\\t\\t: {fast.getThreshold()}\\n'\n",
    "      f'nonmaxSuppression\\t: {fast.getNonmaxSuppression()}\\n'\n",
    "      # f'neighborhood\\t\\t: {fast.getType()}\\n\\n'\n",
    "      f'kps[:10] :\\n{kps[:10]}')\n",
    "# Print all default params\n",
    "\n",
    "image = cv2.drawKeypoints(gray, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)   # image : image output\n",
    "\n",
    "cv2.imshow('Images', np.hstack([orig, image]))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT : Scale Invariant Feature Transform，尺度不變特徵變換。\n",
    "> SIFT特徵對`旋轉、尺度縮放、亮度變化等保持不變性`，是一種非常穩定的局部特徵。\n",
    "\n",
    "> Fast Keypoint 在影像旋轉的情況下也可以檢測到，但是如果減小(或者增加)影像的大小，可能會丟失影像的某些部分，甚至導致檢測到的角點發生改變。這樣的損失現像`需要一種與影像比例無關`的角點檢測方法來解決。尺度不變特徵變換 (Scale-Invariant Feature Transform, SIFT) 可以解決這個問題。\n",
    "\n",
    "> SIFT 演算法利用 DoG (差分高斯)來提取關鍵點(或者說成特徵點)，DoG 的思維是用不同的尺度空間因子 (高斯正態分佈的標準差σ) 對影像進行平滑，然後比較平滑後圖像的區別，差別大的畫素就是特徵明顯的點，即可能是特徵點。對得到的所有特徵點，我們剔除一些不好的，SIFT運算元會把剩下的每個特徵點用一個128維的特徵向量進行描述\n",
    "\n",
    "> * 原理較為複雜。\n",
    "> * 主要針對 blob 偵測，不過亦可偵測 corners。\n",
    "> * 可適應物件的 scale (大小變化)及 angle（旋轉角度）等情況。\n",
    "> * 在DoG模型中，使用了`不同尺寸影像並套用高斯模糊`，`比較不同模糊比例之間的變化`，來決定是否為 keypoint。\n",
    "> * 由於計算量大，DoG 的執行速度較慢，不適用於 realtime 的環境。\n",
    "> * SIFT已廣泛的應用於電腦視覺領域，並成為評定新 keypoint detecter 的效率指標。\n",
    "\n",
    "> 速度上 `ORB > SURF > SIFT，SURF的魯鈍性(抗干擾能力）更好一些`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img src=\"./image/sift_dog.jpg\"  style='width:90%'></img>\n",
    "><img src=\"./image/SIFT.png\"  style='width:90%'></img>\n",
    "https://aishack.in/tutorials/sift-scale-invariant-feature-transform-scale-space/\n",
    "\n",
    "> ### SIFT 演算法: \n",
    "> 1. 準備輸入圖像的不同比例的副本。為每個比例創建八度比例圖像。\n",
    "> 2. 對於每個八度比例，高斯模糊濾鏡會應用到所有強度增加的圖像。\n",
    "> 3. 對八度內的所有兩個後續圖像計算高斯差（DoG）。\n",
    "> 4. 掃描中間DoG相對於 26 鄰域的極值。通過在極值上應用高通濾波器來檢索特徵。\n",
    "> 5. 確定特徵的`漸變，方向和位置`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "gray = cv2.imread('./image/lenaColor.png', 0)\n",
    "sift=cv2.xfeatures2d.SIFT_create()    # create object\n",
    "# sift = cv2.SIFT_create()\n",
    "kp = sift.detect(gray, None)\n",
    "print(f'kp[:5]\\t:\\n{kp[:5]}\\n\\n'\n",
    "      f'len(kp)\\t\\t: {len(kp)}\\n'\n",
    "      f'kp[0].pt\\t: {kp[0].pt}\\n'          # 座標\n",
    "      f'kp[0].size\\t: {kp[0].size}')       # 強度\n",
    "\n",
    "img = cv2.drawKeypoints(gray, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)   # output to 'img'\n",
    "# img = cv2.drawKeypoints(gray, kp, None)   # out to 'img'\n",
    "cv2.imshow('sift_kp', img)\n",
    "\n",
    "cv2.imwrite('./image/lena_sift_kp.jpg', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sift / save keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "gray = cv2.imread('./image/mybaby.jpg', 0)\n",
    "# gray = cv2.imread('./image/lenaColor.png', 0)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# kp = sift.detect(gray,None)\n",
    "\n",
    "# directly find keypoints and descriptors in a single step\n",
    "kp, des = sift.detectAndCompute(gray, None)   # kp指關鍵點, des指關鍵點的特徵描述\n",
    "print(f'kps count : {len(kp)}\\ndes[0] :\\n{des[0:2]}')\n",
    "img = cv2.drawKeypoints(gray, kp, None)\n",
    "\n",
    "np.save('./image/mybaby_sift_kp', des)\n",
    "cv2.imshow('SIFT', img)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "cv2.imwrite('./image/mybaby_sift.jpg', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF : This algorithm `is patented`\n",
    "Speeded Up Robust Features。加速版的SIFT。\n",
    "SURF的流程和 SIFT 比較類似，這些改進體現在以下幾個方面：<br>\n",
    " * 特徵點檢測是基於 Hessian 矩陣，依據 Hessian 矩陣行列式的極值來定位特徵點的位置。\n",
    " * 並且將 Hession 特徵計算與高斯平滑結合在一起，兩個操作通過近似處理得到一個核範本。\n",
    " * 在構建尺度空間時，使用`box filter`與源圖像卷積，而不是使用DoG運算元。\n",
    " * SURF 使用一階 Haar 小波在 x、y 兩個方向的回應作為構建特徵向量的分佈資訊。\n",
    "\n",
    "SURF 演算法比 SIFT 快好幾倍，它吸收了 SIFT 演算法的思想。SURF 採用 Hessian 演算法檢測關鍵點。SURF 需要提供閾值，特徵隨著閾值的增加而減少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "gray = cv2.imread('./image/lenaColor.png', 0)\n",
    "\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "# Find keypoints and descriptors directly\n",
    "kp, des = surf.detectAndCompute(gray, None)\n",
    "len(kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief\n",
    "過程如下：\n",
    "\n",
    " * 為減少雜訊干擾，先對圖像進行高斯濾波（方差為2，高斯窗口為 9x9）<br>\n",
    " * 以特徵點為中心，取 SxS 的鄰域大視窗。在大視窗中隨機選取一對（兩個）5x5 的子視窗，比較子視窗內的圖元和（可用積分圖像完成），進行二進位賦值.（一般 S=31）其中，p(x)，p(y)分別隨機點 x=(u1, v1), y=(u2, v2) 所在 5x5 子視窗的圖元和.<br>\n",
    " * 在大視窗中隨機選取N對子視窗，重複步驟2 的二進位賦值，形成一個二進位編碼，這個編碼就是對特徵點的描述，即特徵描述子.（一般 N=256）<br>\n",
    "非常重要的一點是：BRIEF 是一種特徵描述符，它不提供查找特徵的方法。所以我們不得不使用其他特徵檢測器，比如 SIFT 和 SURF 等。原始文獻推薦使用 CenSurE 特徵檢測器，這種演算法很快。而且 BRIEF 演算法對 CenSurE 關鍵點的描述效果要比 SURF 關鍵點的描述更好。\n",
    "\n",
    "簡單來說 BRIEF 是一種對特徵點描述符計算和匹配的快速方法。這種演算法可以實現很高的識別率，除非出現平面內的大旋轉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png',0)\n",
    "star = cv2.xfeatures2d.StarDetector_create()\n",
    "kp = star.detect(img,None)\n",
    "\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # 初始化BRIEF特徵提取器\n",
    "kp, des = brief.compute(img, kp)                          # 計算特徵描述\n",
    "img = cv2.drawKeypoints(img, kp, None, color=-1)\n",
    "\n",
    "cv2.imshow('Brief', img)\n",
    "\n",
    "print(f'size\\t\\t: {brief.descriptorSize()}\\n\\n'\n",
    "      f'des[0]\\t\\t:\\n{des[0]}\\n\\n'\n",
    "      f'des.shape\\t:\\n{des}\\n\\n'\n",
    "      f'len\\t\\t: {len(kp)}\\n')\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png',0)\n",
    "\n",
    "star = cv2.xfeatures2d.StarDetector_create()               # 初始化STAR檢測器\n",
    "kp = star.detect(img,None)                                # 使用STAR尋找特徵點\n",
    "img = cv2.drawKeypoints(img, kp, None, color=-1)\n",
    "print(len(kp))\n",
    "cv2.imshow('Star', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB : ORiented Brief / Oriented FAST and Rotated BRIEF\n",
    "> ORB亦是針對 FAST 的強化，但除了scale space invariance，亦加入了旋轉不變性(rotation invariance)。\n",
    "\n",
    "> ORB原理有三大步驟：\n",
    "> * Pyramid 圖像尺寸並進行各尺寸的FAST計算。\n",
    "> * 使用 Harris keypoint detector的方法計算每個 keypoint分數（是否近似corner？），並進行排序，最多僅取500個keypoints，其餘則丟棄。\n",
    "> * 於此第三步中加入旋轉不變性，使用「intensity centroid」計算每個 keypoint 的 rotation。ORB 與 BRISK 相同，繼承了 FAST 運算快速的特性，可適用於realtime分析。\n",
    "\n",
    "ORB 是用來取代 SIFT 和 SURF 的，與兩者相比，ORB有更快的速度。ORB 用FAST來檢測關鍵點，用 BRIEF 來進行關鍵點特徵描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# image = cv2.imread('./image/blox.jpg')\n",
    "image = cv2.imread('./image/lenaColor.png')\n",
    "\n",
    "# image = cv2.imread('./image/lenaColor.png', cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift_feature = cv2.xfeatures2d.SIFT_create()\n",
    "sift_kp = sift_feature.detect(image)\n",
    "sift_out = cv2.drawKeypoints(image, sift_kp, None)\n",
    "\n",
    "orb_feature = cv2.ORB_create()\n",
    "orb_kp  = orb_feature.detect(image)\n",
    "orb_out  = cv2.drawKeypoints(image, orb_kp, None)\n",
    "\n",
    "font = 2;    lt = 16\n",
    "loc = (10, 40); color = (255, 255, 255)\n",
    "cv2.putText(image, 'original', loc, font, 1, color, 2, lt)\n",
    "cv2.putText(sift_out, 'sift', loc, font, 1, color, 2, lt)\n",
    "cv2.putText(orb_out, 'orb', loc, font, 1, color, 2, lt)\n",
    "\n",
    "image = cv2.hconcat([image, sift_out, orb_out])\n",
    "\n",
    "cv2.imshow('image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris\n",
    "> 速度相當快，但仍較 FAST 慢，不過偵測 corner 比起 FAST 準確一些。廣泛應用於偵測 edges 及 corners, Harris 可用於識別角點。此函數可以很好的檢測角點，這些角點在圖像旋轉的情況下也能被檢測到。但是如果減少或者增加圖像的尺寸，可能會丟失圖像的某些部分，也有可能增加圖像的角點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png', 1)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "dst = cv2.cornerHarris(gray, blockSize=5, ksize=3, k=0.04)  \n",
    "print(len(dst))\n",
    "# blockSize:檢測的臨點數, ksize:sobel邊緣檢測的核, k:目標函式的一個引數（一般取值較小）\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "# dst = cv2.dilate(dst, None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "cv2.imshow('dst', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-3: Keypoint Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match knn\n",
    "> BFmatcher (暴力匹配)：計算匹配圖層的一個特徵描述子與待匹配圖層的所有特徵描述子的距離返回最近距離。<br>\n",
    "> FlannBasedMatcher：是目前最快的特徵匹配演算法（最近鄰搜尋）\n",
    "\n",
    "那麼這個這個DMatch資料結構究竟是什麼呢？<br>\n",
    "它包含三個非常重要的資料分別是 queryIdx，trainIdx，distance\n",
    "* queryIdx：測試圖像的特徵點描述符的下標（第幾個特徵點描述符），同時也是描述符對應特徵點的下標。\n",
    "* trainIdx：樣本圖像的特徵點描述符下標,同時也是描述符對應特徵點的下標。\n",
    "* distance：代表這怡翠匹配的特徵點描述符的`歐式距離`，數值越小也就說明倆個特徵點越相近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# ======== example 0 =================\n",
    "# img1 = cv2.imread('./image/box.png')\n",
    "# img2 = cv2.resize(cv2.imread('./image/box_in_scene.png'), None, fx=1.4, fy=1.4)\n",
    "\n",
    "# ======== example 1 =================\n",
    "# img1 = cv2.imread('./image/testpic.jpg')\n",
    "# img2 = cv2.imread('./image/testpic2.jpg')\n",
    "\n",
    "# ======== example 2 =================\n",
    "# img1 = cv2.imread('./image/aiotbooks.jpg')\n",
    "# img2 = cv2.imread('./image/aiotimage.jpg')\n",
    "\n",
    "# ======== example 3 =================\n",
    "img1 = cv2.imread('./image/mario.jpg')\n",
    "img2 = cv2.imread('./image/marioCoin.jpg')\n",
    "\n",
    "feature = cv2.xfeatures2d.SIFT_create()\n",
    "# feature = cv2.xfeatures2d.SURF_create()   # This algorithm is patented and is excluded in this configuration\n",
    "\n",
    "kp1, des1 = feature.detectAndCompute(img1, None)\n",
    "kp2, des2 = feature.detectAndCompute(img2, None)\n",
    "\n",
    "# ========= BFMatcher ================\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)  # k Count of best matches found per each query descriptor\n",
    "# matches = bf.Match(des1, des2)  # k Count of best matches found per each query descriptor\n",
    "# matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "print(f'len(matches)\\t\\t= {len(matches)} 組, \\tk = 2\\n')\n",
    "\n",
    "for i in range(3) :\n",
    "    print(f'matches[{i}]\\t\\t= {matches[i]}')\n",
    "    for j in range(2) :\n",
    "        print(f'matches[{i}][{j}].queryIdx\\t= {matches[i][j].queryIdx}\\n'\n",
    "              f'matches[{i}][{j}].trainIdx\\t= {matches[i][j].trainIdx}\\n'\n",
    "              f'matches[{i}][{j}].distance\\t= {matches[i][j].distance:.2f}\\n')\n",
    "    print('-'*80)\n",
    "\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if (m.distance < .4 * n.distance):      # #如果第一個鄰近距離比第二個鄰近距離的0.4倍小，則保留 try 0.5, 0.6, 0.7\n",
    "    # if m.distance / n.distance < 0.4:      # #如果第一個鄰近距離比第二個鄰近距離的0.4倍小，則保留 try 0.5, 0.6, 0.7\n",
    "    # if m.distance < 100:      # #如果第一個鄰近距離\n",
    "        good.append(m)\n",
    "print(f'Matching points : {len(good)}')\n",
    "img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, [good], outImg=None, \n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "img3 = cv2.resize(img3, None, fx=.8, fy=.8)\n",
    "cv2.imshow('video', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orb match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# ======== example 0 =================\n",
    "# img1 = cv2.imread('./image/box.png')\n",
    "# img2 = cv2.imread('./image/box_in_scene.png')\n",
    "\n",
    "# ======== example 1 =================\n",
    "# img1 = cv2.imread('./image/testpic.jpg')\n",
    "# img2 = cv2.imread('./image/testpic2.jpg')\n",
    "\n",
    "# ======== example 2 =================\n",
    "img1 = cv2.imread('./image/aiotbooks01.jpg')\n",
    "img2 = cv2.imread('./image/aiotimage.jpg')\n",
    "\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:60], outImg=None,\n",
    "                       flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# width, height, channel = img3.shape\n",
    "# ratio = float(width) / float(height)\n",
    "# img3 = cv2.resize(img3, (1024, int(1024 * ratio)))\n",
    "cv2.imshow('image', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN 是快速最近鄰搜尋包 (Fast_Library_for_Approximate_Nearest_Neighbors) 的簡稱。\n",
    "> 它是一個對大資料集和高維特徵進行最近鄰搜尋的演算法的集合,而且這些演算法都已經被優化過了。在面對大資料集時它的效果要好於 BFMatcher。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('./image/box.png', 0)  # queryImage\n",
    "img2 = cv2.imread('./image/box_in_scene.png', 0)  # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "# sift = cv2.SIFT()\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# ============= FLANN parameters ===========\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)  # or pass empty dictionary\n",
    "\n",
    "# ============= Brute-Force ============\n",
    "# bf = cv2.BFMatcher()\n",
    "# matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# ============= Flann ============\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, (m, n) in enumerate(matches):\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        matchesMask[i] = [1, 0]\n",
    "\n",
    "draw_params = dict(matchColor=(0, 255, 0),\n",
    "                   singlePointColor=(255, 0, 0),\n",
    "                   matchesMask=matchesMask,\n",
    "                   flags=0)\n",
    "\n",
    "img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(img3, ), plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MIN_MATCH_COUNT = 10\n",
    "# =======================================\n",
    "# img1 = cv2.imread('./image/box.png', 0)    # Source Image\n",
    "# img2 = cv2.imread('./image/box_in_scene.png', 0)    # Check Image\n",
    "#=======================================\n",
    "img1 = cv2.imread('./image/jp_01.png', 0)    # Source Image\n",
    "img2 = cv2.imread('./image/jp_02.png', 0)    # Check Image\n",
    "\n",
    "#=======================================\n",
    "# img1 = cv2.imread('./image/douglas_01.png', 0)    # Source Image\n",
    "# img2 = cv2.imread('./image/douglas_02.png', 0)    # Check Image\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "   \n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.flatten().tolist()\n",
    "\n",
    "    h,w = img1.shape\n",
    "    pts = np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "else:\n",
    "    print(f'Not enough matches are found - {len(good)}/{MIN_MATCH_COUNT}')\n",
    "    matchesMask = None\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "        singlePointColor = None,\n",
    "        matchesMask = matchesMask, # draw only inliers\n",
    "        flags = 2)\n",
    "\n",
    "img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "\n",
    "# cv2.imshow('Result', img3)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img3, 'gray'),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 12. 直方圖處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12-1: 直方圖的含意\n",
    "> 定義 : 直方圖反應了一張圖像內所有像素強度從 `0 至 255 的分佈情形`<br>\n",
    "> 特色 : 能掌握圖像的明暗、對比及像素強度分布\n",
    "\n",
    "> Histograms 是直方圖的意思，在攝影領域一般稱為`曝光直方圖`，主要用以分析一張照片的曝光是否正確。應用在電腦視覺則主要利用它來判斷白平衡、處理 thresholding，也可用於物體追蹤（例如 CamShift 演算法即使用彩色直方圖的變化達到跟蹤目的），另外也可用於圖像檢索目的（例如 Bag-of-words 詞袋演算法）、或應用於機器學習運算…等等。因為 Histograms 為我們統計出各像素`強度頻率`的資訊來呈現整張相片的色彩分佈情況，因此我們得以透過此資訊來猜測該圖片的物件及特性。\n",
    "\n",
    ">><img src=\"./image/calcHist.jpg\"  style='width:60%'>\n",
    "\n",
    "### 直方圖均衡化，用於提高影像的質量, 直方圖均衡化是通過拉伸畫素`強度分佈範圍`來增強影像對比度的一種方法.\n",
    "> 所謂直方圖就是對影像的中的這些畫素點的值進行統計，得到一個統一的整體的灰度概念。直方圖的好處就在於可以清晰瞭解影像的整體灰度分佈，這對於後面依據直方圖處理影像來說至關重要。\n",
    "\n",
    "> 在開發影像處理的程式時，我們時常會需要觀察影像像素值的分佈與特性，以便選用適合的演算法、制定門檻值、設計出適合的影像處理流程。\n",
    "\n",
    "><img src=\"./image/histogram_equalization.jpg\"  style='width:90%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "## 12-2: 直方圖均衡化\n",
    "\n",
    "> 使用 cv2.calcHist 可產生指定圖片的直方圖資訊。<br>\n",
    "> cv2.calcHist ( images, channels, mask, histSize, ranges )\n",
    "\n",
    "> |引數      |說明                                                                          |\n",
    "> |---------|------------------------------------------------------------------------------|\n",
    "> |images   |要分析的圖片檔，其型別可以是 uint8 或 float32，變數必須放在中`括號當中`，例如：[img]。|\n",
    "> |channels |產生的直方圖類型指定影像的通道（channel)。同樣必須放在中`括號當中`例：[0]→灰階，[0, 1, 2]→RGB三色。|\n",
    "> |mask     |optional，若有提供則僅計算mask的部份。，若指定為 None 則會計算整張圖形的所有像素。  |\n",
    "> |histSize |要切分的像素強度值範圍，`預設為256`。每個channel皆可指定一個範圍（bins)。例如，[32,32,32] 表示RGB三個channels皆切分為32區段，也就是圖形畫出來要有幾條長方形。                                  |\n",
    "> |ranges   |X軸(像素強度)的範圍，`預設為[0,256]`）。|\n",
    "\n",
    "https://zh.wikipedia.org/zh-tw/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12-3: 實作直方圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gray image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o = cv2.imread('./image/lenaColor.png', 0)\n",
    "cv2.imshow('original', o)\n",
    "cv_hist256 = cv2.calcHist([o], [0], None, [256], [0,255])\n",
    "cv_hist16 = cv2.calcHist([o], [0], None, [16], [0,255])\n",
    "\n",
    "hist, bins = np.histogram(o.flatten(), 256, [0,256])  # 拉平\n",
    "cdf = hist.cumsum()                                  # 累加 cdf : cumulate distribution function\n",
    "cdf_normalized = cdf * hist.max()/ cdf.max()         # 標準化\n",
    "print(hist.max(), cdf.max())\n",
    "\n",
    "fig=plt.figure(figsize=(12, 6))\n",
    "plt.subplot(221)  \n",
    "plt.hist(o.flatten(), 256); plt.title('bin=256')       # flatten 將多維陣列轉換為一維陣列的功能 like flatten\n",
    "plt.plot(cdf_normalized, color='r')                 # plot cdf\n",
    "\n",
    "plt.subplot(222)  \n",
    "plt.hist(o.flatten(), 16);  plt.title('bin=16')\n",
    "\n",
    "plt.subplot(223)  \n",
    "plt.plot(cv_hist256, color='y');  plt.title('cv_hist256')\n",
    "\n",
    "plt.subplot(224)  \n",
    "plt.plot(cv_hist16, color='r');  plt.title('cv_hist16')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### color image : cv mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o=cv2.imread('./image/lenaColor.png')\n",
    "cv2.imshow('original', o)\n",
    "\n",
    "cv_histb = cv2.calcHist([o], [0], None, [256], [0,255])   # [0] B\n",
    "cv_histg = cv2.calcHist([o], [1], None, [256], [0,255])   # [1] G\n",
    "cv_histr = cv2.calcHist([o], [2], None, [256], [0,255])   # [2] R\n",
    "\n",
    "mean, std = cv2.meanStdDev(o);    print(f'mean :\\n{mean}\\n\\nstd :\\n{std}')\n",
    "fig=plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(cv_histb, color='b');  plt.title('cv_hist b');  plt.axvline(x=mean[0], color='r')\n",
    "plt.text(mean[0]+2, cv_histb.max()*.95, f'avg.:{mean[0][0]:.2f}', color='r', fontsize=10)        \n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(cv_histg, color='g');  plt.title('cv_hist g');  plt.axvline(x=mean[1], color='r')\n",
    "plt.text(mean[1]+2, cv_histg.max()*.95, f'avg.:{mean[1][0]:.2f}', color='r', fontsize=10)        \n",
    "\n",
    "plt.subplot(223)  \n",
    "plt.plot(cv_histr, color='r');  plt.title('cv_hist r');  plt.axvline(x=mean[2], color='r')\n",
    "plt.text(mean[2]+2, cv_histr.max()*.95, f'avg.:{mean[2][0]:.2f}', color='r', fontsize=10)        \n",
    "\n",
    "color = ('b', 'g', 'r')\n",
    "plt.subplot(224);  plt.title('cv_hist rgb')\n",
    "for i, col in enumerate(color):\n",
    "    histr = cv2.calcHist([o], [i], None, [256], [0, 256])\n",
    "    plt.plot(histr, color = col)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gray Level 直方圖均衡化處理實例 Histogram Equalization (HE)\n",
    "> 定義 : 通過拉伸影像的像素強度分佈範圍來`增強圖像對比度`，適用於`過曝或背光的圖片`<br>\n",
    "> 缺點 : 對處理的數據不加選擇(全局處理)，如此一來`會增加背景雜訊的對比度`並且`降低有用訊號(特別亮或暗)的對比度`\n",
    "\n",
    "經過比較可以發現均衡化後的圖像明亮的部分出現了`過曝`，失去了原本的細節。這是因為原圖整體暗的部份較多，而經過直方圖均衡化後使原本亮的地方更亮，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gray level Histogram Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o = cv2.imread('./image/lenaColor.png', 0)\n",
    "equ=cv2.equalizeHist(o)\n",
    "\n",
    "# cv2.imshow('original', o)\n",
    "# cv2.imshow('equ', equ)\n",
    "print(o.size)\n",
    "\n",
    "hist, bins = np.histogram(o.flatten(),256,[0,256])\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized_o = cdf * hist.max()/ cdf.max()\n",
    "\n",
    "hist, bins = np.histogram(equ.flatten(),256,[0,256])\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized_eq = cdf * hist.max()/ cdf.max()\n",
    "\n",
    "fig=plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(o.flatten(), 256)\n",
    "plt.plot(cdf_normalized_o, color='r')\n",
    "\n",
    "plt.subplot(122)  \n",
    "plt.hist(equ.flatten(), 256)\n",
    "plt.plot(cdf_normalized_eq, color='r')\n",
    "plt.show()\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('gray_equ', equ)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color 均衡化處理實例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "equ = cv2.equalizeHist(gray)     # 灰度圖均衡化\n",
    "\n",
    "result1 = np.hstack((gray, equ))  # 水平拼接原圖和均衡圖\n",
    "cv2.imshow('grey_equ', result1)\n",
    "\n",
    "(b, g, r) = cv2.split(img)        # 彩色影像均衡化,需要分解通道 對每一個通道均衡化\n",
    "bH = cv2.equalizeHist(b)\n",
    "gH = cv2.equalizeHist(g)\n",
    "rH = cv2.equalizeHist(r)\n",
    "\n",
    "equ2 = cv2.merge((bH, gH, rH))    # 合併每一個通道\n",
    "\n",
    "result2 = np.hstack((img, equ2)) # 水平拼接原圖和均衡圖\n",
    "cv2.imshow('bgr_equ', result2)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自適應直方圖均衡 Adaptive Histogram Equalization AHE\n",
    "\n",
    "> 為了提高影像的`區域性對比度`, 將影像分成`若干子塊對子塊進行HE處理`，這便是AHE（自適應直方圖均衡化）\n",
    "它在每一個小區域內`（預設 8×8 ）`進行直方圖均衡化。當然，如果有噪點的話，噪點會被放大，需要對小區域內的對比度進行了限制。\n",
    "\n",
    "> * 演算法 ：與一般的直方圖均衡 (全局) 相比，AHE 透過計算圖像每一個顯著`區域的直方圖`，重新分佈圖像的亮度值來改變影像對比度。\n",
    "> * 優點 ：適合於改善影像的區域性對比度以及獲得更多的影像細節。\n",
    "> * 缺點 ：在對比度增強的同時，也放大了影像的噪音，\n",
    "\n",
    ">限制對比度自適應直方圖均衡 Contrast Limited Adaptive Histogram Equalization CLAHE 演算法 ：CLAHE與AHE都是局部均衡化，也就是把整個圖像分成許多小塊Tiles (OpenCV default為8×8)，對每個小塊進行均衡化。這種方法主要對於圖像直方圖不是那麼單一的圖像(e.g. 多峰情況)比較實用。所以在每一個的區域中，直方圖會集中在某一個小的區域中\n",
    "> * clipLimit 參數表示對比度的大小。 \n",
    "> * tileGridSize 參數表示每次處理塊的大小 。<br>\n",
    "\n",
    "https://iter01.com/518513.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png', 0)\n",
    "\n",
    "# 全域性直方圖均衡\n",
    "equ = cv2.equalizeHist(img)\n",
    "\n",
    "# 自適應直方圖均衡\n",
    "# @param clipLimit Threshold for contrast limiting.\n",
    "clahe = cv2.createCLAHE(clipLimit = 100.0, tileGridSize = (8, 8))  # AHE   # default cliplimit  = 40 \n",
    "cl1 = clahe.apply(img)\n",
    "\n",
    "# 水平拼接三張影像\n",
    "result1 = np.hstack((img, equ, cl1))\n",
    "cv2.imshow('clahe_result', cv2.resize(result1, (0,0), fx=.8, fy=.8))\n",
    "\n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(img.flatten(), 256, [0, 255], label='original image'), plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(equ.flatten(), 256, [0, 255], label='equalize image', color='orange'), plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(cl1.flatten(), 256, [0, 255], label='clahe image', color='g'), plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(img.flatten(), 256, [0, 255], label='original image')\n",
    "plt.hist(equ.flatten(), 256, [0, 255], label='equalize image')\n",
    "plt.hist(cl1.flatten(), 256, [0, 255], label='clahe image')\n",
    "plt.legend()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 13. 視訊處理\n",
    "> 介紹如何使用 OpenCV 擷取網路攝影機影像，處理與顯示即時的畫面影像，並將連續的畫面影像寫入影片檔案中儲存起來。網路攝影機的串流影像，可以透過 OpenCV 模組的 VideoCapture 影片擷取功能來達成，至於寫入影片檔則可使用 VideoWriter\n",
    "\n",
    "## 13-1:視訊預覽\n",
    "> 在準備擷取攝影機的影像之前，要先呼叫 cv2.VideoCapture 建立一個 VideoCapture 物件，這個 VideoCapture 物件會連接到一隻網路攝影機，我們可以靠著它的參數來指定要使用那一隻攝影機（0 代表第一隻、1 代表第二隻）。\n",
    "\n",
    "> 建立好 VideoCapture 物件之後，就可以使用它的 read 函數來擷取一張張連續的畫面影像了。\n",
    "\n",
    "> 在這個無窮迴圈中，每次呼叫 cap.read() 就會讀取一張畫面，其第一個傳回值 ret 代表成功與否（True 代表成功，False 代表失敗），而第二個傳回值 frame 就是攝影機的單張畫面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### play video file or camera preview\n",
    "> * cap = VideoCapture([index, file]) 開啟相機裝置 index, 視訊檔案 file\n",
    "> * retval = VideoCapture.isOpened()  : True, False 判斷視訊捕獲是否初始化成功。初始化成功返回true\n",
    "> * ret, frame = cap.read() : \n",
    "> * cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture('./video/chaplin.mp4')   # play video file\n",
    "cap = cv2.VideoCapture(0)                         # from camera\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)               # Frame Per Second\n",
    "F_Count = cap.get(cv2.CAP_PROP_FRAME_COUNT)   # frame count\n",
    "print(f'fps : {fps:.2f} f/s, Frame_Count : {F_Count}')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(1) == 27: break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)   # left side right\n",
    "    c=cv2.waitKey(25)            # 25 ms per frame     1/fps\n",
    "\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play video in jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "    <video alt='test' controls>\n",
    "        <source src='./video/chaplin.mp4' type='video/mp4'>\n",
    "    </video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-2: 視訊讀寫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imWrite : image writer\n",
    "* cv2.WINDOW_NORMAL : ``可以`` 讓使用者改變視窗大小\n",
    "* cv2.WINDOW_AUTOSIZE : 使用者 ``不可`` 改變視窗大小\n",
    "* CAP_PROP_FRAME_WIDTH, CAP_PROP_FRAME_HEIGHT : 取的影像的長寬\n",
    "* cv2.VideoCapture.set( propid, value )\n",
    "\n",
    "|類別                       | propId | 說明                                  |\n",
    "|---------------------------|:------:|---------------------------------------|\n",
    "|cv2.CAP_PROP_POS_MSEC      | 0      | 視訊檔案的當前位置（ms）                |\n",
    "|`cv2.CAP_PROP_POS_FRAMES`    | 1      | 從0開始索引幀，幀位置。                 |\n",
    "|cv2.CAP_PROP_POS_AVI_RATIO | 2      | 視訊檔案的相對位置（0表示開始，1表示結束）|\n",
    "|`cv2.CAP_PROP_FRAME_WIDTH`   | 3    | 視訊流的幀寬度                         |\n",
    "|`cv2.CAP_PROP_FRAME_HEIGHT`  | 4    | 視訊流的幀高度                         |\n",
    "|`cv2.CAP_PROP_FPS`           | 5    | 幀率                                  |\n",
    "|`cv2.CAP_PROP_FOURCC`        | 6    | 編解碼器四字元程式碼                   |\n",
    "|`cv2.CAP_PROP_FRAME_COUNT`   | 7    | 視訊檔案的幀數                         |\n",
    "|cv2.CAP_PROP_FORMAT        | 8      | retrieve()返回的Mat物件的格式          |\n",
    "|cv2.CAP_PROP_MODE          | 9      | 後端專用的值，指示當前捕獲模式           |\n",
    "|cv2.CAP_PROP_BRIGHTNESS    | 10     | 影像的亮度，僅適用於支援的相機           |\n",
    "|cv2.CAP_PROP_CONTRAST      | 11     | 影像對比度，僅適用於相機                |\n",
    "|cv2.CAP_PROP_SATURATION    | 12     | 影像飽和度，僅適用於相機                 |\n",
    "|cv2.CAP_PROP_HUE           | 13     | 影像色調，僅適用於相機                   |\n",
    "|cv2.CAP_PROP_GAIN          | 14     | 影像增益，僅適用於支援的相機             |\n",
    "|cv2.CAP_PROP_EXPOSURE      | 15     | 曝光，僅適用於支援的相機                 |\n",
    "|cv2.CAP_PROP_CONVERT_RGB   | 16     | 布林標誌，指示是否應將影像轉換為RGB       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take 1 picture manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import datetime as dt\n",
    "from datetime import datetime as dt\n",
    "now_dt = dt.now().strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "# cap = cv2.VideoCapture('./video/chaplin.mp4')   # play video file\n",
    "cap = cv2.VideoCapture(0)                         # from camera\n",
    "\n",
    "font = 2;    lt = 16\n",
    "print(f'frame_w\\t\\t: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}\\n'\n",
    "      f'frame_fps\\t: {cap.get(cv2.CAP_PROP_FPS)}\\n'\n",
    "      f'frame_count\\t: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "\n",
    "# ratio = cap.get(cv2.CAP_PROP_FRAME_WIDTH) / cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # cap.get得到相機/視訊檔案的屬性\n",
    "# w = 500;  h = int(w / ratio)\n",
    "\n",
    "# cv2.resizeWindow('frame', w, h)       # change frame size\n",
    "count=0\n",
    "while True:\n",
    "    ret, frame = cap.read()                          #  read frame : ret: True/ False,  frame:image\n",
    "    if not ret or cv2.waitKey(30) == 27: break             # wait for 1 ms     \n",
    "    \n",
    "    frame = cv2.flip(frame, 1)                       # 0 : 上下左右顛倒,  -1 : 上下顛倒\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('t') or cv2.waitKey(1)==ord('T'):\n",
    "        cv2.putText(frame, now_dt, (100, 300), font, 1, (0,0,255), 2, lt)\n",
    "        cv2.imwrite(f'./image/frame{count}.jpg', frame)     # save frame as JPEG file          \n",
    "        print(f'save image : frame{count}.jpg')\n",
    "        count+=1 \n",
    "        \n",
    "#     frame = cv2.resize(frame, (w, h))\n",
    "    cv2.putText(frame, f'{cap.get(cv2.CAP_PROP_POS_FRAMES):.0f} frames, {cap.get(cv2.CAP_PROP_POS_MSEC):.0f} ms', \n",
    "                (50, 250), font, .8, (0,0,255), 2, lt)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capture 3 images from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from datetime import datetime as dt\n",
    "now_dt = dt.now().strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "vidcap = cv2.VideoCapture('./video/chaplin.mp4')\n",
    "font = 2;    lt = 16\n",
    "success, image = vidcap.read()\n",
    "count = 0\n",
    "while success and count < 3:\n",
    "    print('Read a new frame :', success)\n",
    "    success, image = vidcap.read()\n",
    "    cv2.putText(image, now_dt+' cap 3', (100, 300), font, 1, (0,0,255), 2, lt)\n",
    "    cv2.imwrite(f'./image/frame{count}.jpg', image)     # save frame as JPEG file      \n",
    "    print(f'Write a new frame: frame{count}.jpg\\n'+'='*20)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-3: 視訊物件ROI, 追蹤和去背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### track one obj in video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture('./video/vtest.avi')\n",
    "# cap = cv2.VideoCapture('./video/overpass.mp4')\n",
    "cap = cv2.VideoCapture('./video/car_chase_01.mp4')\n",
    "\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "# tracker = cv2.TrackerBoosting_create()   # error\n",
    "# tracker = cv2.TrackerMIL_create()\n",
    "# tracker = cv2.TrackerKCF_create()\n",
    "# tracker = cv2.TrackerTLD_create()            # error\n",
    "# tracker = cv2.TrackerMedianFlow_create()       # error\n",
    "# tracker = cv2.TrackerGOTURN_create()          # error\n",
    "# tracker = cv2.TrackerMOSSE_create()         # error\n",
    "\n",
    "roi = None\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(40) == 27  : break\n",
    "    \n",
    "    if roi is None:\n",
    "        roi = cv2.selectROI('frame', frame)\n",
    "        if roi != (0, 0, 0, 0):\n",
    "            tracker.init(frame, roi)\n",
    "\n",
    "    success, rect = tracker.update(frame)\n",
    "    if success: \n",
    "        (x, y, w, h) = [int(i) for i in rect]\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track one obj in live Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "# tracker = cv2.TrackerBoosting_create()   # error\n",
    "# tracker = cv2.TrackerMIL_create()\n",
    "# tracker = cv2.TrackerKCF_create()\n",
    "# tracker = cv2.TrackerTLD_create()            # error\n",
    "# tracker = cv2.TrackerMedianFlow_create()       # error\n",
    "# tracker = cv2.TrackerGOTURN_create()          # error\n",
    "# tracker = cv2.TrackerMOSSE_create()         # error\n",
    "time.sleep(3)\n",
    "roi = None\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(30) == 27: break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    if roi is None:\n",
    "        roi = cv2.selectROI('frame', frame)\n",
    "        if roi != (0, 0, 0, 0):\n",
    "            tracker.init(frame, roi)\n",
    "\n",
    "    success, rect = tracker.update(frame)\n",
    "    if success: \n",
    "        (x, y, w, h) = [int(i) for i in rect]\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "cap.release()        \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI (Region Of Interest) 只處理部分有興趣的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 指定 ROI 座標位置\n",
    "RECT = ((320, 20), (570, 390))\n",
    "(left, top), (right, bottom) = RECT\n",
    "\n",
    "def roiarea(frame):                  # 取出 ROI 子畫面\n",
    "    return frame[top:bottom, left:right]\n",
    "\n",
    "def replaceroi(frame, roi):             # 將 ROI 區域貼回到原畫面\n",
    "    frame[top:bottom, left:right] = roi\n",
    "    return frame\n",
    "\n",
    "cap = cv2.VideoCapture(0)               # 開啟攝影機, 讀取畫面\n",
    "ratio = cap.get(cv2.CAP_PROP_FRAME_WIDTH) / cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "w = 800\n",
    "h = int(w / ratio)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(30) == 27: break\n",
    "    frame = cv2.resize(frame, (w, h))\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    roi = roiarea(frame)                   # 取出子畫面\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)        # BGR to HSV\n",
    "#     roi = cv2.cvtColor(roi, cv2.COLOR_BGR2XYZ)          # BGR to XYZ\n",
    "    \n",
    "    frame = replaceroi(frame, roi)         # 將處理完的子畫面貼回到原本畫面中\n",
    "    \n",
    "    cv2.rectangle(frame, RECT[0], RECT[1], (0,0,255), 2)      # 在 ROI 範圍處畫個框\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 14. OpenCV 函式庫 DLib 介紹（Python）\n",
    "> Dlib 使用的人臉偵測演算法是以方向梯度直方圖 Histogram of Oriented Gradients（HOG）的特徵加上線性分類器（linear classifier）、影像金字塔（image pyramid）與滑動窗格（sliding window）來實作的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-1: DLib 影像辨識應用\n",
    "> dlib 是一套包含了機器學習、計算機視覺、圖像處理等的函式庫，使用 C++ 開發而成，目前廣泛使用於工業及學術界，也應用在機器人、嵌入式系統、手機、甚至於大型的運算架構中，而且最重要的是，它不但開源且完全免費，而且可跨平台使用（Linux、Mac OS、Windows），並且除了 C++ 之外還提供了 Python API ，因此如果我們想要建立一套物件偵測系統，dlib 是相當適合的平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-2: DLib 套件應用\n",
    "> * pip install dlib==19.22.99<br>\n",
    ">> or (pip install C:\\Users\\User\\Desktop\\OpenCV\\install\\dlib-19.22.99-cp39-cp39-win_amd64.whl)\n",
    "\n",
    "## 14-3: DLib 特徵點描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 照片人臉偵測\n",
    "> detector 函數的第二個參數是指定不取樣（unsample）的次數，如果`圖片太小的時候`，將其設為 1 可讓程式偵較容易測出更多的人臉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "print(f'dlib ver.\\t: {dlib.__version__}\\n\\t\\t: {dlib.__file__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread('./image/dlib.jpg')   # 讀取照片圖檔\n",
    "# img = cv2.imread('./image/faces02.png')\n",
    "img = cv2.imread('./image/face03.jpg')\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()  # Dlib 的人臉偵測器\n",
    "\n",
    "face_rects = detector(img, 1)   # try (img, 1)   # 偵測人臉\n",
    "print(f'detected face numder : {len(face_rects)}')\n",
    "\n",
    "# 取出所有偵測的結果\n",
    "for i, d in enumerate(face_rects):\n",
    "    x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 4, cv2.LINE_AA)  # 以方框標示偵測的人臉\n",
    "    cv2.putText(img, f'{i}', (x1, y1-10), 0, .7, (0, 255, 0), 2, 16)\n",
    "cv2.imshow('Face Detection', img)     # 顯示結果\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偵測結果與分數\n",
    ">改用 detector.run 來偵測人臉，它的第三個參數是指定分數的門檻值，所有分數超過這個門檻值的偵測結果都會被輸出，而傳回的結果除了人臉的位置之外，還有分數（scores）與子偵測器的編號（idx），`子偵測器的編號可以用來判斷人臉的方向`，請參考 Dlib 的說明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./image/dlib.jpg')\n",
    "# img = cv2.imread('./image/faces02.png')\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "font = 2;    lt = 16\n",
    "face_rects, scores, idx = detector.run(img, 1, -0)   # 偵測人臉，輸出分數\n",
    "print(f'detected face numder : {len(face_rects)}')\n",
    "\n",
    "for i, d in enumerate(face_rects):\n",
    "    x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
    "\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.putText(img, f'{scores[i]:.2f}({idx[i]:0.0f})', (x1, y1), font,       # 標示分數\n",
    "          0.7, (0, 0, 255), 1, lt)\n",
    "\n",
    "cv2.imshow('Face Detection', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 影片人臉偵測\n",
    ">FourCC　是一個4位元組碼，用來確定視頻的編碼格式。<br>\n",
    "FOURCC is short for `FOUR Character Code` - an identifier for a video codec, compression format, color or pixel format used in media files.<br>\n",
    "https://calculla.com/fourcc<br>\n",
    "https://blog.csdn.net/u013943420/article/details/78779197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./video/Alec_Baldwin.mp4')    # 開啟影片檔案\n",
    "\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))        # 取得畫面尺寸\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')             # 使用 XVID 編碼\n",
    "cap.set(cv2.CAP_PROP_POS_MSEC, 10000)               #  從第 10 秒開始撥放\n",
    "\n",
    "# 建立 VideoWriter 物件，輸出影片至 output.avi，FPS 值為 20.0\n",
    "out = cv2.VideoWriter('./video/Alec_Baldwin_out.mp4', fourcc, fps, (w, h))\n",
    "font = 2;    lt = 16\n",
    "detector = dlib.get_frontal_face_detector()          # Dlib 的人臉偵測器\n",
    "\n",
    "while(cap.isOpened() and cap.get(cv2.CAP_PROP_POS_MSEC)) < 20000:    # 以迴圈從影片檔案讀取影格，並顯示出來\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(1) & 0xFF == 27 : break\n",
    "    \n",
    "    face_rects, scores, idx = detector.run(frame, 0, -.5)  # 偵測人臉\n",
    "    for i, d in enumerate(face_rects):               # 取出所有偵測的結果\n",
    "        x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4, cv2.LINE_AA) # 以方框標示偵測的人臉\n",
    "        cv2.putText(frame, f'{scores[i]:.2f}, ({idx[i]:0.0f})', (x1, y1), font,          # 標示分數\n",
    "                    0.7, (255, 255, 255), 1, lt)\n",
    "            \n",
    "    cv2.imshow('Face Detection', frame)           # 顯示結果\n",
    "    out.write(frame)                               # 寫入影格\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 即時串流影像人臉偵測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)   # 開啟影片檔案\n",
    "detector = dlib.get_frontal_face_detector() # Dlib 的人臉偵測器\n",
    "font = 2;    lt = 16\n",
    "while(cap.isOpened()):               # 以迴圈從影片檔案讀取影格，並顯示出來\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or cv2.waitKey(1) & 0xFF == 27 : break\n",
    "    \n",
    "    face_rects, scores, idx = detector.run(frame, 0)  # 偵測人臉\n",
    "\n",
    "    for i, d in enumerate(face_rects):                  # 取出所有偵測的結果\n",
    "        x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4, cv2.LINE_AA) # 以方框標示偵測的人臉\n",
    "        cv2.putText(frame, f'{scores[i]:.2f}, ({idx[i]:0.0f})', (x1, y1), font,    # 標示分數\n",
    "            0.7, (255, 255, 255), 1, lt)\n",
    "\n",
    "    cv2.imshow('Face Detection', frame)                      # 顯示結果\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實現人臉 68 個關鍵點檢測\n",
    "> dlib 也提供訓練好的模型，可以辨識出人臉的 68 的特徵點，68 特徵點包括鼻子、眼睛、眉毛，以及嘴巴等等，如上圖紅點就是偵測出人臉的 68 個特徵點。\n",
    "$$EAR = \\frac{||p_2-p_6||+||p_3-p_5||}{2||p_1-p_4||}$$\n",
    "\n",
    "><img src=\"./image/dlib68.jpg\"  style='width:48%'>\n",
    "><img src=\"./image/dlib01.jpg\"  style='width:48%'><br>\n",
    "><img src=\"./image/dlib02.jpg\"  style='width:48%'>\n",
    "><img src=\"./image/dlib03.jfif\"  style='width:48%'>\n",
    "><img src=\"./image/face68_1.jpg\"  style='width:60%'>\n",
    "### reference :\n",
    "https://www.youtube.com/watch?v=DHYM4-8x9Po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r'./model/shape_predictor_68_face_landmarks.dat')\n",
    "# predictor = dlib.shape_predictor(r'./model/shape_predictor_5_face_landmarks.dat')\n",
    "font = 2;    lt = 16            # 利用cv2.putText輸出1-68\n",
    "\n",
    "img = cv2.imread('./image/face68.JFIF')     # cv2讀取影像\n",
    "# img = cv2.imread('./image/lenaColor.png')     # cv2讀取影像\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # 取灰度\n",
    "\n",
    "rects = detector(img_gray, 0)             # 人臉數rects\n",
    "for i in range(len(rects)):\n",
    "    landmarks = np.matrix([[p.x, p.y] for p in predictor(img,rects[i]).parts()])\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        \n",
    "        pos = (point[0, 0], point[0, 1])   # 68點的座標\n",
    "#         print(idx,pos)\n",
    "\n",
    "        cv2.circle(img, pos, 3, color=(0, 255, 0)) # 利用cv2.circle給每個特徵點畫一個圈，共68個\n",
    "        cv2.putText(img, str(idx+1), pos, font, 0.8, (0, 0, 255), 1, lt)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlib - Obj detect\n",
    "> * 先收集欲辨識的人正面照片 ( 甚至可以收集多角度照片 ) 於特定資料夾 (此處設為 ./dlib_ObjectCategories10)中，並將檔名設為人名。\n",
    "> * 使用 dlib.get_frontal_face_detector() 擷取資料夾中照片的人臉，再利用 dlib.shape_predictor() 取出臉部 68 個關鍵點。\n",
    "> * dlib.face_recognition_model_v1().compute_face_descriptor() 將 68 個關鍵點進行嵌入成一個 128 維的向量 ν1, ν2, ⋯。\n",
    "> * 相同的方式將鏡頭中的人臉也嵌入成 128 維的向量 ν。\n",
    "> * 計算 ν 與 ν1, ν2, ⋯ 個別計算歐式距離，最接近者及判定其身分。\n",
    "\n",
    "#### download : 101_ObjectCategories.tar.gz (131Mbytes)\n",
    "#### dlownload : Annotations.tar\n",
    "http://www.vision.caltech.edu/Image_Datasets/Caltech101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入必要的library\n",
    "from scipy.io import loadmat\n",
    "from skimage import io\n",
    "import dlib, os, glob, re\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dlib.simple_object_detector_training_options()\n",
    "images = [] # 存放相片圖檔\n",
    "boxes = []  # 存放Annotations\n",
    "\n",
    "for imagePath in paths.list_images('./dlib_ObjectCategories10'):       #依序處理path下的每張圖片\n",
    "\n",
    "    imageID = imagePath.split('\\\\')[1:]     #從圖片路徑名稱中取出ImageID : according\n",
    "    imageCat = imageID[1].split('.')[0].split('_')[1]  # 0001\n",
    "#     print(imagePath, imageID, imageCat)\n",
    "\n",
    "    p = f'./dlib_Annotations10/{imageID[0]}/annotation_{imageCat}.mat'   #載入Annotation\n",
    "    annotations = loadmat(p)['box_coord']\n",
    "    #取出annotations資訊繪成矩形物件，放入boxes變數中。\n",
    "#     (x, y, w, h) = (b.left(), b.top(), b.right(), b.bottom())\n",
    "    \n",
    "    bb = [dlib.rectangle(left=int(x), top=int(y), right=int(w), bottom=int(h)) for (y, h, x, w) in annotations]\n",
    "    boxes.append(bb)\n",
    "    print(f'{imagePath},   {p}')\n",
    "    images.append(io.imread(imagePath))          #將圖片放入images變數\n",
    "print('done', len(images), len(boxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#丟入三個參數開始訓練\n",
    "\n",
    "print('[INFO] training detector...')\n",
    "detector = dlib.train_simple_object_detector(images, boxes, options)\n",
    " \n",
    "# 將訓練結果匯出到檔案\n",
    "print('[INFO] dumping classifier to file...')\n",
    "detector.save('./dlib_output/model.svm')           # save\n",
    "\n",
    "detector = dlib.simple_object_detector('./dlib_output/model.svm')   # load back\n",
    "\n",
    "# 圖形化顯示 Histogram of Oriented Gradients（簡稱HOG）\n",
    "win = dlib.image_window()\n",
    "win.set_image(detector)\n",
    "dlib.hit_enter_to_continue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(detector, open('./dlib_output/model.pkl', 'wb'))\n",
    "detector = pickle.load(open(r'./dlib_output/model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "from imutils import paths\n",
    "#載入訓練好的detector\n",
    "# detector = dlib.simple_object_detector(detector)\n",
    " \n",
    "#載入測試圖片逐張進行\n",
    "for idx, testingPath in enumerate(paths.list_images('./dlib_ObjectCategories10/accordion')):\n",
    "# for idx, testingPath in enumerate(paths.list_images('./dlib_ObjectCategories10/camera')):\n",
    "#讀取圖片並執行dector並產生矩形物件以便用於標記辨識出的部份\n",
    "    image = cv2.imread(testingPath)\n",
    "    boxes = detector(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    for b in boxes:                    #在圖片上繪出該矩形\n",
    "        (x, y, w, h) = (b.left(), b.top(), b.right(), b.bottom())\n",
    "        cv2.rectangle(image, (x, y), (w, h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(str(idx), image)         #顯示圖片\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 15. OCR 光學字元識別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-1 OCR 介紹(Optical Character Recognition)\n",
    "> OCR 為光學文字識別的縮寫（Optical Character Recognition，OCR），就是將`圖片翻譯為文字`。而 Tesseract 是一個 OCR 模組，目前由 Google 贊助。Tesseract 已經有 30 年歷史，一開始它是惠普實驗室的一款專利軟體，於 2005 年開源，從 2006 年後由 Google 贊助進行後續的開發和維護, Tesseract 也是目前公認最優秀、最精準的開源 OCR 系統\n",
    "\n",
    "> Tesseract目前已作為開源項目發佈在Google Project，其最新版本3.0已經`支持中文OCR`，並提供了一個命令行工具。\n",
    "主要使用在辨識掃描文件/圖片的文字。\n",
    "\n",
    ">除了極高的精準度外，Tesseract 也有很高的靈活性，能夠通過訓練識別出任何字體（只要這些字體的風格不變就可以），也能識別出任何 Unicode 字符，是不是非常厲害呢？我們待會會用到的 pytesseract 模組就像是Tesseract的 python 包裝器。\n",
    "\n",
    "><img src=\"./image/OCR1.png\"  style='width:90%'>\n",
    "><img src=\"./image/OCR2.png\"  style='width:90%'>\n",
    "><img src=\"./image/OCR.jpeg\"  style='width:90%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-2 Tesseract 安裝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * pip install pytesseract\n",
    "> * https://github.com/UB-Mannheim/tesseract/wiki <br>\n",
    ">> (download : tesseract-ocr-w64-setup-v..........exe)\n",
    "> * run : tesseract-ocr-w64-setup-5.3.1.20230401.exe\n",
    "> * 安裝好後找到 tesseract.exe 的位置，並複製其絕對路徑，通常會在<br>\n",
    ">> C:\\Program Files\\Tesseract-OCR\\tesseract.exe。\n",
    "\n",
    "> 安裝好後已內建英文包，到此網站下載中文的語言辨識包\n",
    "\n",
    ">> https://github.com/tesseract-ocr/tessdata_best<br>\n",
    ">> — chi_tra.traineddata → 繁體中文包<br>\n",
    ">> — chi_sim.traineddata → 簡體中文包\n",
    "\n",
    ">>下載後將檔案放到 : C:\\Program Files\\Tesseract-OCR\\tessdata\\ 中，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run 下列指令確保 tesseract 安裝成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "pytesseract.get_tesseract_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-3 實作光學字元辨識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "# img = cv2.imread('./image/tess01.jpeg', 0)\n",
    "# img = cv2.resize(img, (650, 850))\n",
    "\n",
    "# img = cv2.imread('./image/starbucks.jpg', 0)\n",
    "# img=cv2.resize(img, (850, 650))\n",
    "\n",
    "# img = cv2.imread('./image/CarPlate1.jpg', 0)   # CarPlate1, 2\n",
    "# img = cv2.imread('./image/CarPlate2.jpg', 0)   # CarPlate1, 2\n",
    "img = cv2.imread('./image/CarPlate3.jpg', 0)   # CarPlate1, 2\n",
    "# img=cv2.resize(img, (450, 550))\n",
    "\n",
    "cv2.imshow('original', img)\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "text = pytesseract.image_to_string(img, lang='eng')  # chi_tra, chi_sim 修改 lang 參數變就可以\n",
    "print(text)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi_tri : OCR 辨識繁體中文\n",
    "\n",
    "> OCR 識別提取圖片中文字原理\n",
    "\n",
    "> * 預處理：在這個步驟通常有：灰度化（如果是彩色影像）、降噪、二值化、字元切分以及歸一化這些子步驟。\n",
    ">> * 經過二值化後，影像只剩下兩種顏色，即黑和白，其中一個是影像背景，另一個顏色就是要識別的文字了。\n",
    ">> * 降噪在這個階段非常重要，降噪演算法的好壞對特徵提取的影響很大。\n",
    ">> * 字元切分則是將影像中的文字分割成單個文字——識別的時候是一個字一個字識別的。如果文字行有傾斜的話往往還要進行傾斜校正。\n",
    ">> * 歸一化則是將單個的文字影像規整到同樣的尺寸，在同一個規格下，才能應用統一的演算法。\n",
    "\n",
    "> * 特徵提取和降維：對於漢字來說，特徵提取比較困難，因為首先漢字是大字符集，漢字中光是最常用的\n",
    ">> * 第一級漢字就有3755個\n",
    ">> * 第二漢字結構複雜，形近字多。\n",
    "\n",
    ">> 在確定了使用何種特徵後，視情況而定，還有可能要進行特徵降維，這種情況就是如果特徵的維數太高（特徵一般用一個向量表示，維數即該向量的分量數），分類器的效率會受到很大的影響，為了提高識別速率，往往就要進行降維，這個過程也很重要，既要降低維數吧，又得使得`減少維數後的特徵向量還保留了足夠的資訊量`（以區分不同的文字）。\n",
    ">分類器設計、訓練和實際識別：分類器是用來進行識別的，就是對於第二步，對一個文字影像，提取出特徵給，丟給分類器，分類器就對其進行分類，告訴你這個特徵該識別成哪個文字。\n",
    "\n",
    "> * 後處理：後處理是用來對分類結果進行優化的，\n",
    ">> * 第一個，分類器的分類有時候不一定是完全正確的（實際上也做不到完全正確），比如對漢字的識別，由於漢字中形近字的存在，很容易將一個字識別成其形近字。後處理中可以去解決這個問題，比如通過語言模型來進行校正——如果分類器將“在哪裡”識別成“存哪裡”，通過`語言模型`會發現“存哪裡”是錯誤的，然後進行校正。\n",
    ">> * 第二個，OCR的識別影像往往是有大量文字的，而且這些文字存在排版、字型大小等複雜情況，後處理中可以嘗試去對識別結果進行格式化，比如按照影像中的排版排列什麼的，舉個例子，一張影像，其左半部分的文字和右半部分的文字毫無關係，而在字元切分過程中，往往是按行切分的，那麼識別結果中左半部分的第一行後面會跟著右半部分的第一行諸如此類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "# img = cv2.imread('./image/OCR7.jpg', 0)\n",
    "img = cv2.imread('./image/tess02.jpg', 0)\n",
    "\n",
    "img1=cv2.resize(img, (650, 850))\n",
    "cv2.imshow('original', img)\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "text = pytesseract.image_to_string(img, lang='chi_tra')  # try : chi_tra+eng, chi_sim\n",
    "print(text)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.replace(' ', '').replace('.', ', ').replace('\\n', '').replace('‧', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesseract 命令列參數最重要的三個是 -l, --oem, --psm。\n",
    "* -l ：控制輸入文本的語言，用 eng 表示英文（預設語言），用 chi_sim 表示中文簡體，用 chi_tra 表示中文繁體。\n",
    "* --oem ：OCR Engine modes，Tesseract 有兩個OCR引擎，使用 -oem 選擇演算法類型，有四種操作模式可供選擇。\n",
    "\n",
    "|值 |說明                                 |\n",
    "|---|------------------------------------|\n",
    "| 0 |Legacy engine only                  |\n",
    "| 1 |Neural nets LSTM engine only        |\n",
    "| 2 |Legacy + LSTM engines               |\n",
    "| 3 |Default, based on what is available |\n",
    "\n",
    "> 用 --oem 1 表示我們希望只使用LSTM neural network。\n",
    "* --psm：Page segmentation modes，控制 Tesseract 使用的自動頁面分割模式。\n",
    "\n",
    ">> Page segmentation modes:\n",
    "\n",
    "|值 |說明                                                              |\n",
    "|---|------------------------------------------------------------------|\n",
    "| 0 |Orientation and script detection (OSD) only                       |\n",
    "| 1 |Automatic page segmentation with OSD                              |\n",
    "| 2 |Automatic page segmentation, but no OSD, or OCR (not implemented) |\n",
    "| `3` |`Fully automatic page segmentation, but no OSD (Default)`           |\n",
    "| 4 |Assume a single column of text of variable sizes                  |\n",
    "| 5 |Assume a single uniform block of vertically aligned text          |\n",
    "| 6 |Assume a single uniform block of text                             |\n",
    "| 7 |Treat the image as a single text line                             |\n",
    "| 8 |Treat the image as a single word                                  |\n",
    "| 9 |Treat the image as a single word in a circle                      |\n",
    "| 10| Treat the image as a single character                            |\n",
    "| 11| Sparse text. Find as much text as possible in no particular order|\n",
    "| 12| Sparse text with OSD                                             |\n",
    "| 13| Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pytesseract\n",
    "\n",
    "img = cv2.imread('./image/tess01.jpeg', 0)\n",
    "img=cv2.resize(img, (650, 850))\n",
    "\n",
    "# Adding custom options\n",
    "custom_config = r'--oem 3 --psm 3'\n",
    "text = pytesseract.image_to_string(img, config=custom_config)\n",
    "print(text)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image_to_boxes 返回包含已識別字元及其框邊界的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "img = cv2.imread('./image/tess01.jpeg')\n",
    "img=cv2.resize(img, (650, 850))\n",
    "\n",
    "h, w, c = img.shape\n",
    "boxes = pytesseract.image_to_boxes(img) \n",
    "for b in boxes.splitlines():\n",
    "    b = b.split(' ')\n",
    "    img = cv2.rectangle(img, (int(b[1]), h - int(b[2])), (int(b[3]), h - int(b[4])), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image_to_data 定位圖片中的文字，以及該函數輸出資料的格式詳解, 包含框邊界，置信度和其他資訊的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "img = cv2.imread('./image/tess01.jpeg')\n",
    "img=cv2.resize(img, (650, 850))\n",
    "d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "print(f'd.type : {type(d)}\\n\\n'\n",
    "      f'd.keys() :\\n{d.keys()}\\n\\n'\n",
    "      f'd :\\n{d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text key in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boxes = len(d['text'])\n",
    "for i in range(n_boxes):\n",
    "    if float(d['conf'][i]) > 60:\n",
    "        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>--- the end ---</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
